<?xml version="1.0" encoding="utf-8"?>
<rss version="2.0"
  xmlns:atom="http://www.w3.org/2005/Atom"
  xmlns:content="http://purl.org/rss/1.0/modules/content/">
  <channel>
    <title>Dailycrush</title>
    <link>https://katie0809.github.io/</link>
    
    <atom:link href="/rss2.xml" rel="self" type="application/rss+xml"/>
    
    <description></description>
    <pubDate>Fri, 20 Mar 2020 06:45:37 GMT</pubDate>
    <generator>http://hexo.io/</generator>
    
    <item>
      <title>Vue.js 코딩가이드 : Strongly Recommended (Improving Readability)</title>
      <link>https://katie0809.github.io/2020/03/20/vue-study2/</link>
      <guid>https://katie0809.github.io/2020/03/20/vue-study2/</guid>
      <pubDate>Fri, 20 Mar 2020 06:19:30 GMT</pubDate>
      <description>
      
        &lt;p&gt;참고한 문서는 &lt;a href=&quot;https://vuejs.org/v2/style-guide/index.html&quot; rel=&quot;external nofollow noopener noreferrer&quot; target=&quot;_blank&quot;&gt;Vue.js Official Style Guide&lt;/a&gt; 이다.&lt;/p&gt;
&lt;p&gt;모든 가이드가 그렇듯, 절대적인 지표는 아니며 개발 방향을 정해주는 정도로 참고하면 되겠다. &lt;/p&gt;
&lt;h1 id=&quot;Priority-B-Rules-Strongly-Recommended-Improving-Readability&quot;&gt;&lt;a href=&quot;#Priority-B-Rules-Strongly-Recommended-Improving-Readability&quot; class=&quot;headerlink&quot; title=&quot;Priority B Rules : Strongly Recommended (Improving Readability)&quot;&gt;&lt;/a&gt;Priority B Rules : Strongly Recommended (Improving Readability)&lt;/h1&gt;&lt;h2 id=&quot;Component-files&quot;&gt;&lt;a href=&quot;#Component-files&quot; class=&quot;headerlink&quot; title=&quot;Component files&quot;&gt;&lt;/a&gt;Component files&lt;/h2&gt;&lt;ul&gt;
&lt;li&gt;빌드시스템이 파일의 병합이 가능하다면, 가급적 각 컴포넌트는 각자의 파일로 분리하도록 한다.&lt;/li&gt;
&lt;/ul&gt;
      
      </description>
      
      
      <content:encoded><![CDATA[<p>참고한 문서는 <a href="https://vuejs.org/v2/style-guide/index.html" rel="external nofollow noopener noreferrer" target="_blank">Vue.js Official Style Guide</a> 이다.</p><p>모든 가이드가 그렇듯, 절대적인 지표는 아니며 개발 방향을 정해주는 정도로 참고하면 되겠다. </p><h1 id="Priority-B-Rules-Strongly-Recommended-Improving-Readability"><a href="#Priority-B-Rules-Strongly-Recommended-Improving-Readability" class="headerlink" title="Priority B Rules : Strongly Recommended (Improving Readability)"></a>Priority B Rules : Strongly Recommended (Improving Readability)</h1><h2 id="Component-files"><a href="#Component-files" class="headerlink" title="Component files"></a>Component files</h2><ul><li>빌드시스템이 파일의 병합이 가능하다면, 가급적 각 컴포넌트는 각자의 파일로 분리하도록 한다.</li></ul><a id="more"></a><h3 id="Good"><a href="#Good" class="headerlink" title="Good"></a>Good</h3><figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br></pre></td><td class="code"><pre><span class="line">components&#x2F;</span><br><span class="line">|- TodoList.vue</span><br><span class="line">|- TodoItem.vue</span><br></pre></td></tr></table></figure><h2 id="Single-file-component-filename-casing"><a href="#Single-file-component-filename-casing" class="headerlink" title="Single-file component filename casing"></a>Single-file component filename casing</h2><ul><li>싱글파일 컴포넌트 체계를 사용한다면 single-file component는 <code>PascalCase</code>나 <code>kebab-case</code>를 사용하도록 한다.</li><li>PascalCase는 코드 작성시 에디터툴들과의 호환성이 좋다. 다만 case-insensitive system에서는 가끔 오류를 발생시킬 수 있다. kebab-case는 모든 경우에 잘 호환된다.</li></ul><h3 id="Good-1"><a href="#Good-1" class="headerlink" title="Good"></a>Good</h3><figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br></pre></td><td class="code"><pre><span class="line">components&#x2F;</span><br><span class="line">|- MyComponent.vue</span><br></pre></td></tr></table></figure><figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br></pre></td><td class="code"><pre><span class="line">components&#x2F;</span><br><span class="line">|- my-component.vue</span><br></pre></td></tr></table></figure><h2 id="Base-component-name"><a href="#Base-component-name" class="headerlink" title="Base component name"></a>Base component name</h2><ul><li><p>Base component는 <code>Base</code>, <code>App</code>, <code>V</code> 와 같은 special prefix로 시작해야한다.</p></li><li><p>Base component는 앱에서 사용되는 consistent styling과 behavior의 기본이 된다. </p><p>Base component는 다음의 항목만 포함하는 컴포넌트를 의미한다.</p><ul><li>HTML elements</li><li>other base compoenents</li><li>3rd-party UI components</li></ul></li></ul>]]></content:encoded>
      
      <comments>https://katie0809.github.io/2020/03/20/vue-study2/#disqus_thread</comments>
    </item>
    
    <item>
      <title>Vue.js 코딩가이드 : Essential(Error Prevention)</title>
      <link>https://katie0809.github.io/2020/03/20/vue-study1/</link>
      <guid>https://katie0809.github.io/2020/03/20/vue-study1/</guid>
      <pubDate>Fri, 20 Mar 2020 03:59:30 GMT</pubDate>
      <description>
      
        &lt;p&gt;맨날 회사에서 Vue를 사용하지만 코딩 스타일 가이드를 정독한 적은 없는 관계로.. 조금 짬이 난 김에 한번 스타일가이드를 정리해보려 한다.&lt;/p&gt;
&lt;p&gt;참고한 문서는 &lt;a href=&quot;https://vuejs.org/v2/style-guide/index.html&quot; rel=&quot;external nofollow noopener noreferrer&quot; target=&quot;_blank&quot;&gt;Vue.js Official Style Guide&lt;/a&gt; 이다.&lt;/p&gt;
&lt;p&gt;모든 가이드가 그렇듯, 절대적인 지표는 아니며 개발 방향을 정해주는 정도로 참고하면 되겠다. &lt;/p&gt;
&lt;h1 id=&quot;Rule-Categories&quot;&gt;&lt;a href=&quot;#Rule-Categories&quot; class=&quot;headerlink&quot; title=&quot;Rule Categories&quot;&gt;&lt;/a&gt;Rule Categories&lt;/h1&gt;&lt;p&gt;이하의 룰들은 총 4가지 카테고리로 나뉘게 된다.&lt;/p&gt;
      
      </description>
      
      
      <content:encoded><![CDATA[<p>맨날 회사에서 Vue를 사용하지만 코딩 스타일 가이드를 정독한 적은 없는 관계로.. 조금 짬이 난 김에 한번 스타일가이드를 정리해보려 한다.</p><p>참고한 문서는 <a href="https://vuejs.org/v2/style-guide/index.html" rel="external nofollow noopener noreferrer" target="_blank">Vue.js Official Style Guide</a> 이다.</p><p>모든 가이드가 그렇듯, 절대적인 지표는 아니며 개발 방향을 정해주는 정도로 참고하면 되겠다. </p><h1 id="Rule-Categories"><a href="#Rule-Categories" class="headerlink" title="Rule Categories"></a>Rule Categories</h1><p>이하의 룰들은 총 4가지 카테고리로 나뉘게 된다.</p><a id="more"></a><table><thead><tr><th>명칭</th><th>우선순위</th><th>설명</th></tr></thead><tbody><tr><td>Priority A</td><td>Essential</td><td>- 에러를 방지한다. <br>- 룰의 예외는 극히 드물다.</td></tr><tr><td>Priority B</td><td>Strongly Recommended</td><td>- 코드의 가독성/개발 편의성을 높여준다.<br>- 해당 룰을 지키지 않아도 코드는 실행될 수 있지만 가급적 룰을 따르는 것이 좋다.</td></tr><tr><td>Priority C</td><td>Recommended</td><td>- 허용가능한/타당한 이유의 다른 방식이 있을때에는 꼭 따르지 않아도 된다. <br>- 해당 룰에서는 가능한 다른 방법과 함께 default option을 제안한다. 개발자는 이들 중 자신의 프로젝트에 맞춰 따를 룰을 정할 수 있다.<br>- 이러한 Community rule을 따르면 다른 코드를 이식해올 때 용이하다</td></tr><tr><td>Priority D</td><td>Use with caution</td><td>- 희귀한 edge case들을 처리하거나 legacy code의 migration을 위한 룰<br>- 남용시 코드 가독성을 떨어뜨리고 오류를 발생시킬 수 있다.</td></tr></tbody></table><h1 id="Priority-A-Rules-Essential-Error-Prevention"><a href="#Priority-A-Rules-Essential-Error-Prevention" class="headerlink" title="Priority A Rules : Essential(Error Prevention)"></a>Priority A Rules : Essential(Error Prevention)</h1><h2 id="Multi-word-component-names"><a href="#Multi-word-component-names" class="headerlink" title="Multi-word component names"></a>Multi-word component names</h2><ul><li><p><code>컴포넌트명</code>은 반드시 <strong>여러개의 단어</strong>로 되어있어야 한다.(Root의 App 컴포넌트나 <component>, <transition> 같은 내장 컴포넌트 제외) </transition></component></p></li><li><p>현재 존재하는, 앞으로 만들어질 수 있는 HTML 태그와의 혼선을 방지할 수 있다.(모든 HTML 태그는 single word)</p></li></ul><h3 id="Bad"><a href="#Bad" class="headerlink" title="Bad"></a>Bad</h3><figure class="highlight javascript"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br></pre></td><td class="code"><pre><span class="line">Vue.component(<span class="string">'todo'</span>, &#123;</span><br><span class="line">  <span class="comment">// ...</span></span><br><span class="line">&#125;)</span><br></pre></td></tr></table></figure><h3 id="Good"><a href="#Good" class="headerlink" title="Good"></a>Good</h3><figure class="highlight javascript"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br></pre></td><td class="code"><pre><span class="line">Vue.component(<span class="string">'todo-item'</span>, &#123;</span><br><span class="line">  <span class="comment">// ...</span></span><br><span class="line">&#125;)</span><br></pre></td></tr></table></figure><h2 id="Component-Data"><a href="#Component-Data" class="headerlink" title="Component Data"></a>Component Data</h2><ul><li>컴포넌트의 <code>data</code>는 <strong>반드시 함수형</strong>이어야 한다.(object 반환, <code>new Vue</code> 초기화시 제외)</li><li>컴포넌트 data를 단순한 object형으로 선언할 경우 컴포넌트의 <u>재사용성</u>에 큰 문제 생긴다.<ul><li>해당 컴포넌트를 사용하는 <strong><u>모든 instance에서 같은 data object를 참조</u></strong>한다.</li><li>즉, 컴포넌트의 데이터가 its own data가 아닌 shared data가 되어버린다. 어떤 하나가 행하는 deletion/adding/changing은 모든 다른 컴포넌트들에 영향을 미침.</li></ul></li><li>따라서 각 컴포넌트는 자신만의 고유한 데이터를 가질 수 있어야하며, 컴포넌트의 data 필드를 특정 데이터 객체를 반환하는 함수형으로 선언함으로서 문제를 해결할 수 있다.</li></ul><h3 id="Bad-1"><a href="#Bad-1" class="headerlink" title="Bad"></a>Bad</h3><figure class="highlight javascript"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br></pre></td><td class="code"><pre><span class="line">Vue.component(<span class="string">'some-comp'</span>, &#123;</span><br><span class="line">  data: &#123;</span><br><span class="line">    foo: <span class="string">'bar'</span></span><br><span class="line">  &#125;</span><br><span class="line">&#125;)</span><br></pre></td></tr></table></figure><h3 id="Good-1"><a href="#Good-1" class="headerlink" title="Good"></a>Good</h3><figure class="highlight javascript"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br></pre></td><td class="code"><pre><span class="line">Vue.component(<span class="string">'some-comp'</span>, &#123;</span><br><span class="line">  data: <span class="function"><span class="keyword">function</span> (<span class="params"></span>) </span>&#123;</span><br><span class="line">    <span class="keyword">return</span> &#123;</span><br><span class="line">      foo: <span class="string">'bar'</span></span><br><span class="line">    &#125;</span><br><span class="line">  &#125;</span><br><span class="line">&#125;)</span><br></pre></td></tr></table></figure><h3 id="Exception"><a href="#Exception" class="headerlink" title="Exception"></a>Exception</h3><figure class="highlight javascript"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br></pre></td><td class="code"><pre><span class="line"><span class="comment">// It's OK to use an object directly in a root</span></span><br><span class="line"><span class="comment">// Vue instance, since only a single instance</span></span><br><span class="line"><span class="comment">// will ever exist.</span></span><br><span class="line"><span class="keyword">new</span> Vue(&#123;</span><br><span class="line">  data: &#123;</span><br><span class="line">    foo: <span class="string">'bar'</span></span><br><span class="line">  &#125;</span><br><span class="line">&#125;)</span><br></pre></td></tr></table></figure><h2 id="Prop-definition"><a href="#Prop-definition" class="headerlink" title="Prop definition"></a>Prop definition</h2><ul><li><code>prop</code>은 가능한 한 <strong>상세하게 필드를 정의</strong>하는 것이 좋다.</li><li><u>최소한 type</u>은 명세해야 한다.</li></ul><h3 id="Bad-2"><a href="#Bad-2" class="headerlink" title="Bad"></a>Bad</h3><figure class="highlight javascript"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br></pre></td><td class="code"><pre><span class="line"><span class="comment">// This is only OK when prototyping</span></span><br><span class="line">props: [<span class="string">'status'</span>]</span><br></pre></td></tr></table></figure><h3 id="Good-2"><a href="#Good-2" class="headerlink" title="Good"></a>Good</h3><figure class="highlight javascript"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br></pre></td><td class="code"><pre><span class="line"><span class="comment">// Not bad</span></span><br><span class="line">props: &#123;</span><br><span class="line">  status: <span class="built_in">String</span></span><br><span class="line">&#125;</span><br><span class="line"></span><br><span class="line"><span class="comment">// Even better!</span></span><br><span class="line">props: &#123;</span><br><span class="line">  status: &#123;</span><br><span class="line">    type: <span class="built_in">String</span>,</span><br><span class="line">    required: <span class="literal">true</span>,</span><br><span class="line">    validator: <span class="function"><span class="keyword">function</span> (<span class="params">value</span>) </span>&#123;</span><br><span class="line">      <span class="keyword">return</span> [</span><br><span class="line">        <span class="string">'syncing'</span>,</span><br><span class="line">        <span class="string">'synced'</span>,</span><br><span class="line">        <span class="string">'version-conflict'</span>,</span><br><span class="line">        <span class="string">'error'</span></span><br><span class="line">      ].indexOf(value) !== <span class="number">-1</span></span><br><span class="line">    &#125;</span><br><span class="line">  &#125;</span><br><span class="line">&#125;</span><br></pre></td></tr></table></figure><h2 id="Keyed-v-for"><a href="#Keyed-v-for" class="headerlink" title="Keyed v-for"></a>Keyed v-for</h2><ul><li>v-for 사용시에는 항상 <code>key</code>가 있어야 한다.</li><li>v-for의 key는 내부 subtree의 제어에 사용된다. </li><li>object consistancy를 유지하는 등의  <u>예측가능한 행위를 제어/관리</u>할 수 있도록 두는것이 좋다.<ul><li>예를들어 리스트 아이템 [b,a]를 알파벳 순서로 정렬한다고 가정해보자.</li><li>Vue는 가장 비용이 적게 드는 방식을 선택할 것. 가장 간단한 방법은 b를 삭제한 뒤 a 뒤에 다시 넣는 것이다.</li><li>하지만 만약 애니메이션 등을 사용해서 b를 삭제하면 안된다면? b가 텍스트필드라 정렬후에도 b에대한 focus를 잃어선 안된다면?</li><li>이때 b, a 각각에 고유한 키를 부여한다면 Vue가 보다 더 predictable 하게 행동하도록 제어할 수 있다.</li></ul></li></ul><h3 id="Bad-3"><a href="#Bad-3" class="headerlink" title="Bad"></a>Bad</h3><figure class="highlight javascript"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br></pre></td><td class="code"><pre><span class="line">&lt;ul&gt;</span><br><span class="line">  &lt;li v-<span class="keyword">for</span>=<span class="string">"todo in todos"</span>&gt;</span><br><span class="line">    &#123;&#123; todo.text &#125;&#125;</span><br><span class="line">  &lt;<span class="regexp">/li&gt;</span></span><br><span class="line"><span class="regexp">&lt;/u</span>l&gt;</span><br></pre></td></tr></table></figure><h3 id="Good-3"><a href="#Good-3" class="headerlink" title="Good"></a>Good</h3><figure class="highlight javascript"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br></pre></td><td class="code"><pre><span class="line">&lt;ul&gt;</span><br><span class="line">  &lt;li v-<span class="keyword">for</span>=<span class="string">"todo in todos"</span> :key=<span class="string">"todo.id"</span>&gt;</span><br><span class="line">    &#123;&#123; todo.text &#125;&#125;</span><br><span class="line">  &lt;<span class="regexp">/li&gt;</span></span><br><span class="line"><span class="regexp">&lt;/u</span>l&gt;</span><br></pre></td></tr></table></figure><h2 id="Avoid-v-if-with-v-for"><a href="#Avoid-v-if-with-v-for" class="headerlink" title="Avoid v-if with v-for"></a>Avoid v-if with v-for</h2><ul><li><p>v-for과 v-if를 같은 element 내에서 사용하면 안된다.</p><ul><li>vue가 directive를 처리할떄 <strong><code>v-for</code>는 <code>v-if</code>보다 높은 우선순위</strong>를 갖는다.</li><li>따라서 v-for와 v-if를 같이 사용한 아래와 같은 코드는</li></ul><figure class="highlight javascript"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br></pre></td><td class="code"><pre><span class="line">&lt;ul&gt;</span><br><span class="line">  &lt;li v-<span class="keyword">for</span>=<span class="string">"user in users"</span> v-<span class="keyword">if</span>=<span class="string">"user.isActive"</span> :key=<span class="string">"user.id"</span>&gt;</span><br><span class="line">    &#123;&#123; user.name &#125;&#125;</span><br><span class="line">  &lt;<span class="regexp">/li&gt;</span></span><br><span class="line"><span class="regexp">&lt;/u</span>l&gt;</span><br></pre></td></tr></table></figure><p>다음과 같이 변환된다.</p><figure class="highlight javascript"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br></pre></td><td class="code"><pre><span class="line"><span class="keyword">this</span>.users.map(<span class="function"><span class="keyword">function</span> (<span class="params">user</span>) </span>&#123;</span><br><span class="line">  <span class="keyword">if</span> (user.isActive) &#123;</span><br><span class="line">    <span class="keyword">return</span> user.name</span><br><span class="line">  &#125;</span><br><span class="line">&#125;)</span><br></pre></td></tr></table></figure><p>따라서 user의 정보가 일부분만 바뀌어도 re-render를 위해 리스트 전체를 다시 iterate해야한다.</p><ul><li><p>위와같은 경우 </p><ol><li><p>if조건을 v-for 컴포넌트의 상위 컴포넌트로 옮긴다 </p></li><li><p>Computed 메서드에 user에 대한 필터링 조건을 설정하여 v-for로 iterate되는 리스트 자체를 필터링된 리스트로 사용한다.</p></li></ol></li></ul><figure class="highlight javascript"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br></pre></td><td class="code"><pre><span class="line">&lt;ul&gt;</span><br><span class="line">  &lt;li</span><br><span class="line">    v-<span class="keyword">for</span>=<span class="string">"user in activeUsers"</span></span><br><span class="line">    :key=<span class="string">"user.id"</span></span><br><span class="line">  &gt;</span><br><span class="line">    &#123;&#123; user.name &#125;&#125;</span><br><span class="line">  &lt;<span class="regexp">/li&gt;</span></span><br><span class="line"><span class="regexp">&lt;/u</span>l&gt;</span><br><span class="line"></span><br><span class="line">...</span><br><span class="line"></span><br><span class="line">computed: &#123;</span><br><span class="line">  activeUsers: <span class="function"><span class="keyword">function</span> (<span class="params"></span>) </span>&#123;</span><br><span class="line">    <span class="keyword">return</span> <span class="keyword">this</span>.users.filter(<span class="function"><span class="keyword">function</span> (<span class="params">user</span>) </span>&#123;</span><br><span class="line">      <span class="keyword">return</span> user.isActive</span><br><span class="line">    &#125;)</span><br><span class="line">  &#125;</span><br><span class="line">&#125;</span><br></pre></td></tr></table></figure></li></ul><h2 id="Component-style-scoping"><a href="#Component-style-scoping" class="headerlink" title="Component style scoping"></a>Component style scoping</h2><ul><li>Root의 App컴포넌트를 제외한 모든 컴포넌트의 스타일은 scoped 되어야 한다 : <style scoped></li><li>혹은 고유한 클래스명을 사용하는 것도 방법일 수 있다.</li></ul><h2 id="Private-property-names"><a href="#Private-property-names" class="headerlink" title="Private property names"></a>Private property names</h2><ul><li>private function이 외부에서 접근 불가능하도록 module scoping을 사용한다.</li><li>혹은 plugin, mixin 의 모든 custom private properties에 대해 <code>$_</code> prefix를 사용한다.<ul><li><code>_</code> prefix는 뷰 자체 private properties를 정의하는데에 사용하므로 <code>_update</code> 와 같은 이름은 instance property를 overwrite할 위험이 있다.</li><li><code>$</code> 는 special instance properties를 지칭하는데 사용된다.</li><li>따라서 두개의 prefix를 합친 <code>$_yourPluginName</code>의 사용을 권장.</li></ul></li></ul><h3 id="Bad-4"><a href="#Bad-4" class="headerlink" title="Bad"></a>Bad</h3><figure class="highlight javascript"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br></pre></td><td class="code"><pre><span class="line"><span class="keyword">var</span> myGreatMixin = &#123;</span><br><span class="line">  <span class="comment">// ...</span></span><br><span class="line">  methods: &#123;</span><br><span class="line">    update: <span class="function"><span class="keyword">function</span> (<span class="params"></span>) </span>&#123;</span><br><span class="line">      <span class="comment">// ...</span></span><br><span class="line">    &#125;,</span><br><span class="line">    _update: <span class="function"><span class="keyword">function</span> (<span class="params"></span>) </span>&#123;</span><br><span class="line">      <span class="comment">// ...</span></span><br><span class="line">    &#125;,</span><br><span class="line">    $update: <span class="function"><span class="keyword">function</span> (<span class="params"></span>) </span>&#123;</span><br><span class="line">      <span class="comment">// ...</span></span><br><span class="line">    &#125;,</span><br><span class="line">    $_update: <span class="function"><span class="keyword">function</span> (<span class="params"></span>) </span>&#123;</span><br><span class="line">      <span class="comment">// ...</span></span><br><span class="line">    &#125;,</span><br><span class="line">  &#125;</span><br><span class="line">&#125;</span><br></pre></td></tr></table></figure><h3 id="Good-4"><a href="#Good-4" class="headerlink" title="Good"></a>Good</h3><figure class="highlight javascript"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br></pre></td><td class="code"><pre><span class="line"><span class="keyword">var</span> myGreatMixin = &#123;</span><br><span class="line">  <span class="comment">// ...</span></span><br><span class="line">  methods: &#123;</span><br><span class="line">    $_myGreatMixin_update: <span class="function"><span class="keyword">function</span> (<span class="params"></span>) </span>&#123;</span><br><span class="line">      <span class="comment">// ...</span></span><br><span class="line">    &#125;</span><br><span class="line">  &#125;</span><br><span class="line">&#125;</span><br></pre></td></tr></table></figure><h3 id="Excellent"><a href="#Excellent" class="headerlink" title="Excellent"></a>Excellent</h3><figure class="highlight javascript"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br></pre></td><td class="code"><pre><span class="line"><span class="comment">// Even better!</span></span><br><span class="line"><span class="keyword">var</span> myGreatMixin = &#123;</span><br><span class="line">  <span class="comment">// ...</span></span><br><span class="line">  methods: &#123;</span><br><span class="line">    publicMethod() &#123;</span><br><span class="line">      <span class="comment">// ...</span></span><br><span class="line">      myPrivateFunction()</span><br><span class="line">    &#125;</span><br><span class="line">  &#125;</span><br><span class="line">&#125;</span><br><span class="line"></span><br><span class="line"><span class="function"><span class="keyword">function</span> <span class="title">myPrivateFunction</span>(<span class="params"></span>) </span>&#123;</span><br><span class="line">  <span class="comment">// ...</span></span><br><span class="line">&#125;</span><br><span class="line"></span><br><span class="line"><span class="keyword">export</span> <span class="keyword">default</span> myGreatMixin</span><br></pre></td></tr></table></figure></style></li></ul>]]></content:encoded>
      
      <comments>https://katie0809.github.io/2020/03/20/vue-study1/#disqus_thread</comments>
    </item>
    
    <item>
      <title>[실전 JSP] 오리엔테이션 및 소개</title>
      <link>https://katie0809.github.io/2020/03/10/jsp-study/</link>
      <guid>https://katie0809.github.io/2020/03/10/jsp-study/</guid>
      <pubDate>Tue, 10 Mar 2020 06:14:53 GMT</pubDate>
      <description>
      
        &lt;p&gt;해당 포스트는 인프런의 강좌 &lt;a href=&quot;[https://www.inflearn.com/course/%EC%8B%A4%EC%A0%84-jsp_renew](https://www.inflearn.com/course/실전-jsp_renew)&quot;&gt;실전 JSP renew&lt;/a&gt; 를 공부하며 작성하였습니다.&lt;/p&gt;
&lt;h1 id=&quot;웹-프로그램-개요&quot;&gt;&lt;a href=&quot;#웹-프로그램-개요&quot; class=&quot;headerlink&quot; title=&quot;웹 프로그램 개요&quot;&gt;&lt;/a&gt;웹 프로그램 개요&lt;/h1&gt;&lt;p&gt;: 웹 프로그램이란 &lt;strong&gt;인터넷 서비스를 이용&lt;/strong&gt;해서 서로 다른 구성요소들이 &lt;code&gt;통신&lt;/code&gt;할 수 있는 프로그램이다.&lt;/p&gt;
&lt;ul&gt;
&lt;li&gt;가장 간단한 흐름의 형태는 사용자가 웹서버에 리퀘스트를 날리고, 이를 웹서버가 처리하여 다시 요청한 곳으로 응답해주는 것이다.&lt;/li&gt;
&lt;li&gt;이떄 웹서버가 처리하는 로직을 개발하는 것이 웹 프로그램 개발이라고 할 수 있다.&lt;/li&gt;
&lt;/ul&gt;
      
      </description>
      
      
      <content:encoded><![CDATA[<p>해당 포스트는 인프런의 강좌 <a href="[https://www.inflearn.com/course/%EC%8B%A4%EC%A0%84-jsp_renew](https://www.inflearn.com/course/실전-jsp_renew)">실전 JSP renew</a> 를 공부하며 작성하였습니다.</p><h1 id="웹-프로그램-개요"><a href="#웹-프로그램-개요" class="headerlink" title="웹 프로그램 개요"></a>웹 프로그램 개요</h1><p>: 웹 프로그램이란 <strong>인터넷 서비스를 이용</strong>해서 서로 다른 구성요소들이 <code>통신</code>할 수 있는 프로그램이다.</p><ul><li>가장 간단한 흐름의 형태는 사용자가 웹서버에 리퀘스트를 날리고, 이를 웹서버가 처리하여 다시 요청한 곳으로 응답해주는 것이다.</li><li>이떄 웹서버가 처리하는 로직을 개발하는 것이 웹 프로그램 개발이라고 할 수 있다.</li></ul><a id="more"></a><h2 id="네트워크-프로토콜"><a href="#네트워크-프로토콜" class="headerlink" title="네트워크 프로토콜"></a>네트워크 프로토콜</h2><p>: 통신을 위한 규약으로, HTTP, FTP, SMTP, POP 등이 있다.</p><h2 id="도메인의-구조"><a href="#도메인의-구조" class="headerlink" title="도메인의 구조"></a>도메인의 구조</h2><blockquote><p><code>http</code>(protocol) :// <code>www</code>(인터넷 서비스구분) . <code>google.com</code>(도메인) : <code>80</code>(port) / <code>index.html</code>(경로) </p></blockquote><h2 id="웹-프로그램의-동작-원리"><a href="#웹-프로그램의-동작-원리" class="headerlink" title="웹 프로그램의 동작 원리"></a>웹 프로그램의 동작 원리</h2><p><img src="https://mblogthumb-phinf.pstatic.net/MjAxODEyMTdfMTI4/MDAxNTQ1MDU0NTk1MTg0.DNXQNyBAGlW8vAfRmEVO0Qz8RkVQdAlBPAwgp9R12gMg.CtWJqcJ2aRKsdfazITsJB1_00l9ytgk_INVmKgRYnr8g.PNG.leejjin_/%EB%9E%A802_%EC%9B%B9_%ED%94%84%EB%A1%9C%EA%B7%B8%EB%9E%A8_%EB%8F%99%EC%9E%91%EC%9B%90%EB%A6%AC.PNG?type=w800" alt></p><ul><li>웹서버와 사용자간의 통신에는 html 사용</li><li>사용자 데이터 및 요청 처리/가공은 동적 데이터인 컨테이너에서 처리한다.</li></ul><h1 id="개발-환경-설정"><a href="#개발-환경-설정" class="headerlink" title="개발 환경 설정"></a>개발 환경 설정</h1><p>다음의 목록을 설치한다.</p><ul><li>JDK</li><li>Eclipse(본인은 Intellij 사용)</li><li>Apache Tomcat 8.5 설치(웹 컨테이너) : 최신버전은 더 높지만 안정화된 8.5버전을 많이 사용한다..</li></ul>]]></content:encoded>
      
      <comments>https://katie0809.github.io/2020/03/10/jsp-study/#disqus_thread</comments>
    </item>
    
    <item>
      <title>맥 Permission denied writing to file</title>
      <link>https://katie0809.github.io/2020/02/24/change-permission/</link>
      <guid>https://katie0809.github.io/2020/02/24/change-permission/</guid>
      <pubDate>Mon, 24 Feb 2020 13:40:05 GMT</pubDate>
      <description>
      
        &lt;p&gt;맥은 가끔보면 편한건지 불편한건지 모르겠다. 방금도 VSCode로 프로젝트 실습하다가 안되길래 개빡쳐서 알아보고 쓰는글임.&lt;/p&gt;
&lt;p&gt;&lt;img src=&quot;/image/mac.png&quot; alt=&quot;mac&quot;&gt;&lt;/p&gt;
      
      </description>
      
      
      <content:encoded><![CDATA[<p>맥은 가끔보면 편한건지 불편한건지 모르겠다. 방금도 VSCode로 프로젝트 실습하다가 안되길래 개빡쳐서 알아보고 쓰는글임.</p><p><img src="/image/mac.png" alt="mac"></p><a id="more"></a><p>Permission denied writing to file 오류가 떴다. 아니 내가 만든 프로젝트를 내가 편집하겠다는데 왜 안돼 왜… 왜…. why…</p><p><a href="https://support.apple.com/ko-kr/guide/mac-help/mchlp1203/mac" rel="external nofollow noopener noreferrer" target="_blank">https://support.apple.com/ko-kr/guide/mac-help/mchlp1203/mac</a></p><p>해결방법은 위의 링크에.</p><p><img src="https://help.apple.com/assets/5DE56E530946226A64A4CC4A/5DE56EA30946226A64A4CC52/ko_KR/a17ab93c1c3e9345a90a147e493a31f8.png" alt></p><ol><li>폴더 컨텍스트 메뉴에서 <code>정보 가져오기</code> 누르면 위의 메뉴가 뜬다. </li><li>오른쪽 아래 자물쇠 잠금해제</li><li>권한 변경</li></ol><p>는 안됨. 응 안돼 돌아가^^ 아 Gae BBAK Chin da..</p><p>결국 그냥 <a href="https://medium.com/@AnkitMaheshwariIn/mac-vs-code-error-permission-denied-writing-to-file-bb112180ede" rel="external nofollow noopener noreferrer" target="_blank">https://medium.com/@AnkitMaheshwariIn/mac-vs-code-error-permission-denied-writing-to-file-bb112180ede</a></p><p>터미널에서 해당 디렉터리에 대해 권한 풀어준다.</p><blockquote><p>sudo chmod -R 777 <project_dir_name></project_dir_name></p></blockquote>]]></content:encoded>
      
      <comments>https://katie0809.github.io/2020/02/24/change-permission/#disqus_thread</comments>
    </item>
    
    <item>
      <title>Working with Nuxt.js</title>
      <link>https://katie0809.github.io/2020/02/24/nuxt-study2/</link>
      <guid>https://katie0809.github.io/2020/02/24/nuxt-study2/</guid>
      <pubDate>Mon, 24 Feb 2020 13:22:10 GMT</pubDate>
      <description>
      
        &lt;p&gt;본격적으로 Nuxt.js를 활용하는 방법에 대해 배워보도록 하자.&lt;/p&gt;
      
      </description>
      
      
      <content:encoded><![CDATA[<p>본격적으로 Nuxt.js를 활용하는 방법에 대해 배워보도록 하자.</p><a id="more"></a><h1 id="Customize-the-home-page"><a href="#Customize-the-home-page" class="headerlink" title="Customize the home page"></a>Customize the home page</h1><p>npm으로 이전 포스트의 프로젝트를 실행하면 보이는 첫 페이지. 이때 계속 말했듯 맥북은 명령어 앞에 sudo를 붙여줘야 한다. 귀찮아 죽겠네. </p><p><img src="/image/nuxt3.png" alt="사이트 대문"></p><p>: 위의 화면은 /pages/index.vue 파일의 내용임. 디폴트로 지역 컴포넌트, 링크, heading등을 렌더링한다.</p><h1 id="Nuxt-js-Page-Components"><a href="#Nuxt-js-Page-Components" class="headerlink" title="Nuxt.js Page Components"></a>Nuxt.js Page Components</h1><p>Nuxt application에서 페이지를 만들기 위해서는 페이지 디렉터리 내에 컴포넌트를 만들면 된다. =&gt; Nuxt will <strong>automatically create the route</strong></p><p>만들어진 Nuxt.js 앱의 소스를 확인해보면 <u>default layout</u>을 쓰고 있음을 알 수 있다. Default layout은 vue-cli 프로젝트의 App.vue랑 비슷한데, 마찬가지로 <code>top-level component</code>이다.</p><p><img src="/source/image/nuxt4.png" alt="nuxt4"></p><ul><li>Default layout에 적용된 스타일은 해당 레이아웃을 사용하는 모든 페이지에 일괄적으로 적용된다.</li><li>이러한 전역 스타일을 사용하는 것은 기본 폰트설정, 표 등 몇가지 경우를 제외하고는 <code>지양해야 한다</code>.</li></ul><p><img src="/image/nuxt5.png" alt></p><ul><li>Network 탭에서 해당 페이지를 보여주기 위해 로드된 js번들을 확인할 수 있다. </li><li><u>메인</u> 페이지에서 <strong>post.vue 와 관련된 번들</strong>은 <code>로드되지 않았다</code>.</li><li>이것이 바로 이전 포스트에서 얘기한 <strong>Code Splitting</strong></li></ul>]]></content:encoded>
      
      <comments>https://katie0809.github.io/2020/02/24/nuxt-study2/#disqus_thread</comments>
    </item>
    
    <item>
      <title>Introduction to Nuxt.js</title>
      <link>https://katie0809.github.io/2020/02/23/nuxt-study1/</link>
      <guid>https://katie0809.github.io/2020/02/23/nuxt-study1/</guid>
      <pubDate>Sun, 23 Feb 2020 10:43:11 GMT</pubDate>
      <description>
      
        &lt;p&gt;Nuxt.js is a &lt;strong&gt;framework&lt;/strong&gt; for creating Vue.js application.&lt;/p&gt;
&lt;p&gt;There are some awesome features that nuxt.js provides.&lt;/p&gt;
&lt;ul&gt;
&lt;li&gt;SEO with Server Side Rendering (SSR)&lt;/li&gt;
&lt;li&gt;Pre Rendering&lt;/li&gt;
&lt;li&gt;Code Splitting&lt;/li&gt;
&lt;/ul&gt;
&lt;p&gt;이런 기능을 서비스에 직접 구현해낼 필요 없이 Nuxt를 사용함으로서 benefit을 얻을 수 있다.&lt;/p&gt;
      
      </description>
      
      
      <content:encoded><![CDATA[<p>Nuxt.js is a <strong>framework</strong> for creating Vue.js application.</p><p>There are some awesome features that nuxt.js provides.</p><ul><li>SEO with Server Side Rendering (SSR)</li><li>Pre Rendering</li><li>Code Splitting</li></ul><p>이런 기능을 서비스에 직접 구현해낼 필요 없이 Nuxt를 사용함으로서 benefit을 얻을 수 있다.</p><a id="more"></a><h1 id="What-is-Nuxt-js"><a href="#What-is-Nuxt-js" class="headerlink" title="What is Nuxt.js?"></a>What is Nuxt.js?</h1><h2 id="Server-Side-Rendering"><a href="#Server-Side-Rendering" class="headerlink" title="Server Side Rendering"></a>Server Side Rendering</h2><h3 id="Benefits"><a href="#Benefits" class="headerlink" title="Benefits"></a>Benefits</h3><ul><li>Search Engine Optimization(검색엔진 최적화)</li><li>Meta Tags</li><li>Performance</li></ul><h3 id="Why-SSR"><a href="#Why-SSR" class="headerlink" title="Why SSR?"></a>Why SSR?</h3><blockquote><p>One of the common problems javascript developers have is that it is <u>hard to deal with SEO and Meta Tags</u>.</p></blockquote><p>: 처음 fetch로 데이터 로드할때 페이지는 비어있는 상태(The page is empty on the initial load of something like the content and meta titles). 뿐만 아니라 자바스크립트가 실행되면 <code>there is no content to index or parse</code>. </p><p>=&gt; 따라서 많은 크롤링 시스템은 자바스크립트를 지원하지 않는다. 즉, 자바스크립트로 만든 페이지는 실제 제목이 아닌 title of undefined로 검색엔진에 보이는 것. </p><h3 id="SSR-key-ideas"><a href="#SSR-key-ideas" class="headerlink" title="SSR key ideas"></a>SSR key ideas</h3><p>: 이에 대한 해결책으로 SSR이 등장하게 된다. SSR의 아이디어는 다음과 같다.</p><ul><li>실행중인 서버가 있으며, 이는 <code>html응답을 생성(create the html response)</code> 한다.</li><li>생성된 응답은 클라이언트나 크롤링 시스템에 사용된다(serve it to the client or the crawler)</li><li>따라서 API콜은 서버에서 실행되며,(the call to the API would take place on the server) 실행이 완료되면 메타데이터와 최종 페이지가 생성된다(the meta data will be set and the final page will be served).</li><li>최종적으로 크롤링 시스템이 필요로 하는 SEO정보와 메타테그는 크롤링 되는 시점에 페이지 내에 존재하게 된다.</li></ul><p>뿐만 아니라 페이지를 서버에서 렌더링 함으로서 속도적인 이득을 볼 수 있다.</p><h3 id="Pre-Rendering"><a href="#Pre-Rendering" class="headerlink" title="Pre Rendering"></a>Pre Rendering</h3><p>: Pre rendering 기법을 통해 서버가 페이지의 생성과 배포를 담당하도록 하는 대신 페이지가 먼저 생성되게 된다(?).(Instead of having a server to generate and serve a page, the <code>pages are generated upfront</code>)</p><p>따라서</p><ol><li>배포 폴더는 각 페이지에 대해 <strong>하나의 html파일</strong>을 갖게 된다.</li><li>따라서 트래픽이 많은 서비스에 대해서도 무료로 호스팅이 가능하다(SSR benefits + free hosting).</li></ol><h3 id="Code-Splitting"><a href="#Code-Splitting" class="headerlink" title="Code Splitting"></a>Code Splitting</h3><p>: Nuxt는 어플리케이션이 code-splitting 되도록 한다.</p><blockquote><p>Code splitting : 자바스크립트 코드를 multiple files로 split하기위한 테크닉.</p><p>=&gt; 서비스의 비용 절감, 속도 향상</p></blockquote><h4 id="예시"><a href="#예시" class="headerlink" title="예시"></a>예시</h4><ul><li><p>서비스 전체에 총 100개의 컴포넌트가 있고, 메인 페이지에서는 10개의 컴포넌트만 사용하는 경우</p><p>: 메인 페이지에서 사용되는 자바스크립트 파일(or 번들)은 사용되지 않는 컴포넌트까지 포함하고 있을 필요가 없다.</p><p>Nuxt는 자동적으로 각 페이지에 대한 자바스크립트 파일을 생성하여 프로젝트의 의존관계를 관리한다(take care of the project’s dependencies). </p></li></ul><h1 id="Create-Nuxt-App"><a href="#Create-Nuxt-App" class="headerlink" title="Create Nuxt App"></a>Create Nuxt App</h1><figure class="highlight bash"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line">npx create-nuxt-app nuxt-fundamentals</span><br></pre></td></tr></table></figure><p><img src="/image/nuxtapp.png" alt></p><ul><li>서버사이드 렌더링을 사용할 것이므로 렌더링 모드만 Universal로 해주도록 한다.</li><li>Official scaffolding tool <code>create-nuxt-app</code> 사용한다.</li><li>npx는 npm 5.2.0버전부터 기본적으로 제공된다.</li><li>맥을 사용한다면 명령어 앞에 sudo를 사용해줘야 한다(플젝 돌릴때도 마찬가지)</li></ul><p>설치 완료됐으면 프로젝트를 실행시켜보자. 프로젝트 폴더로 들어가서 npm혹은 yarn명령어 실행</p><figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line">npm run dev</span><br></pre></td></tr></table></figure><h1 id="Guided-Nuxt-js-Project-Tour"><a href="#Guided-Nuxt-js-Project-Tour" class="headerlink" title="Guided Nuxt.js Project Tour"></a>Guided Nuxt.js Project Tour</h1><p>: In this lesson, we’ll show you around in our newly created Nuxt project. </p><p>Please note that <strong>each directory includes a readme file</strong>, that explains <code>what the directory is</code>. You can safely delete the directories of the features you do not need.</p><p><img src="/image/nuxt2.png" alt="생성된 Nuxt.js 앱 프로젝트 구조"></p><table><thead><tr><th>폴더명</th><th>역할</th></tr></thead><tbody><tr><td>assets</td><td>- 아직 컴파일되지 않은 asset들을 포함한다(SASS, 이미지, 폰트 등). <br>- vue cli 프로젝트의 asset 디렉터리와 같은 역할을 한다.</td></tr><tr><td>components</td><td>- vue.js 컴포넌트들이 위치하는 디렉터리.</td></tr><tr><td>layouts</td><td>- 말 그대로 페이지의 레이아웃을 저장하는 디렉터리.<br>- 기본 레이아웃, 갤러리 레이아웃 같이 원하는 커스텀 레이아웃 저장.</td></tr><tr><td>middleware</td><td>- 미들웨어를 저장하는 디렉터리.<br>- 페이지가 렌더링되기 전에 실행가능한 함수 등을 정의할 수 있다.</td></tr><tr><td>pages</td><td>- Contains your application views and routes<br>- nuxt는 이 디렉터리에 있는 모든 vue파일을 읽어 자동으로 application router를 생성한다.</td></tr><tr><td>plugins</td><td>- 자바스크립트 플러그인을 포함한다.<br>- 해당 플러그인들은 instantiating the route vue instance하기 전에 실행된다.<br>- 전역 컴포넌트를 등록하거나 상수/함수를 삽입하는 위치이기도 하다(register the components globally or to inject functions or constants).</td></tr><tr><td>static</td><td>- static 파일을 저장한다. 해당 파일들은 자동으로 서버의 루트위치에 저장된다(automatically mapped to the server’s root).<br>- 해당 위치에 저장된 파일은 <code>웹사이트 url/파일명</code> 으로 접근이 가능하다.</td></tr><tr><td>store</td><td>- Contains your Vuex Store<br>- Vuex Store는 별도의 설치나 구성 없이 Nuxt에서 바로 사용할 수 있지만(out of the box), 디폴트로 disable되어있다.</td></tr></tbody></table><ul><li>각 디렉터리의 Readme.md 파일에는 해당 디렉터리의 기능이 소개되어있다. </li><li>필요없는 기능의 디렉터리는 걍 지우면 됨.</li></ul>]]></content:encoded>
      
      <comments>https://katie0809.github.io/2020/02/23/nuxt-study1/#disqus_thread</comments>
    </item>
    
    <item>
      <title>[딥러닝 스터디] Attention을 활용한 기계번역</title>
      <link>https://katie0809.github.io/2020/02/23/ai-study7/</link>
      <guid>https://katie0809.github.io/2020/02/23/ai-study7/</guid>
      <pubDate>Sun, 23 Feb 2020 07:54:46 GMT</pubDate>
      <description>
      
        &lt;p&gt;다음의 책을 공부하며 정리한 내용입니다.&lt;/p&gt;
&lt;ul&gt;
&lt;li&gt;&lt;a href=&quot;https://wikidocs.net/24996&quot; rel=&quot;external nofollow noopener noreferrer&quot; target=&quot;_blank&quot;&gt;https://wikidocs.net/24996&lt;/a&gt; : seq2seq 정리&lt;/li&gt;
&lt;li&gt;&lt;a href=&quot;https://wikidocs.net/22893&quot; rel=&quot;external nofollow noopener noreferrer&quot; target=&quot;_blank&quot;&gt;https://wikidocs.net/22893&lt;/a&gt; : 어텐션 모델 정리&lt;/li&gt;
&lt;li&gt;&lt;a href=&quot;https://www.youtube.com/watch?v=c8y9ZAb9aks&amp;amp;t=1032s&quot; rel=&quot;external nofollow noopener noreferrer&quot; target=&quot;_blank&quot;&gt;https://www.youtube.com/watch?v=c8y9ZAb9aks&amp;amp;t=1032s&lt;/a&gt; : seq2seq에서 attention까지(매우 좋음, 꼭 참고하세요)&lt;/li&gt;
&lt;li&gt;&lt;a href=&quot;https://www.tensorflow.org/tutorials/text/nmt_with_attention&quot; rel=&quot;external nofollow noopener noreferrer&quot; target=&quot;_blank&quot;&gt;https://www.tensorflow.org/tutorials/text/nmt_with_attention&lt;/a&gt; : attention 실습&lt;/li&gt;
&lt;/ul&gt;
&lt;h1 id=&quot;시퀀스-투-시퀀스-seq2seq&quot;&gt;&lt;a href=&quot;#시퀀스-투-시퀀스-seq2seq&quot; class=&quot;headerlink&quot; title=&quot;시퀀스-투-시퀀스(seq2seq)&quot;&gt;&lt;/a&gt;시퀀스-투-시퀀스(seq2seq)&lt;/h1&gt;&lt;p&gt;: 입력된 시퀀스로부터 다른 도메인의 시퀀스를 출력하는 모델.&lt;/p&gt;
&lt;p&gt;이는 다음과 같은 분야에서 사용된다.&lt;/p&gt;
&lt;ul&gt;
&lt;li&gt;챗봇: 입력시퀀스와 출력시퀀스를 각각 질문/대답으로 구성하면 챗봇을 만들 수 있다.&lt;/li&gt;
&lt;li&gt;기계번역: 입력시퀀스와 출력시퀀스를 입력/번역문장으로 구성하면 번역기를 만들 수 있다.&lt;/li&gt;
&lt;li&gt;Text Summerization, Speech to Text 등에 사용될 수 있다.&lt;/li&gt;
&lt;/ul&gt;
      
      </description>
      
      
      <content:encoded><![CDATA[<p>다음의 책을 공부하며 정리한 내용입니다.</p><ul><li><a href="https://wikidocs.net/24996" rel="external nofollow noopener noreferrer" target="_blank">https://wikidocs.net/24996</a> : seq2seq 정리</li><li><a href="https://wikidocs.net/22893" rel="external nofollow noopener noreferrer" target="_blank">https://wikidocs.net/22893</a> : 어텐션 모델 정리</li><li><a href="https://www.youtube.com/watch?v=c8y9ZAb9aks&amp;t=1032s" rel="external nofollow noopener noreferrer" target="_blank">https://www.youtube.com/watch?v=c8y9ZAb9aks&amp;t=1032s</a> : seq2seq에서 attention까지(매우 좋음, 꼭 참고하세요)</li><li><a href="https://www.tensorflow.org/tutorials/text/nmt_with_attention" rel="external nofollow noopener noreferrer" target="_blank">https://www.tensorflow.org/tutorials/text/nmt_with_attention</a> : attention 실습</li></ul><h1 id="시퀀스-투-시퀀스-seq2seq"><a href="#시퀀스-투-시퀀스-seq2seq" class="headerlink" title="시퀀스-투-시퀀스(seq2seq)"></a>시퀀스-투-시퀀스(seq2seq)</h1><p>: 입력된 시퀀스로부터 다른 도메인의 시퀀스를 출력하는 모델.</p><p>이는 다음과 같은 분야에서 사용된다.</p><ul><li>챗봇: 입력시퀀스와 출력시퀀스를 각각 질문/대답으로 구성하면 챗봇을 만들 수 있다.</li><li>기계번역: 입력시퀀스와 출력시퀀스를 입력/번역문장으로 구성하면 번역기를 만들 수 있다.</li><li>Text Summerization, Speech to Text 등에 사용될 수 있다.</li></ul><a id="more"></a><p><img src="https://wikidocs.net/images/page/24996/%EC%9D%B8%EC%BD%94%EB%8D%94%EB%94%94%EC%BD%94%EB%8D%94%EB%AA%A8%EB%8D%B8.PNG" alt></p><p>seq2seq 모델은 기본적으로 위의 구조를 띄고 있다.</p><ul><li>인코더와 디코더는 두개의 RNN 아키텍처이다. 입력 문장을 처리하는 RNN셀을 인코더, 출력 문장(번역된 문장)을 처리하는 RNN셀을 디코더라고 하는 것.</li><li>중간의 컨텍스트 벡터는 인코더 마지막 시점의 히든 스테이트의 크기이다. 즉, 이전의 내용이 함축된 하나의 벡터이다.</li></ul><p>이때 디코더에서 단순히 매 단계마다 가장 가능성이 높은 단어 하나를 선택하는 방식은 생각보다 효율적이지 않다. 이때 적용하는 방법이 Beam search.</p><blockquote><p>Beam search : 매 스텝마다 가장 확률이 높은 n개의 단어를 선택하여, 이 N개의 단어 각각에 대해 다음 스텝에서 등장할 수 있는 모든 단어들의 확률을 예측한다. 이러한 방식으로 <strong>매 스텝마다 n개의 후보군을 유지</strong>하여 최적의 시퀀스 후보를 뽑아낸다.</p></blockquote><hr><h1 id="Attention-model"><a href="#Attention-model" class="headerlink" title="Attention model"></a>Attention model</h1><p><img src="https://www.tensorflow.org/images/seq2seq/attention_mechanism.jpg" alt></p><ul><li><p>seq2seq 모델은 기본적으로 <code>인코더 -&gt; [컨텍스트 벡터] -&gt; 디코더</code> 의 구조를 갖는다.</p></li><li><p>이러한 모델의 문제는 아래와 같다</p><ul><li>컨텍스트 벡터는 결국 <code>하나의 벡터</code>에 불과하다. 인코더의 모든 정보를 하나의 고정된 크기의 벡터에 압축하다 보면 필연적으로 <strong><code>정보의 손실</code></strong>이 발생하게 된다.</li><li>RNN의 고질적인 Vanishing gradient 문제가 발생한다.</li></ul><p>: 이러한 문제는 결과적으로 <u>입력 시퀀스가 길어질수록 번역의 품질이 저하</u>되는 문제를 야기한다.</p></li></ul><h2 id="Attention-Overview"><a href="#Attention-Overview" class="headerlink" title="Attention Overview"></a>Attention Overview</h2><p>어텐션 모델의 기본 아이디어는 다음과 같다.</p><blockquote><p>디코더의 매 time step마다 인코더에서의 <code>전체 입력문장을 다시 한번 참고</code>한다.</p></blockquote><ul><li>이때 입력 문장의 전체 토큰을 동일한 비중으로 참고하는 것이 아니라, 해당 시점에서 예측해야할 단어와 <em>연관이 있는 입력토큰</em> 부분을 좀더 집중(<strong>attention</strong>)해서 참고한다. </li></ul><h2 id="Dot-product-attention"><a href="#Dot-product-attention" class="headerlink" title="Dot product attention"></a>Dot product attention</h2><p>어텐션은 다양한 종류가 있다. 그 중 가장 기본적인 닷 프로덕트 어텐션의 구조를 살펴보자.</p><p>먼저 기본적인 용어 정의, seq2seq모델과 어텐션 모델의 차이점을 알아보자.</p><ul><li><strong>1, 2, 3… n</strong> : 인코더의 시점</li><li><strong>h1, h2, h3… hn</strong> : 각 시점에서의 인코더의 은닉 상태(hidden state)</li><li><strong>t</strong> : 디코더의 현재시점</li><li><strong>st</strong> : 현재 시점에서의 디코더의 은닉 상태</li></ul><p>이전에 배웠던 seq2seq에서 디코더는 두개의 값(<u>이전시점의 은닉상태, 이전시점의 출력</u>)을 통해 현재시점의 은닉상태를 계산했다. 이때 어텐션 모델에서는 계산을 위해 필요한 값이 하나 더 추가된다. 바로 <strong><code>t시점의 어텐션 값 at</code></strong>이다.</p><p>따라서 <u>어텐션 모델은 (seq2seq + 어텐션 값 계산) 인 모델</u>이다. 즉, 어텐션 모델에서의 핵심은 이 <strong>어텐션 값을 어떻게 구하는가</strong>이며, 이 과정에서 어텐션 스코어값을 구하는 방법에 따라 닷 프로덕트 어텐션, 루옹 어텐션, 바다나우 어텐션 등으로 종류가 나뉘게 된다.</p><p>또한 모든 어텐션 값 at는(“값”이라는 명칭에서 예상할 수 있듯) 스칼라 값이다.</p><h3 id="Step-1-어텐션-스코어를-구한다"><a href="#Step-1-어텐션-스코어를-구한다" class="headerlink" title="Step 1. 어텐션 스코어를 구한다."></a>Step 1. 어텐션 스코어를 구한다.</h3><p><img src="https://wikidocs.net/images/page/22893/dotproductattention2_final.PNG" alt></p><p>어텐션 스코어는 다음을 의미한다.</p><blockquote><p><strong>Attention score</strong> : 인코더의 <strong>각 은닉상태 h1 - hn</strong>이 현재 시점의 디코더 은닉상태 <strong>st</strong>와 <code>얼마나 유사한지</code>의 정도</p></blockquote><p>닷 프로덕트 어텐션에서는 이 스코어 값을 구하기 위해 <u>st와 hi(i : 1~n)를 닷 프로덕트 한다</u>. 이때 둘다 열벡터이므로 디코더의 은닉상태 st값을 전치하여 내적한다.</p><p>따라서 dot product attention의 attention score 함수 수식은 다음과 같다.</p><blockquote><p>score(st, hi) = stT * hi</p></blockquote><p>따라서 디코더의 현재 시점 t에 대한 은닉상태 st와 인코더의 모든 시점에 대한 은닉상태의 어텐션 스코어의 모음(et)은 아래와 같다.</p><blockquote><p>et = [stT * h1, …, stT * hN]</p></blockquote><h3 id="Step-2-Softmax를-통해-Attention-Distribution-어텐션-분포-를-구한다"><a href="#Step-2-Softmax를-통해-Attention-Distribution-어텐션-분포-를-구한다" class="headerlink" title="Step 2. Softmax를 통해 Attention Distribution(어텐션 분포)를 구한다."></a>Step 2. Softmax를 통해 Attention Distribution(어텐션 분포)를 구한다.</h3><p><img src="https://wikidocs.net/images/page/22893/dotproductattention3_final.PNG" alt></p><p>구해낸 모든 어텐션 스코어의 모음, <u>et에 <strong>softmax</strong>를 적용해 확률 분포를 얻어낸다</u>. </p><p>이를 통해 얻어낸 분포를 <code>Attention Distribution</code>(어텐션 분포)라 하며, 각각의 값을 <code>Attention Weight</code>(어텐션 가중치)라고 한다.</p><p><strong>어텐션 분포와 어텐션 값</strong>은 <code>디코더의 현재시점 t에 대해 정의</code>된다.</p><blockquote><p>Attention Distribution : 어텐션 스코어의 모음에 softmax를 적용해 얻어낸 확률분포. 어텐션 가중치의 모음값</p><p>Attention Weight : 어텐션 분포의 각각의 값</p></blockquote><p>따라서 어텐션 분포를 αt의 식은 다음과 같다.</p><blockquote><p>αt = softmax(et)</p></blockquote><h3 id="Step-3-Attention-Weight과-인코더-은닉상태를-가중합하여-Attention-Value를-구한다"><a href="#Step-3-Attention-Weight과-인코더-은닉상태를-가중합하여-Attention-Value를-구한다" class="headerlink" title="Step 3. Attention Weight과 인코더 은닉상태를 가중합하여 Attention Value를 구한다."></a>Step 3. Attention Weight과 인코더 은닉상태를 가중합하여 Attention Value를 구한다.</h3><p><img src="https://wikidocs.net/images/page/22893/dotproductattention4_final.PNG" alt></p><p>구해낸 Attention Distribution의 각 <strong>Attention Weight</strong>들을 <strong>해당 시점의 인코더 은닉상태(h1 ~ hN)</strong>와 <code>가중합(Weighted sum)</code> 한다. </p><p><strong>결과로 나오는 벡터 a</strong>는 최종 어텐션 값, 즉 <u>Attention Value</u>가 되며 이는 인코더의 문맥을 내포하고 있다는 의미에서 <u>Context Vector</u>라고 부르기도 한다.</p><blockquote><p>a = ∑αti * hi (i : 1~N, ati : i번째 어텐션 분포의 값)</p></blockquote><h3 id="Step-4-어텐션-값과-현재-상태-대코더의-은닉상태-st를-연결한다-Concatenation"><a href="#Step-4-어텐션-값과-현재-상태-대코더의-은닉상태-st를-연결한다-Concatenation" class="headerlink" title="Step 4. 어텐션 값과 현재 상태 대코더의 은닉상태 st를 연결한다(Concatenation)."></a>Step 4. 어텐션 값과 현재 상태 대코더의 은닉상태 st를 연결한다(Concatenation).</h3><p><img src="https://wikidocs.net/images/page/22893/dotproductattention5_final_final.PNG" alt></p><p>최종적으로 구해낸 어텐션 값(컨텍스트 벡터) a를 현재 시점의 디코더 은닉상태 st와 결합한다. 이때 둘을 연결해 하나의 벡터로 만드는(concatenation) 작업을 수행한다.</p><p>해당 결합 작업을 통해 산출된 최종 벡터 vt는 t시점의 디코더 예측값 y_hat를 도출하기 위한 연산의 입력값으로 사용된다.</p><hr><h1 id="Bahdanau-Attention-바다나우-어텐션"><a href="#Bahdanau-Attention-바다나우-어텐션" class="headerlink" title="Bahdanau Attention(바다나우 어텐션)"></a>Bahdanau Attention(바다나우 어텐션)</h1><p>: 위에서 어텐션의 종류는 step 1의 어텐션 스코어를 구하는 방법에 따라 달라진다고 했다. </p><p>닷 프로덕트 어텐션은 인코더의 각 은닉상태 h1 ~ hN과 현재시점 디코더 은닉상태 st의 유사도인 어텐션 스코어를 내적으로 구하였기 때문에 닷 프로덕트 어텐션이라는 이름이 붙었다.</p><p>이때 현재시점 디코더의 은닉상태를 query로, 이전 인코더의 모든 은닉상태를 key라고 한다면 닷 프로덕트 어텐션의 스코어 함수는 아래와 같을 것이다.</p><blockquote><p>score(query, key) = queryT * key</p></blockquote><p>이때 바다나우 어텐션은 아래와 같은 스코어 함수를 사용한다.</p><blockquote><p>score(query, key) = vT * tanh(W1 * key + W2 * query)</p><p>: vT는 transposed weight vector</p></blockquote><h1 id="Neural-machine-translation-with-attention"><a href="#Neural-machine-translation-with-attention" class="headerlink" title="Neural machine translation with attention"></a>Neural machine translation with attention</h1><ul><li>스페인어에서 영어로 기계번역을 수행하는 seq2seq 모델을 직접 구현해본다.</li></ul><h2 id="데이터셋-다운로드"><a href="#데이터셋-다운로드" class="headerlink" title="데이터셋 다운로드"></a>데이터셋 다운로드</h2>]]></content:encoded>
      
      <comments>https://katie0809.github.io/2020/02/23/ai-study7/#disqus_thread</comments>
    </item>
    
    <item>
      <title>[딥러닝 스터디] 케라스(Keras) 실습</title>
      <link>https://katie0809.github.io/2020/02/19/ai-study6/</link>
      <guid>https://katie0809.github.io/2020/02/19/ai-study6/</guid>
      <pubDate>Wed, 19 Feb 2020 01:37:17 GMT</pubDate>
      <description>
      
        &lt;p&gt;다음의 책을 공부하며 정리한 내용입니다.&lt;/p&gt;
&lt;ul&gt;
&lt;li&gt;&lt;a href=&quot;https://wikidocs.net/48649&quot; rel=&quot;external nofollow noopener noreferrer&quot; target=&quot;_blank&quot;&gt;https://wikidocs.net/48649&lt;/a&gt;&lt;/li&gt;
&lt;li&gt;&lt;a href=&quot;https://wikidocs.net/32105&quot; rel=&quot;external nofollow noopener noreferrer&quot; target=&quot;_blank&quot;&gt;https://wikidocs.net/32105&lt;/a&gt;&lt;/li&gt;
&lt;/ul&gt;
&lt;p&gt;&lt;a href=&quot;https://keras.io/&quot; rel=&quot;external nofollow noopener noreferrer&quot; target=&quot;_blank&quot;&gt;케라스 공식문서&lt;/a&gt;&lt;/p&gt;
&lt;p&gt;딥러닝 라이브러리 케라스의 사용법을 익히고 실제 RNN모델을 설계해본다. 케라스는 딥러닝을 도와주는 파이썬 라이브러리이다.&lt;/p&gt;
      
      </description>
      
      
      <content:encoded><![CDATA[<p>다음의 책을 공부하며 정리한 내용입니다.</p><ul><li><a href="https://wikidocs.net/48649" rel="external nofollow noopener noreferrer" target="_blank">https://wikidocs.net/48649</a></li><li><a href="https://wikidocs.net/32105" rel="external nofollow noopener noreferrer" target="_blank">https://wikidocs.net/32105</a></li></ul><p><a href="https://keras.io/" rel="external nofollow noopener noreferrer" target="_blank">케라스 공식문서</a></p><p>딥러닝 라이브러리 케라스의 사용법을 익히고 실제 RNN모델을 설계해본다. 케라스는 딥러닝을 도와주는 파이썬 라이브러리이다.</p><a id="more"></a><h1 id="케라스-훑어보기"><a href="#케라스-훑어보기" class="headerlink" title="케라스 훑어보기"></a>케라스 훑어보기</h1><h2 id="전처리-도구"><a href="#전처리-도구" class="headerlink" title="전처리 도구"></a>전처리 도구</h2><h3 id="Tokenizer"><a href="#Tokenizer" class="headerlink" title="Tokenizer"></a>Tokenizer</h3><h1 id="글자-단위-RNN-Char-RNN"><a href="#글자-단위-RNN-Char-RNN" class="headerlink" title="글자 단위 RNN(Char RNN)"></a>글자 단위 RNN(Char RNN)</h1><p>: 입출력의 단위가 글자인 RNN을 케라스로 구현하여 언어모델의 훈련/테스트 과정을 이해한다.</p>]]></content:encoded>
      
      <comments>https://katie0809.github.io/2020/02/19/ai-study6/#disqus_thread</comments>
    </item>
    
    <item>
      <title>[딥러닝 스터디] 자연어 전처리 실습</title>
      <link>https://katie0809.github.io/2020/02/17/ai-study5/</link>
      <guid>https://katie0809.github.io/2020/02/17/ai-study5/</guid>
      <pubDate>Mon, 17 Feb 2020 12:58:36 GMT</pubDate>
      <description>
      
        &lt;p&gt;자연어 처리 라이브러리인 토치텍스트를 활용해 이전 포스트의 &lt;u&gt;전처리 이론을 실제로 구현&lt;/u&gt;해보자.&lt;/p&gt;
&lt;h2 id=&quot;토치텍스트&quot;&gt;&lt;a href=&quot;#토치텍스트&quot; class=&quot;headerlink&quot; title=&quot;토치텍스트&quot;&gt;&lt;/a&gt;토치텍스트&lt;/h2&gt;&lt;p&gt;: 텍스트에 대한 여러 추상화 기능을 제공하는 &lt;strong&gt;자연어 처리 라이브러리&lt;/strong&gt;&lt;/p&gt;
      
      </description>
      
      
      <content:encoded><![CDATA[<p>자연어 처리 라이브러리인 토치텍스트를 활용해 이전 포스트의 <u>전처리 이론을 실제로 구현</u>해보자.</p><h2 id="토치텍스트"><a href="#토치텍스트" class="headerlink" title="토치텍스트"></a>토치텍스트</h2><p>: 텍스트에 대한 여러 추상화 기능을 제공하는 <strong>자연어 처리 라이브러리</strong></p><a id="more"></a><h3 id="토치텍스트-제공-기능"><a href="#토치텍스트-제공-기능" class="headerlink" title="토치텍스트 제공 기능"></a>토치텍스트 제공 기능</h3><ol><li>파일 로드하기(File Loading) : 다양한 포맷의 <code>코퍼스를 로드</code>한다.</li><li><code>토큰화</code>(Tokenization) : 문장을 단어 단위로 분리한다.</li><li><code>단어 집합 생성</code>(Vocab) : 단어 집합을 만든다.</li><li><code>정수 인코딩</code>(Integer encoding) : 전체 코퍼스의 단어들을 각각의 고유한 정수로 맵핑한다.</li><li>단어 벡터(Word Vector) : 단어 집합의 단어들에 고유한 <code>임베딩</code> 벡터를 만들어줍니다. 랜덤값으로 초기화한 값일 수도 있고, 사전 훈련된 임베딩 벡터들을 로드할 수도 있습니다.</li><li><code>배치</code>화(Batching) : 훈련 샘플들의 배치를 만들어줍니다. 이 과정에서 패딩 작업(Padding)도 이루어집니다.</li></ol><h3 id="실습-1-IMDB-리뷰데이터-분류하기-영어"><a href="#실습-1-IMDB-리뷰데이터-분류하기-영어" class="headerlink" title="실습 1. IMDB 리뷰데이터 분류하기(영어)"></a>실습 1. IMDB 리뷰데이터 분류하기(영어)</h3><h4 id="데이터-다운-및-용도에-따른-분류-진행-훈련-테스트"><a href="#데이터-다운-및-용도에-따른-분류-진행-훈련-테스트" class="headerlink" title="데이터 다운 및 용도에 따른 분류 진행(훈련/테스트)"></a>데이터 다운 및 용도에 따른 분류 진행(훈련/테스트)</h4><ul><li>데이터는 text(리뷰데이터)와 sentiment(리뷰의 긍정:1/부정:0 여부)</li></ul><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br></pre></td><td class="code"><pre><span class="line"><span class="comment"># 토치텍스트 설치</span></span><br><span class="line">pip install torchtext</span><br><span class="line"></span><br><span class="line"><span class="keyword">import</span> urllib.request</span><br><span class="line"><span class="keyword">import</span> pandas <span class="keyword">as</span> pd</span><br><span class="line"></span><br><span class="line"><span class="comment"># IMDB 리뷰 데이터 다운로드</span></span><br><span class="line">urllib.request.urlretrieve(<span class="string">"https://raw.githubusercontent.com/LawrenceDuan/IMDb-Review-Analysis/master/IMDb_Reviews.csv"</span>, filename=<span class="string">"IMDb_Reviews.csv"</span>)</span><br><span class="line"></span><br><span class="line"><span class="comment"># 데이터 csv파일로 저장, 상위 5개 행 출력해본다</span></span><br><span class="line">df = pd.read_csv(<span class="string">'IMDb_Reviews.csv'</span>, encoding=<span class="string">'latin1'</span>)</span><br><span class="line">df.head()</span><br><span class="line"></span><br><span class="line"><span class="comment"># 훈련 데이터와 테스트 데이터로 분류(총 5만개)</span></span><br><span class="line">train_df = df[:<span class="number">40000</span>]</span><br><span class="line">test_df = df[<span class="number">40001</span>:]</span><br><span class="line"></span><br><span class="line"><span class="comment"># 각 데이터를 csv 파일로 저장 (index=False :: 인덱스를 저장하지 않음)</span></span><br><span class="line">train_df.to_csv(<span class="string">"train_data.csv"</span>, index=<span class="literal">False</span>)</span><br><span class="line">test_df.to_csv(<span class="string">"test_data.csv"</span>, index=<span class="literal">False</span>)</span><br></pre></td></tr></table></figure><h4 id="필드-정의하기-torchtext-data"><a href="#필드-정의하기-torchtext-data" class="headerlink" title="필드 정의하기(torchtext.data)"></a>필드 정의하기(torchtext.data)</h4><blockquote><p>torchtext.data 의 Field함수를 활용해 진행할 자연어 전처리를 정의할 수 있다.</p></blockquote><ul><li>sequential : 순차적인 데이터 여부. (True가 기본값) LABEL은 긍정/부정의 단순한 클래스를 나타내는 숫자값이지 순차적 데이터가 아니므로 False이다.</li><li><code>use_vocab</code> : 단어 집합을 만들 것인지 여부. (True가 기본값)</li><li><code>tokenize</code> : 어떤 토큰화 함수를 사용할 것인지 지정. (string.split이 기본값)</li><li>lower : 영어 데이터를 전부 소문자화한다. (False가 기본값)</li><li><code>batch_first</code> : 신경망에 입력되는 텐서의 첫번째 차원값이 batch_size가 되도록 한다. (False가 기본값)</li><li>is_target : 레이블 데이터 여부. (False가 기본값)</li><li><code>fix_length</code> : 최대 허용 길이. 이 길이에 맞춰서 패딩 작업(Padding)이 진행된다.</li></ul><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br></pre></td><td class="code"><pre><span class="line"><span class="keyword">from</span> torchtext.data <span class="keyword">import</span> TabularDataset</span><br><span class="line"></span><br><span class="line"><span class="comment"># 경로, 훈련데이터, 테스트데이터, 데이터포멧, 텍스트객체</span></span><br><span class="line">train_data, test_data = TabularDataset.splits(</span><br><span class="line">        path=<span class="string">'.'</span>, train=<span class="string">'train_data.csv'</span>, test=<span class="string">'test_data.csv'</span>, format=<span class="string">'csv'</span>,</span><br><span class="line">        fields=[(<span class="string">'text'</span>, TEXT), (<span class="string">'label'</span>, LABEL)], skip_header=<span class="literal">True</span>)</span><br><span class="line"></span><br><span class="line"><span class="comment"># 훈련 데이터의 샘플을 확인한다.</span></span><br><span class="line">print(vars(train_data[<span class="number">1</span>])[<span class="string">'text'</span>])</span><br><span class="line">print(vars(train_data[<span class="number">1</span>])[<span class="string">'label'</span>])</span><br></pre></td></tr></table></figure><figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br></pre></td><td class="code"><pre><span class="line">[결과]</span><br><span class="line">[&#39;believe&#39;, &#39;it&#39;, &#39;or&#39;, &#39;not,&#39;, &#39;this&#39;, &#39;was&#39;, &#39;at&#39;, &#39;one&#39;, &#39;time&#39;, &#39;the&#39;, &#39;worst&#39;, &#39;movie&#39;, &#39;i&#39;, &#39;had&#39;, &#39;ever&#39;, &#39;seen.&#39;, &#39;since&#39;, &#39;that&#39;, &#39;time,&#39;, &#39;i&#39;, &#39;have&#39;, &#39;seen&#39;, &#39;many&#39;, &#39;more&#39;, &#39;movies&#39;, &#39;that&#39;, &#39;are&#39;, &#39;worse&#39;, &#39;(how&#39;, &#39;is&#39;, &#39;it&#39;, &#39;possible??)&#39;, &#39;therefore,&#39;, &#39;to&#39;, &#39;be&#39;, &#39;fair,&#39;, &#39;i&#39;, &#39;had&#39;, &#39;to&#39;, &#39;give&#39;, &#39;this&#39;, &#39;movie&#39;, &#39;a&#39;, &#39;2&#39;, &#39;out&#39;, &#39;of&#39;, &#39;10.&#39;, &#39;but&#39;, &#39;it&#39;, &#39;was&#39;, &#39;a&#39;, &#39;tough&#39;, &#39;call.&#39;]</span><br><span class="line">0</span><br></pre></td></tr></table></figure><h4 id="단어집합을-생성한다"><a href="#단어집합을-생성한다" class="headerlink" title="단어집합을 생성한다."></a>단어집합을 생성한다.</h4><ul><li>전체 리뷰의 단어들 내에서 중복을 제거한 단어집합을 생성한다.</li><li>각 단어에 고유한 정수를 부여한다(정수 인코딩)<ul><li>정의한 필드 객체의 <code>.build_vocab()</code> 함수를 활용해 단어집합을 생성할 수 있다.</li></ul></li></ul><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br></pre></td><td class="code"><pre><span class="line"><span class="comment"># 리뷰 데이터의 단어집합을 만든다.</span></span><br><span class="line"><span class="comment"># min_freq : 단어집합에 추가되기 위한 최소 등장빈도조건</span></span><br><span class="line"><span class="comment"># max_size : 단어집합의 최대 크기</span></span><br><span class="line">TEXT.build_vocab(train_data, min_freq=<span class="number">10</span>, max_size=<span class="number">10000</span>)</span><br><span class="line"></span><br><span class="line">print(<span class="string">'Size of vocab : '</span>, len(TEXT.vocab))</span><br><span class="line">print(<span class="string">'Integer index of word [the] : '</span>, TEXT.vocab.stoi[<span class="string">'the'</span>])</span><br></pre></td></tr></table></figure><figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br></pre></td><td class="code"><pre><span class="line">[결과]</span><br><span class="line">Size of vocab :  10002</span><br><span class="line">Integer index of word [the] :  2</span><br></pre></td></tr></table></figure><ul><li>이때 단어집합의 크기는 기존에 정의한 10000이 아닌 10002임을 알 수 있다. 더해진 두개는 토치텍스트가 자동으로 추가한 특별토큰 <strong><em>unk\</em></strong>와 <strong><em>pad\</em></strong>이다.</li><li>unk는 0, pad는 1의 정수가 부여된다.</li></ul><p>(+) 데이터로더 만들기 : 특정 배치크기로 데이터를 로드하도록 하는 데이터 로더를 만든다. torchtext.data의 Iterator를 사용해 만들 수 있다.</p><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br></pre></td><td class="code"><pre><span class="line"><span class="keyword">from</span> torchtext.data <span class="keyword">import</span> Iterator</span><br><span class="line"></span><br><span class="line"><span class="comment"># 배치사이즈 100으로 훈련데이터에 대한 데이터로더를 만든다</span></span><br><span class="line">train_loader = Iterator(dataset=train_data, batch_size=<span class="number">100</span>)</span><br><span class="line">print(<span class="string">"len of train data : "</span>, len(train_data), <span class="string">" | num of batches : "</span>, len(train_loader))</span><br></pre></td></tr></table></figure><figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br></pre></td><td class="code"><pre><span class="line">[결과]</span><br><span class="line">len of train data :  40000  | num of batches :  400</span><br></pre></td></tr></table></figure><hr><h3 id="실습-2-네이버-영화데이터-분류하기-한국어"><a href="#실습-2-네이버-영화데이터-분류하기-한국어" class="headerlink" title="실습 2. 네이버 영화데이터 분류하기(한국어)"></a>실습 2. 네이버 영화데이터 분류하기(한국어)</h3><p>: 이전 IMDB데이터를 토치텍스트로 전처리 한 것과 마찬가지의 과정으로 진행한다.</p><ol><li>네이버 영화리뷰데이터 다운</li><li>훈련데이터/테스트 데이터로 분류</li><li>필드 정의하기(전처리 방식 지정)</li><li>데이터셋 제작하기(전처리 수행)</li><li>단어집합 생성/정수인코딩 수행</li></ol><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br><span class="line">23</span><br><span class="line">24</span><br><span class="line">25</span><br><span class="line">26</span><br><span class="line">27</span><br><span class="line">28</span><br><span class="line">29</span><br><span class="line">30</span><br><span class="line">31</span><br><span class="line">32</span><br><span class="line">33</span><br><span class="line">34</span><br><span class="line">35</span><br><span class="line">36</span><br><span class="line">37</span><br><span class="line">38</span><br><span class="line">39</span><br><span class="line">40</span><br><span class="line">41</span><br><span class="line">42</span><br><span class="line">43</span><br><span class="line">44</span><br><span class="line">45</span><br><span class="line">46</span><br><span class="line">47</span><br></pre></td><td class="code"><pre><span class="line"><span class="comment"># 한국어 형태소 분석기 Mecab 설치</span></span><br><span class="line">!git clone https://github.com/SOMJANG/Mecab-ko-<span class="keyword">for</span>-Google-Colab.git</span><br><span class="line">%cd Mecab-ko-<span class="keyword">for</span>-Google-Colab</span><br><span class="line">!bash install_mecab-ko_on_colab190912.sh</span><br><span class="line"></span><br><span class="line"><span class="comment"># 네이버 영화 리뷰데이터 다운</span></span><br><span class="line"><span class="keyword">import</span> urllib.request</span><br><span class="line"><span class="keyword">import</span> pandas <span class="keyword">as</span> pd</span><br><span class="line"></span><br><span class="line">urllib.request.urlretrieve(<span class="string">"https://raw.githubusercontent.com/e9t/nsmc/master/ratings_train.txt"</span>, filename=<span class="string">"ratings_train.txt"</span>)</span><br><span class="line">urllib.request.urlretrieve(<span class="string">"https://raw.githubusercontent.com/e9t/nsmc/master/ratings_test.txt"</span>, filename=<span class="string">"ratings_test.txt"</span>)</span><br><span class="line"></span><br><span class="line"><span class="comment"># 훈련 데이터와 테스트 데이터로 분리한다.</span></span><br><span class="line">train_data = pd.read_table(<span class="string">'ratings_train.txt'</span>)</span><br><span class="line">test_data = pd.read_table(<span class="string">'ratings_test.txt'</span>)</span><br><span class="line"></span><br><span class="line"><span class="keyword">from</span> torchtext <span class="keyword">import</span> data</span><br><span class="line"><span class="keyword">from</span> konlpy.tag <span class="keyword">import</span> Mecab</span><br><span class="line"></span><br><span class="line"><span class="comment"># 필드 정의하기(전처리 방식 지정)</span></span><br><span class="line">TEXT = data.Field(sequential=<span class="literal">True</span>,</span><br><span class="line">                  use_vocab=<span class="literal">True</span>, <span class="comment"># 단어집합을 만든다</span></span><br><span class="line">                  tokenize=Mecab().morphs, <span class="comment"># 토크나이저로는 Mecab 사용.</span></span><br><span class="line">                  lower=<span class="literal">True</span>,</span><br><span class="line">                  batch_first=<span class="literal">True</span>,</span><br><span class="line">                  fix_length=<span class="number">20</span>) <span class="comment"># 패딩 길이는 20</span></span><br><span class="line"></span><br><span class="line">LABEL = data.Field(sequential=<span class="literal">False</span>,</span><br><span class="line">                   use_vocab=<span class="literal">False</span>,</span><br><span class="line">                   is_target=<span class="literal">True</span>)</span><br><span class="line"></span><br><span class="line"><span class="comment"># ID필드는 사용하지 않는다. </span></span><br><span class="line"><span class="comment"># 디민 TabularDataset.splits()은 받은 데이터를 앞에서부터 순서대로 자르므로 필요함.</span></span><br><span class="line"><span class="comment"># 네이버 영화리뷰 데이터는 [리뷰아이디, 리뷰, 라벨] 세가지로 이뤄져있기 때문</span></span><br><span class="line">ID = data.Field(sequential=<span class="literal">False</span>,  </span><br><span class="line">                   use_vocab=<span class="literal">False</span>,)</span><br><span class="line">                   </span><br><span class="line"><span class="keyword">from</span> torchtext.data <span class="keyword">import</span> TabularDataset</span><br><span class="line"></span><br><span class="line"><span class="comment"># 데이터셋 제작하기(전처리 수행)</span></span><br><span class="line">train_data, test_data = TabularDataset.splits(</span><br><span class="line">        path=<span class="string">'.'</span>, train=<span class="string">'ratings_train.txt'</span>, test=<span class="string">'ratings_test.txt'</span>, format=<span class="string">'tsv'</span>,</span><br><span class="line">        fields=[(<span class="string">'id'</span>, ID), (<span class="string">'text'</span>, TEXT), (<span class="string">'label'</span>, LABEL)], skip_header=<span class="literal">True</span>)</span><br><span class="line"></span><br><span class="line">print(<span class="string">'훈련 샘플의 개수 : &#123;&#125;'</span>.format(len(train_data)))</span><br><span class="line">print(<span class="string">'테스트 샘플의 개수 : &#123;&#125;'</span>.format(len(test_data)))</span><br><span class="line">print(<span class="string">'훈련 데이터 예제 : &#123;&#125;'</span>.format(vars(train_data[<span class="number">0</span>])))</span><br></pre></td></tr></table></figure><figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br></pre></td><td class="code"><pre><span class="line">[결과]</span><br><span class="line">훈련 샘플의 개수 : 150000</span><br><span class="line">테스트 샘플의 개수 : 50000</span><br><span class="line">훈련 데이터 예제 : &#123;&#39;id&#39;: &#39;9976970&#39;, &#39;text&#39;: [&#39;아&#39;, &#39;더&#39;, &#39;빙&#39;, &#39;.&#39;, &#39;.&#39;, &#39;진짜&#39;, &#39;짜증&#39;, &#39;나&#39;, &#39;네요&#39;, &#39;목소리&#39;], &#39;label&#39;: &#39;0&#39;&#125;</span><br></pre></td></tr></table></figure><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br></pre></td><td class="code"><pre><span class="line"><span class="comment"># 단어집합을 생성한다(정수인코딩 수행)</span></span><br><span class="line">TEXT.build_vocab(train_data, min_freq=<span class="number">10</span>, max_size=<span class="number">10000</span>)</span><br><span class="line"></span><br><span class="line"><span class="comment"># 생성된 단어집합 내 단어 확인해보기</span></span><br><span class="line">print(<span class="string">'단어 "좋아"의 인덱스는 [&#123;&#125;]'</span>.format(TEXT.vocab.stoi[<span class="string">'좋아'</span>]))</span><br></pre></td></tr></table></figure><figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br></pre></td><td class="code"><pre><span class="line">[결과]</span><br><span class="line">단어 &quot;좋아&quot;의 인덱스는 [8343]</span><br></pre></td></tr></table></figure>]]></content:encoded>
      
      <comments>https://katie0809.github.io/2020/02/17/ai-study5/#disqus_thread</comments>
    </item>
    
    <item>
      <title>[딥러닝 스터디] 자연어 처리의 전처리</title>
      <link>https://katie0809.github.io/2020/02/17/ai-study4/</link>
      <guid>https://katie0809.github.io/2020/02/17/ai-study4/</guid>
      <pubDate>Mon, 17 Feb 2020 11:44:47 GMT</pubDate>
      <description>
      
        &lt;p&gt;다음의 책을 공부하며 정리한 내용입니다.&lt;/p&gt;
&lt;ul&gt;
&lt;li&gt;&lt;a href=&quot;https://wikidocs.net/22886&quot; rel=&quot;external nofollow noopener noreferrer&quot; target=&quot;_blank&quot;&gt;https://wikidocs.net/22886&lt;/a&gt;&lt;/li&gt;
&lt;/ul&gt;
&lt;h2 id=&quot;자연어-처리의-전처리&quot;&gt;&lt;a href=&quot;#자연어-처리의-전처리&quot; class=&quot;headerlink&quot; title=&quot;자연어 처리의 전처리&quot;&gt;&lt;/a&gt;자연어 처리의 전처리&lt;/h2&gt;&lt;blockquote&gt;
&lt;p&gt;자연어 처리를 위해 자연어 데이터는 일반적으로 &lt;code&gt;토큰화&lt;/code&gt;, &lt;code&gt;단어집합생성&lt;/code&gt;, &lt;code&gt;정수인코딩&lt;/code&gt;, &lt;code&gt;패딩&lt;/code&gt;, &lt;code&gt;벡터화&lt;/code&gt;의 과정을 거친다.&lt;/p&gt;
&lt;/blockquote&gt;
      
      </description>
      
      
      <content:encoded><![CDATA[<p>다음의 책을 공부하며 정리한 내용입니다.</p><ul><li><a href="https://wikidocs.net/22886" rel="external nofollow noopener noreferrer" target="_blank">https://wikidocs.net/22886</a></li></ul><h2 id="자연어-처리의-전처리"><a href="#자연어-처리의-전처리" class="headerlink" title="자연어 처리의 전처리"></a>자연어 처리의 전처리</h2><blockquote><p>자연어 처리를 위해 자연어 데이터는 일반적으로 <code>토큰화</code>, <code>단어집합생성</code>, <code>정수인코딩</code>, <code>패딩</code>, <code>벡터화</code>의 과정을 거친다.</p></blockquote><a id="more"></a><h3 id="토큰화"><a href="#토큰화" class="headerlink" title="토큰화"></a>토큰화</h3><ul><li>주어진 텍스트를 단어/문자 단위로 자르는 것을 의미한다.</li><li>토큰화 도구<ul><li>spaCy, NLTK: English Tokenization</li><li>.split(): 파이썬 기본함수. 띄어쓰기 등으로 토큰화한다면..</li></ul></li></ul><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br></pre></td><td class="code"><pre><span class="line"><span class="comment"># tokenizer 도구 사용해보기 - 상세 코드는 wikidocs.net/64157 참고</span></span><br><span class="line"><span class="keyword">import</span> spacy</span><br><span class="line"></span><br><span class="line">text = <span class="string">"A dog run back corner near bedrooms"</span></span><br><span class="line">spacy_text = spacy.load(<span class="string">'en'</span>)</span><br><span class="line"></span><br><span class="line"><span class="keyword">for</span> token <span class="keyword">in</span> spacy_text.tokenizer(text):</span><br><span class="line">  print(token.text)</span><br></pre></td></tr></table></figure><figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br></pre></td><td class="code"><pre><span class="line">A</span><br><span class="line">dog</span><br><span class="line">run</span><br><span class="line">back</span><br><span class="line">corner</span><br><span class="line">near</span><br><span class="line">bedrooms</span><br></pre></td></tr></table></figure><h3 id="한국어-토큰화"><a href="#한국어-토큰화" class="headerlink" title="한국어 토큰화"></a>한국어 토큰화</h3><ul><li>영어와 달리 한국어는 띄어쓰기 단위로 토큰화 하면 ‘사과가 =/= 사과는’ 으로 인식되어 단어집합이 불필요하게 커진다.<ul><li>형태소 토큰화 사용 : 형태소 분석기를 사용해 토큰화를 진행한다.</li></ul></li></ul><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br></pre></td><td class="code"><pre><span class="line"><span class="comment"># 형태소 분석기 중 mecab을 사용해 한국어 형태소 토큰화한다.</span></span><br><span class="line"><span class="comment"># !git clone https://github.com/SOMJANG/Mecab-ko-for-Google-Colab.git</span></span><br><span class="line"><span class="comment"># %cd Mecab-ko-for-Google-Colab</span></span><br><span class="line"><span class="comment"># !bash install_mecab-ko_on_colab190912.sh</span></span><br><span class="line"><span class="keyword">from</span> konlpy.tag <span class="keyword">import</span> Mecab</span><br><span class="line"></span><br><span class="line">tokenizer = Mecab()</span><br><span class="line">print(tokenizer.morphs(<span class="string">"사과의 놀라운 효능이라는 글을 봤어. 그래서 오늘 사과를 먹으려고 했는데 사과가 썩어서 슈퍼에 가서 사과랑 오렌지 사왔어"</span>))</span><br></pre></td></tr></table></figure><hr><h3 id="단어집합의-생성"><a href="#단어집합의-생성" class="headerlink" title="단어집합의 생성"></a>단어집합의 생성</h3><p>: 단어집합(vocabulary)란 <code>중복을 제거한 텍스트 내 총 단어의 집합(set)</code>을 의미한다.</p><h3 id="실습-네이버-영화-리뷰-데이터를-통해-단어집합-생성"><a href="#실습-네이버-영화-리뷰-데이터를-통해-단어집합-생성" class="headerlink" title="(실습) 네이버 영화 리뷰 데이터를 통해 단어집합 생성"></a>(실습) 네이버 영화 리뷰 데이터를 통해 단어집합 생성</h3><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br></pre></td><td class="code"><pre><span class="line"><span class="comment"># 20만개의 영화리뷰에 대해 긍정 1, 부정 0으로 레이블링한 네이버 데이터를 다운받는다.</span></span><br><span class="line"><span class="keyword">import</span> urllib.request</span><br><span class="line"><span class="keyword">import</span> pandas <span class="keyword">as</span> pd</span><br><span class="line"><span class="keyword">import</span> numpy <span class="keyword">as</span> np</span><br><span class="line"><span class="keyword">import</span> matplotlib.pyplot <span class="keyword">as</span> plt</span><br><span class="line"></span><br><span class="line">urllib.request.urlretrieve(<span class="string">"https://raw.githubusercontent.com/e9t/nsmc/master/ratings.txt"</span>, filename=<span class="string">"ratings.txt"</span>)</span><br><span class="line">data = pd.read_table(<span class="string">'ratings.txt'</span>) <span class="comment"># 데이터프레임에 저장</span></span><br><span class="line">print(data[<span class="number">15</span>:<span class="number">20</span>]) <span class="comment"># 15번 ~ 20번까지의 리뷰 5개 뽑아보기</span></span><br><span class="line"></span><br><span class="line"><span class="comment"># data는 해시맵으로 특정 리뷰의 내용에만 접근하기 위한 키는 document이다</span></span><br><span class="line">print(data[<span class="string">'document'</span>][<span class="number">15</span>])</span><br></pre></td></tr></table></figure><figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br></pre></td><td class="code"><pre><span class="line">[결과]</span><br><span class="line">      id                                           document  label</span><br><span class="line">15  9034036  평점 왜 낮아? 긴장감 스릴감 진짜 최고인데 진짜 전장에서 느끼는 공포를 생생하게 ...      1</span><br><span class="line">16   979683                      네고시에이터랑 소재만 같을 뿐.. 아무런 관련없음..      1</span><br><span class="line">17   165498                                              단연 최고      1</span><br><span class="line">18  8703997                           가면 갈수록 더욱 빠져드네요 밀회 화이팅!!      1</span><br><span class="line">19  9468781  어?생각없이 봤는데 상당한 수작.일본영화 10년내 최고로 마음에 들었다.강렬한 임팩...      1</span><br><span class="line">평점 왜 낮아? 긴장감 스릴감 진짜 최고인데 진짜 전장에서 느끼는 공포를 생생하게 전해준다.</span><br></pre></td></tr></table></figure><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br><span class="line">23</span><br><span class="line">24</span><br><span class="line">25</span><br></pre></td><td class="code"><pre><span class="line"><span class="keyword">from</span> konlpy.tag <span class="keyword">import</span> Mecab</span><br><span class="line"></span><br><span class="line"><span class="comment"># 임의의 10000개의 리뷰를 sample data로 사용한다.</span></span><br><span class="line">sample_data = data[:<span class="number">10000</span>] </span><br><span class="line"></span><br><span class="line"><span class="comment"># 한글과 공백을 제외하고 불필요한 문자는 모두 제거한다. - 정규표현식 사용</span></span><br><span class="line">sample_data[<span class="string">'document'</span>] = data[<span class="string">'document'</span>].str.replace(<span class="string">"[^ㄱ-ㅎㅏ-ㅣ가-힣 ]"</span>,<span class="string">""</span>)</span><br><span class="line"></span><br><span class="line"><span class="comment"># 불용어를 제거해준다. - 인터넷 검색 시 검색 용어로 사용하지 않는 단어. 관사, 전치사, 조사, 접속사 등은 검색 색인 단어로 의미가 없는 단어</span></span><br><span class="line">stopwords=[<span class="string">'뭐'</span>,<span class="string">'으면'</span>,<span class="string">'을'</span>,<span class="string">'의'</span>,<span class="string">'가'</span>,<span class="string">'이'</span>,<span class="string">'은'</span>,<span class="string">'들'</span>,<span class="string">'는'</span>,<span class="string">'좀'</span>,<span class="string">'잘'</span>,<span class="string">'걍'</span>,<span class="string">'과'</span>,<span class="string">'도'</span>,<span class="string">'를'</span>,<span class="string">'으로'</span>,<span class="string">'자'</span>,<span class="string">'에'</span>,<span class="string">'와'</span>,<span class="string">'한'</span>,<span class="string">'하다'</span>]</span><br><span class="line"></span><br><span class="line">tokenizer = Mecab()</span><br><span class="line">res = []</span><br><span class="line"><span class="keyword">for</span> sentence <span class="keyword">in</span> sample_data[<span class="string">'document'</span>]:</span><br><span class="line">  tmp = []</span><br><span class="line">  tmp = tokenizer.morphs(sentence)</span><br><span class="line">  </span><br><span class="line">  tokenized = []</span><br><span class="line">  <span class="keyword">for</span> token <span class="keyword">in</span> tmp:</span><br><span class="line">    <span class="keyword">if</span> <span class="keyword">not</span> token <span class="keyword">in</span> stopwords:</span><br><span class="line">      tokenized.append(token)</span><br><span class="line"></span><br><span class="line">  res.append(tokenized)</span><br><span class="line"></span><br><span class="line">print(res[:<span class="number">2</span>])</span><br></pre></td></tr></table></figure><figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br></pre></td><td class="code"><pre><span class="line">[결과]</span><br><span class="line">&#x2F;usr&#x2F;local&#x2F;lib&#x2F;python3.6&#x2F;dist-packages&#x2F;ipykernel_launcher.py:7: SettingWithCopyWarning: </span><br><span class="line">A value is trying to be set on a copy of a slice from a DataFrame.</span><br><span class="line">Try using .loc[row_indexer,col_indexer] &#x3D; value instead</span><br><span class="line"></span><br><span class="line">See the caveats in the documentation: http:&#x2F;&#x2F;pandas.pydata.org&#x2F;pandas-docs&#x2F;stable&#x2F;user_guide&#x2F;indexing.html#returning-a-view-versus-a-copy</span><br><span class="line">  import sys</span><br><span class="line">[[&#39;어릴&#39;, &#39;때&#39;, &#39;보&#39;, &#39;고&#39;, &#39;지금&#39;, &#39;다시&#39;, &#39;봐도&#39;, &#39;재밌&#39;, &#39;어요&#39;, &#39;ㅋㅋ&#39;], [&#39;디자인&#39;, &#39;배우&#39;, &#39;학생&#39;, &#39;외국&#39;, &#39;디자이너&#39;, &#39;그&#39;, &#39;일군&#39;, &#39;전통&#39;, &#39;통해&#39;, &#39;발전&#39;, &#39;해&#39;, &#39;문화&#39;, &#39;산업&#39;, &#39;부러웠&#39;, &#39;는데&#39;, &#39;사실&#39;, &#39;우리&#39;, &#39;나라&#39;, &#39;에서&#39;, &#39;그&#39;, &#39;어려운&#39;, &#39;시절&#39;, &#39;끝&#39;, &#39;까지&#39;, &#39;열정&#39;, &#39;지킨&#39;, &#39;노라노&#39;, &#39;같&#39;, &#39;전통&#39;, &#39;있&#39;, &#39;어&#39;, &#39;저&#39;, &#39;같&#39;, &#39;사람&#39;, &#39;꿈&#39;, &#39;꾸&#39;, &#39;고&#39;, &#39;이뤄나갈&#39;, &#39;수&#39;, &#39;있&#39;, &#39;다는&#39;, &#39;것&#39;, &#39;감사&#39;, &#39;합니다&#39;]]</span><br></pre></td></tr></table></figure><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br></pre></td><td class="code"><pre><span class="line"><span class="keyword">import</span> nltk</span><br><span class="line"><span class="keyword">from</span> nltk <span class="keyword">import</span> FreqDist</span><br><span class="line"></span><br><span class="line"><span class="comment"># 단어-빈도수 조합으로 이루어진 단어집합 해시맵 vocab을 생성한다.</span></span><br><span class="line"><span class="comment"># NLTK에서는 빈도수 계산 도구인 FreqDist()를 지원한다.</span></span><br><span class="line">vocab = FreqDist(np.hstack(res))</span><br><span class="line">print(<span class="string">'단어 [별로]의 빈도수는? '</span>, vocab[<span class="string">'별로'</span>], <span class="string">'번'</span>)</span><br><span class="line"></span><br><span class="line"><span class="comment"># most_common(N) : 가장 빈도수가 높은 N개의 단어를 반환</span></span><br><span class="line"><span class="comment"># 상위 500개의 단어만 보존</span></span><br><span class="line">vocab = vocab.most_common(<span class="number">500</span>)</span><br></pre></td></tr></table></figure><figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br></pre></td><td class="code"><pre><span class="line">[결과]</span><br><span class="line">단어 [별로]의 빈도수는?  37 번</span><br></pre></td></tr></table></figure><hr><h3 id="각-단어에-고유한-정수-부여"><a href="#각-단어에-고유한-정수-부여" class="headerlink" title="각 단어에 고유한 정수 부여"></a>각 단어에 고유한 정수 부여</h3><ul><li>각 토큰에 고유한 정수를 부여한다.</li><li>0과 1은 특수 인덱스로 사용한다.<ul><li>인덱스 0 : 단어집합에 없는 토큰</li><li>인덱스 1 : 패딩 토큰(길이맞추기용)</li></ul></li></ul><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br><span class="line">23</span><br><span class="line">24</span><br><span class="line">25</span><br></pre></td><td class="code"><pre><span class="line">word_to_index = &#123;&#125;</span><br><span class="line"></span><br><span class="line"><span class="comment"># 단어들에 순차적으로 2~ 501까지의 인덱스를 부여한다</span></span><br><span class="line">word_to_index = &#123;word[<span class="number">0</span>] : index + <span class="number">2</span> <span class="keyword">for</span> index, word <span class="keyword">in</span> enumerate(vocab)&#125;</span><br><span class="line"></span><br><span class="line"><span class="comment"># 특수 인덱스</span></span><br><span class="line">word_to_index[<span class="string">'unk'</span>] = <span class="number">0</span></span><br><span class="line">word_to_index[<span class="string">'pad'</span>] = <span class="number">1</span></span><br><span class="line"></span><br><span class="line">encoded = []</span><br><span class="line"><span class="keyword">for</span> review <span class="keyword">in</span> res:</span><br><span class="line">  </span><br><span class="line">  tmp = []</span><br><span class="line">  <span class="keyword">for</span> word <span class="keyword">in</span> review:</span><br><span class="line">    <span class="keyword">try</span>:</span><br><span class="line">      <span class="comment"># 각 글자를 해당하는 정수로 변환한다.</span></span><br><span class="line">      tmp.append(word_to_index[word])</span><br><span class="line">    <span class="keyword">except</span> KeyError: </span><br><span class="line">      <span class="comment"># 단어 집합에 없는 단어일 경우(=빈도수 상위 500 이외의 단어) unk로 대체된다.</span></span><br><span class="line">      tmp.append(word_to_index[<span class="string">'unk'</span>])</span><br><span class="line"></span><br><span class="line">  encoded.append(tmp)</span><br><span class="line"></span><br><span class="line"><span class="comment"># 기존의 리뷰가 성공적으로 encoding 되었는지 확인해보기</span></span><br><span class="line">print(encoded[:<span class="number">2</span>])</span><br></pre></td></tr></table></figure><figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line">[[294, 51, 6, 4, 89, 63, 86, 11, 21, 34], [0, 79, 0, 0, 0, 54, 0, 0, 0, 0, 48, 0, 0, 0, 19, 314, 136, 319, 26, 54, 0, 278, 169, 72, 0, 0, 0, 32, 0, 8, 36, 140, 32, 68, 383, 0, 4, 0, 22, 8, 123, 29, 320, 103]]</span><br></pre></td></tr></table></figure><hr><h3 id="Padding"><a href="#Padding" class="headerlink" title="Padding"></a>Padding</h3><p>: 길이가 다른 문장들을 모두 동일한 크기로 바꿔주는 작업</p><ul><li>인코딩한 리뷰를 모두 일정한 길이로 변환해준다.</li><li>특정 길이로 모든 샘플들의 길이를 맞춰준다.</li><li>정한 길이보다 짧은 샘플들에는 ‘pad’ 토큰을 추가하여 길이를 맞춰준다.</li></ul><blockquote><p>리뷰의 길이를 그래프로 출력하는 코드</p></blockquote><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br></pre></td><td class="code"><pre><span class="line">max_len = max(len(l) <span class="keyword">for</span> l <span class="keyword">in</span> encoded)</span><br><span class="line">print(<span class="string">'리뷰의 최대 길이 : %d'</span> % max_len)</span><br><span class="line">print(<span class="string">'리뷰의 최소 길이 : %d'</span> % min(len(l) <span class="keyword">for</span> l <span class="keyword">in</span> encoded))</span><br><span class="line">print(<span class="string">'리뷰의 평균 길이 : %f'</span> % (sum(map(len, encoded))/len(encoded)))</span><br><span class="line">plt.hist([len(s) <span class="keyword">for</span> s <span class="keyword">in</span> encoded], bins=<span class="number">50</span>)</span><br><span class="line">plt.xlabel(<span class="string">'length of sample'</span>)</span><br><span class="line">plt.ylabel(<span class="string">'number of sample'</span>)</span><br><span class="line">plt.show()</span><br></pre></td></tr></table></figure><p><img src="https://wikidocs.net/images/page/64517/%EB%A6%AC%EB%B7%B0%EB%93%A4%EC%9D%98_%EB%B6%84%ED%8F%AC.PNG" alt="대체 텍스트"></p><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br></pre></td><td class="code"><pre><span class="line">max_len = max(len(l) <span class="keyword">for</span> l <span class="keyword">in</span> encoded)</span><br><span class="line">print(<span class="string">'리뷰의 최대 길이 : %d'</span> % max_len)</span><br></pre></td></tr></table></figure><figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br></pre></td><td class="code"><pre><span class="line">[결과]</span><br><span class="line">리뷰의 최대 길이 : 81</span><br></pre></td></tr></table></figure><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br></pre></td><td class="code"><pre><span class="line"><span class="comment"># 리뷰 최대 길이인 81로 모든 리뷰의 길이를 맞춰준다.</span></span><br><span class="line">pad_len = <span class="number">81</span></span><br><span class="line"></span><br><span class="line"><span class="keyword">for</span> review <span class="keyword">in</span> encoded:</span><br><span class="line">  <span class="keyword">if</span> len(review) &lt; pad_len:</span><br><span class="line">    review += [word_to_index[<span class="string">'pad'</span>]] * (pad_len - len(review))</span><br><span class="line">    </span><br><span class="line"><span class="comment"># 기존의 리뷰가 성공적으로 padding 되었는지 보기</span></span><br><span class="line">print(encoded[<span class="number">0</span>])</span><br></pre></td></tr></table></figure><figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br></pre></td><td class="code"><pre><span class="line">[결과]</span><br><span class="line">[294, 51, 6, 4, 89, 63, 86, 11, 21, 34, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1]</span><br></pre></td></tr></table></figure>]]></content:encoded>
      
      <comments>https://katie0809.github.io/2020/02/17/ai-study4/#disqus_thread</comments>
    </item>
    
    <item>
      <title>[딥러닝 스터디] 순환신경망(RNN)</title>
      <link>https://katie0809.github.io/2020/02/17/ai-study3/</link>
      <guid>https://katie0809.github.io/2020/02/17/ai-study3/</guid>
      <pubDate>Mon, 17 Feb 2020 11:31:21 GMT</pubDate>
      <description>
      
        &lt;p&gt;다음의 책을 공부하며 정리한 내용입니다.&lt;/p&gt;
&lt;ul&gt;
&lt;li&gt;&lt;a href=&quot;https://wikidocs.net/22886&quot; rel=&quot;external nofollow noopener noreferrer&quot; target=&quot;_blank&quot;&gt;https://wikidocs.net/22886&lt;/a&gt;&lt;/li&gt;
&lt;/ul&gt;
&lt;h2 id=&quot;순환신경망-RNN-Recurrent-Neural-Network&quot;&gt;&lt;a href=&quot;#순환신경망-RNN-Recurrent-Neural-Network&quot; class=&quot;headerlink&quot; title=&quot;순환신경망(RNN : Recurrent Neural Network)&quot;&gt;&lt;/a&gt;순환신경망(RNN : Recurrent Neural Network)&lt;/h2&gt;&lt;p&gt;: RNN은 입력과 출력을 &lt;strong&gt;시퀀스 단위로 처리&lt;/strong&gt;하는 &lt;code&gt;시퀀스 모델&lt;/code&gt;이다. 이때의 입력은 처리하고자 하는 문장, 즉 단어 시퀀스이며, 출력은 처리된 문장 단어 시퀀스이다. &lt;/p&gt;
      
      </description>
      
      
      <content:encoded><![CDATA[<p>다음의 책을 공부하며 정리한 내용입니다.</p><ul><li><a href="https://wikidocs.net/22886" rel="external nofollow noopener noreferrer" target="_blank">https://wikidocs.net/22886</a></li></ul><h2 id="순환신경망-RNN-Recurrent-Neural-Network"><a href="#순환신경망-RNN-Recurrent-Neural-Network" class="headerlink" title="순환신경망(RNN : Recurrent Neural Network)"></a>순환신경망(RNN : Recurrent Neural Network)</h2><p>: RNN은 입력과 출력을 <strong>시퀀스 단위로 처리</strong>하는 <code>시퀀스 모델</code>이다. 이때의 입력은 처리하고자 하는 문장, 즉 단어 시퀀스이며, 출력은 처리된 문장 단어 시퀀스이다. </p><a id="more"></a><p>RNN의 가장 큰 특징은 은닉층의 노드에서 나온 결과값이 <code>출력층</code> 과 <code>은닉층</code> 노드의 다음 계산을 위한 입력으로, <strong>둘 모두로 보내진다</strong>는 점이다.</p><p><img src="https://wikidocs.net/images/page/22886/rnn_image1_ver2.PNG" alt="대체 텍스트"></p><ul><li><p><strong>xt</strong> : 입력층의 입력 벡터</p></li><li><p><strong>yt</strong> : 출력층의 출력 벡터</p></li><li><p><strong>cell</strong> : 은닉층에서 <code>결과를 두 방향으로 내보내는</code>(출력층 &amp; 다음연산) 노드. <u>메모리 셀</u> 혹은 RNN셀이라고 표현한다.</p><blockquote><p>이때 메모리 셀이 두 방향으로 내보내는 결과를 <code>은닉상태</code>(hidden state) 라고 한다.</p></blockquote></li></ul><p>피드포워드 신경망에서는 기본적으로 <code>뉴런</code>이라는 단위를 사용했지만, RNN에서는 </p><ul><li>입력층/출력층 -&gt; 입력벡터/출력벡터</li><li>은닉층 -&gt; 은닉상태</li></ul><p>의 표현을 일반적으로 사용한다.</p><p>피드포워드 신경망과 같이 <em>뉴런 단위로 RNN을 시각화</em>할 경우 아래와 같이 표현할 수 있다.</p><p><img src="https://wikidocs.net/images/page/22886/rnn_image2.5.PNG" alt="대체 텍스트"></p><ul><li>입력벡터 차원(입력층의 뉴런 수) : 4</li><li>은닉상태 크기(은닉층의 뉴런 수) : 2</li><li>출력벡터 차원(출력층의 뉴런 수) : 2</li><li>시점(timestep) : 2</li></ul><hr><h3 id="RNN의-활용"><a href="#RNN의-활용" class="headerlink" title="RNN의 활용"></a>RNN의 활용</h3><p>: RNN은 입력과 출력의 길이가 고정되어 있지 않다. 즉, 설계에 따라 다양한 용도로 신경망을 사용할 수 있다.</p><p><img src="https://wikidocs.net/images/page/22886/rnn_image3_ver2.PNG" alt="img"></p><h4 id="일대다-모델"><a href="#일대다-모델" class="headerlink" title="일대다 모델"></a>일대다 모델</h4><ul><li>하나의 이미지 입력에 대해서 사진의 제목을 출력하는 <strong>이미지 캡셔닝(Image Captioning)</strong> 작업에 사용할 수 있다. </li><li>사진의 제목은 단어들의 나열이므로 시퀀스 출력이다.</li></ul><p><img src="https://wikidocs.net/images/page/22886/rnn_image3.5.PNG" alt="img"></p><h4 id="다대일-모델"><a href="#다대일-모델" class="headerlink" title="다대일 모델"></a>다대일 모델</h4><ul><li>단어 시퀀스에 대해서 하나의 출력(many-to-one)을 하는 모델.</li><li>입력 문서가 긍정적인지 부정적인지를 판별하는 <strong>감성 분류</strong>(sentiment classification), 또는 메일이 정상 메일인지 스팸 메일인지 판별하는 <strong>스팸 메일 분류</strong>(spam detection)에 사용할 수 있다. </li><li>위 그림은 RNN으로 스팸 메일을 분류할 때의 아키텍처를 보여줍니다.</li></ul><p><img src="https://wikidocs.net/images/page/22886/rnn_image3.7.PNG" alt="img"></p><h4 id="다대다-모델"><a href="#다대다-모델" class="headerlink" title="다대다 모델"></a>다대다 모델</h4><ul><li>다 대 다(many-to-many)의 모델의 경우에는 입력 문장으로 부터 대답 문장을 출력하는 챗봇과 입력 문장으로부터 번역된 문장을 출력하는 <strong>번역기</strong>, <strong>개체명 인식</strong>이나 <strong>품사 태깅</strong>과 같은 작업이 속한다. </li><li>위 그림은 개체명 인식을 수행할 때의 RNN 아키텍처를 보여줍니다.</li></ul><hr><h3 id="RNN의-수식"><a href="#RNN의-수식" class="headerlink" title="RNN의 수식"></a>RNN의 수식</h3><p><img src="https://wikidocs.net/images/page/22886/rnn_image4_ver2.PNG" alt="img"></p><ul><li><strong>ht</strong> : <u>현재시점 t</u>에서의 <code>은닉 상태값</code></li><li><strong>wx</strong> : 입력층의 입력값에 대한 가중치</li><li><strong>wt</strong> : 이전시점 t-1의 은닉상태값 ht-1 에 대한 가중치 wh</li></ul><p>따라서 ht를 계산하는 수식은 다음과 같다.</p><blockquote><p>ht = activation_func(<code>(wh * ht-1) + (wx * xt) + b</code>)</p></blockquote><p>이때 활성화 함수는 일반적으로 <strong>tanh함수</strong>를 사용한다. ReLU를 사용하기도 한다. 출력층 값 yt는 아래와 같이 계산한다.</p><blockquote><p>yt = activation_func(<code>(wy * ht) + b</code>)</p></blockquote><p>이때 비선형 활성화 함수 중 하나를 activation func으로 사용한다.</p><hr><h3 id="실습-파이썬으로-RNN-구현하기"><a href="#실습-파이썬으로-RNN-구현하기" class="headerlink" title="(실습) 파이썬으로 RNN 구현하기"></a>(실습) 파이썬으로 RNN 구현하기</h3><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br></pre></td><td class="code"><pre><span class="line"><span class="keyword">import</span> torch</span><br><span class="line"><span class="keyword">import</span> torch.nn <span class="keyword">as</span> nn</span><br><span class="line"></span><br><span class="line"><span class="comment"># 입력과 은닉상태의 크기를 정의한다.</span></span><br><span class="line">input_size = <span class="number">5</span></span><br><span class="line">hidden_size = <span class="number">8</span></span><br><span class="line"></span><br><span class="line"><span class="comment"># 입력텐서(=입력벡터)를 정의한다. </span></span><br><span class="line"><span class="comment"># (배치크기 * 시점의 수 * 입력크기)를 인자로 받는다.</span></span><br><span class="line">input_vec = torch.Tensor(<span class="number">1</span>, <span class="number">10</span>, input_size)</span><br><span class="line"></span><br><span class="line"><span class="comment"># nn.RNN()으로 RNN셀을 정의한다.</span></span><br><span class="line"><span class="comment"># (입력크기 * 은닉상태 크기)를 인자로 받는다. batch_first=True는 입력텐서의 첫번째 차원이 배치크기임을 알려준다.</span></span><br><span class="line">cell = nn.RNN(input_size, hidden_size, batch_first=<span class="literal">True</span>)</span><br><span class="line"></span><br><span class="line"><span class="comment"># 입력텐서를 RNN셀에 넣어 출력값의 크기를 확인해본다.</span></span><br><span class="line"><span class="comment"># (모든 시점의 은닉상태들, 마지막 시점의 은닉상태)를 반환한다.</span></span><br><span class="line">outputs, final_output = cell(input_vec)</span><br><span class="line">print(outputs.shape)</span><br><span class="line">print(final_output.shape)</span><br></pre></td></tr></table></figure><figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br></pre></td><td class="code"><pre><span class="line">[결과]</span><br><span class="line">torch.Size([1, 10, 8])</span><br><span class="line">torch.Size([1, 1, 8])</span><br></pre></td></tr></table></figure><hr><h3 id="다양한-순환신경망"><a href="#다양한-순환신경망" class="headerlink" title="다양한 순환신경망"></a>다양한 순환신경망</h3><ul><li><p>깊은 순환신경망(Deep Recurrent Neural Network)</p><p>: RNN역시 <code>다수의 은닉층</code>을 가질 수 있다. 2개 이상의 은닉층을 가진 RNN을 Deep RNN이라고 한다.</p></li></ul><p><img src="https://wikidocs.net/images/page/22886/rnn_image4.5_finalPNG.PNG" alt="img"></p><p>깊은 순환 신경망은 <code>nn.RNN()</code>의 인자로 <code>num_layers</code> 파라미터를 추가해줌으로서 구현할 수 있다.</p><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br></pre></td><td class="code"><pre><span class="line"><span class="comment"># (입력텐서 크기, 은닉층 크기, 은닉층 개수)</span></span><br><span class="line">cell = nn.RNN(input_size = <span class="number">5</span>, hidden_size = <span class="number">8</span>, num_layers = <span class="number">2</span>, batch_first=<span class="literal">True</span>)</span><br></pre></td></tr></table></figure><p>이때 마지막 시점의 은닉상태는 다음과 같이 바뀐다.</p><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br></pre></td><td class="code"><pre><span class="line">print(final_output.shape)</span><br><span class="line"> </span><br><span class="line"><span class="comment"># (층의 개수, 배치 크기, 은닉 상태의 크기)</span></span><br><span class="line">&gt;&gt; torch.Size([<span class="number">2</span>, <span class="number">1</span>, <span class="number">8</span>])</span><br></pre></td></tr></table></figure><ul><li><p>양방향 순환신경망(Bidirectional Recurrent Neural Network)</p><p>: 양방향 순환신경망은 특정 시점 t에서 출력값 ht를 예측할 때 이전시점의 데이터 ht-1뿐만 아니라 <code>이후시점의 데이터로도 예측</code>할 수 있다는 아이디어에서 출발한다.</p></li></ul><p><img src="https://wikidocs.net/images/page/22886/rnn_image5_ver2.PNG" alt="img"></p><p>(예제)</p><figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br></pre></td><td class="code"><pre><span class="line">Exercise is very effective at [          ] belly fat.</span><br><span class="line"> </span><br><span class="line">1) reducing</span><br><span class="line">2) increasing</span><br><span class="line">3) multiplying</span><br></pre></td></tr></table></figure><blockquote><p>정답 reducing을 찾기 위해서는 <strong><em>이전에 나온 단어와 이후에 나온 단어 모두를 참고\</em></strong>해야 결정할 수 있다.</p></blockquote><p>즉, 양방향 RNN은 이전 시점의 데이터뿐만 아니라, 이후 시점의 데이터도 힌트로 활용하기 위해서 고안된 모델이다.</p><p><img src="https://wikidocs.net/images/page/22886/rnn_image6_ver3.PNG" alt="img"></p><ul><li>양방향 순환 신경망은 하나의 출력값 ht를 예측하기 위해 <code>두개의 메모리 셀</code>을 사용 한다.<ul><li>첫번째 메모리 셀은 <code>앞 시점의 은닉상태</code>를 전달받아 계산한다.</li><li>두번째 메모리 셀은 <code>뒤 시점의 은닉상태</code>를 전달받아 계산한다.</li></ul></li><li>양방향 RNN도 다수의 은닉층을 가질 수 있다.<ul><li>nn.RNN()의 인자로 <code>bidirectional</code>값을 True로 전달하여 구현할 수 있다. </li></ul></li></ul><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br></pre></td><td class="code"><pre><span class="line"><span class="comment"># (입력텐서 크기, 은닉층 크기, 은닉층 개수, 양방향 여부)</span></span><br><span class="line">cell = nn.RNN(input_size = <span class="number">5</span>, hidden_size = <span class="number">8</span>, num_layers = <span class="number">2</span>, batch_first=<span class="literal">True</span>, bidirectional=<span class="literal">True</span>)</span><br></pre></td></tr></table></figure><p>이때 마지막 시점의 은닉상태는 다음과 같이 바뀐다.</p><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br></pre></td><td class="code"><pre><span class="line">print(final_output.shape)</span><br><span class="line"> </span><br><span class="line"><span class="comment"># (층의 개수 * 2, 배치 크기, 은닉 상태의 크기)</span></span><br><span class="line">&gt;&gt; torch.Size([<span class="number">4</span>, <span class="number">1</span>, <span class="number">8</span>])</span><br></pre></td></tr></table></figure>]]></content:encoded>
      
      <comments>https://katie0809.github.io/2020/02/17/ai-study3/#disqus_thread</comments>
    </item>
    
    <item>
      <title>[딥러닝 기초] Logistic Regression</title>
      <link>https://katie0809.github.io/2020/02/17/ai-start4/</link>
      <guid>https://katie0809.github.io/2020/02/17/ai-start4/</guid>
      <pubDate>Mon, 17 Feb 2020 00:49:40 GMT</pubDate>
      <description>
      
        &lt;p&gt;다음의 책을 공부하며 정리한 내용입니다&lt;/p&gt;
&lt;ul&gt;
&lt;li&gt;&lt;a href=&quot;https://wikidocs.net/book/2788&quot; rel=&quot;external nofollow noopener noreferrer&quot; target=&quot;_blank&quot;&gt;https://wikidocs.net/book/2788&lt;/a&gt;&lt;/li&gt;
&lt;/ul&gt;
&lt;h2 id=&quot;로지스틱-회귀란&quot;&gt;&lt;a href=&quot;#로지스틱-회귀란&quot; class=&quot;headerlink&quot; title=&quot;로지스틱 회귀란&quot;&gt;&lt;/a&gt;로지스틱 회귀란&lt;/h2&gt;&lt;p&gt;: 로지스틱 회귀는 &lt;code&gt;이진분류&lt;/code&gt;(Binary Classification) 문제의 해결에 사용되는 대표적인 알고리즘.&lt;/p&gt;
&lt;blockquote&gt;
&lt;p&gt;이름은 ‘&lt;strong&gt;회귀&lt;/strong&gt;‘이지만 ‘&lt;code&gt;분류&lt;/code&gt;‘에 쓰인다&lt;/p&gt;
&lt;/blockquote&gt;
      
      </description>
      
      
      <content:encoded><![CDATA[<p>다음의 책을 공부하며 정리한 내용입니다</p><ul><li><a href="https://wikidocs.net/book/2788" rel="external nofollow noopener noreferrer" target="_blank">https://wikidocs.net/book/2788</a></li></ul><h2 id="로지스틱-회귀란"><a href="#로지스틱-회귀란" class="headerlink" title="로지스틱 회귀란"></a>로지스틱 회귀란</h2><p>: 로지스틱 회귀는 <code>이진분류</code>(Binary Classification) 문제의 해결에 사용되는 대표적인 알고리즘.</p><blockquote><p>이름은 ‘<strong>회귀</strong>‘이지만 ‘<code>분류</code>‘에 쓰인다</p></blockquote><a id="more"></a><h3 id="이진분류의-모델"><a href="#이진분류의-모델" class="headerlink" title="이진분류의 모델"></a>이진분류의 모델</h3><table><thead><tr><th>점수(x)</th><th>결과(y)</th></tr></thead><tbody><tr><td>45</td><td>불합격</td></tr><tr><td>50</td><td>불합격</td></tr><tr><td>55</td><td>불합격</td></tr><tr><td>60</td><td>합격</td></tr><tr><td>65</td><td>합격</td></tr><tr><td>70</td><td>합격</td></tr></tbody></table><p>위와 같은 데이터가 있다고 하자.</p><p>조건은 아래와 같다.</p><ul><li>합격 커트라인은 알려져있지 않다</li><li>임의의 점수 x의 합격여부를 예측하고 싶다.</li></ul><p>이 경우 주어진 데이터에 대해 합격(1), 불합격(0)으로 그래프를 그리면 아래와 같이 표현할 수 있다.</p><p><img src="https://wikidocs.net/images/page/22881/%EB%A1%9C%EC%A7%80%EC%8A%A4%ED%8B%B1%ED%9A%8C%EA%B7%80.PNG" alt="대체 텍스트"></p><p>위의 간단한 예시를 통해 <strong>이진분류의 문제를 풀기위한 x, y의 관계</strong>는 <code>S 형태의 그래프</code>로 나타내야 함을 알 수 있다.</p><p>따라서 다음과 같은 결론을 얻을 수 있다.</p><ol><li>로지스틱 회귀의 가설은 선형 회귀 때의 <u>H(x)=Wx+b가 아니다</u>.</li><li>위처럼 <code>S자 모양의 그래프</code>를 만들 수 있는 <code>어떤 특정 함수 f</code>를 추가적으로 사용하여 <code>H(x)=f(Wx+b)의 가설을 사용</code>한다.</li><li>어떤 함수 f는 이미 널리 알려져있다. =&gt; <code>시그모이드 함수</code></li></ol><p>즉, 로지스틱 회귀의 가설이자 이진분류 문제를 풀기위한 함수 f는 <code>Sigmoid function</code>이다</p><h3 id="Sigmoid"><a href="#Sigmoid" class="headerlink" title="Sigmoid"></a>Sigmoid</h3><ul><li>수식</li></ul><blockquote><p> H(x)=sigmoid(Wx+b)=1+e−(Wx+b)=σ(Wx+b)</p></blockquote><ul><li>가중치(w)의 변화에 따른 Sigmoid 함수</li></ul><blockquote><p>red: w값이 0.5 ~ blue: w값이 2</p></blockquote><p><img src="https://wikidocs.net/images/page/22881/%EC%8B%9C%EA%B7%B8%EB%AA%A8%EC%9D%B4%EB%93%9C%ED%95%A8%EC%88%98%EC%9D%98%EA%B8%B0%EC%9A%B8%EA%B8%B0%EC%9D%98%EB%B3%80%ED%99%94.png" alt="대체 텍스트"></p><ul><li>편향(b)의 변화에 따른 Sigmoid 함수</li></ul><blockquote><p>red: b값이 0.5 ~ blue: b값이 2</p></blockquote><p><img src="https://wikidocs.net/images/page/22881/b%EC%9D%98%EC%9D%B4%EB%8F%99.png" alt="대체 텍스트"></p><h3 id="Sigmoid-함수의-특성"><a href="#Sigmoid-함수의-특성" class="headerlink" title="Sigmoid 함수의 특성"></a>Sigmoid 함수의 특성</h3><ul><li>시그모이드 함수는 입력값이 한없이 커지면 1에 수렴하고, 입력값이 한없이 작아지면 0에 수렴한다.</li><li>시그모이드 함수의 출력은 0~1</li><li>위의 특성을 이용하여 분류 작업에 사용.</li><li>임계값 x(0 =&lt; x =&lt; 1)를 넘으면 1, 넘지 못하면 0으로 분류</li></ul><h3 id="결론"><a href="#결론" class="headerlink" title="결론"></a>결론</h3><blockquote><p>로지스틱 회귀의 가설/모델은 <code>H(x)=sigmoid(Wx+b)</code> 이다.</p></blockquote><hr><h3 id="로지스틱-회귀-실습"><a href="#로지스틱-회귀-실습" class="headerlink" title="로지스틱 회귀 실습"></a>로지스틱 회귀 실습</h3><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br><span class="line">23</span><br><span class="line">24</span><br><span class="line">25</span><br><span class="line">26</span><br><span class="line">27</span><br><span class="line">28</span><br><span class="line">29</span><br><span class="line">30</span><br><span class="line">31</span><br><span class="line">32</span><br><span class="line">33</span><br><span class="line">34</span><br></pre></td><td class="code"><pre><span class="line"><span class="comment"># 최종코드</span></span><br><span class="line"><span class="keyword">import</span> torch</span><br><span class="line"><span class="keyword">import</span> torch.nn.functional <span class="keyword">as</span> F</span><br><span class="line">torch.manual_seed(<span class="number">1</span>)</span><br><span class="line"></span><br><span class="line">x_train = torch.FloatTensor([[<span class="number">1</span>, <span class="number">2</span>], [<span class="number">2</span>, <span class="number">3</span>], [<span class="number">3</span>, <span class="number">1</span>], [<span class="number">4</span>, <span class="number">3</span>], [<span class="number">5</span>, <span class="number">3</span>], [<span class="number">6</span>, <span class="number">2</span>]])</span><br><span class="line">y_train = torch.FloatTensor([[<span class="number">0</span>], [<span class="number">0</span>], [<span class="number">0</span>], [<span class="number">1</span>], [<span class="number">1</span>], [<span class="number">1</span>]])</span><br><span class="line"></span><br><span class="line">w = torch.zeros([<span class="number">2</span>, <span class="number">1</span>], requires_grad=<span class="literal">True</span>)</span><br><span class="line">b = torch.zeros([<span class="number">1</span>], requires_grad=<span class="literal">True</span>)</span><br><span class="line"></span><br><span class="line"><span class="comment"># hypothesis = 1 / (1 + torch.exp(-(x_train.matmul(w) + b)))</span></span><br><span class="line">hypothesis = torch.sigmoid(x_train.matmul(w) + b)</span><br><span class="line"></span><br><span class="line">optimizer = torch.optim.SGD([w, b], lr=<span class="number">1</span>)</span><br><span class="line"></span><br><span class="line">tot_epoch = <span class="number">1000</span></span><br><span class="line"><span class="keyword">for</span> cur_epoch <span class="keyword">in</span> range(tot_epoch + <span class="number">1</span>):</span><br><span class="line">   </span><br><span class="line">   y_hat = torch.sigmoid(x_train.matmul(w) + b)</span><br><span class="line"></span><br><span class="line">   cost = F.binary_cross_entropy(y_hat, y_train)</span><br><span class="line"></span><br><span class="line">   optimizer.zero_grad()</span><br><span class="line">   cost.backward()</span><br><span class="line">   optimizer.step()</span><br><span class="line">   </span><br><span class="line">   <span class="comment"># 100번마다 로그 출력</span></span><br><span class="line">   <span class="keyword">if</span> cur_epoch % <span class="number">100</span> == <span class="number">0</span>:</span><br><span class="line">     print(<span class="string">'Epoch &#123;:4d&#125;/&#123;&#125; Cost: &#123;:.6f&#125;'</span>.format(cur_epoch, tot_epoch, cost.item())) </span><br><span class="line"></span><br><span class="line"><span class="comment"># 제대로 학습됐는지 확인</span></span><br><span class="line">print(w)</span><br><span class="line">print(b)</span><br></pre></td></tr></table></figure><figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br></pre></td><td class="code"><pre><span class="line">[결과]</span><br><span class="line">Epoch    0&#x2F;1000 Cost: 0.693147</span><br><span class="line">Epoch  100&#x2F;1000 Cost: 0.134722</span><br><span class="line">Epoch  200&#x2F;1000 Cost: 0.080643</span><br><span class="line">Epoch  300&#x2F;1000 Cost: 0.057900</span><br><span class="line">Epoch  400&#x2F;1000 Cost: 0.045300</span><br><span class="line">Epoch  500&#x2F;1000 Cost: 0.037261</span><br><span class="line">Epoch  600&#x2F;1000 Cost: 0.031672</span><br><span class="line">Epoch  700&#x2F;1000 Cost: 0.027556</span><br><span class="line">Epoch  800&#x2F;1000 Cost: 0.024394</span><br><span class="line">Epoch  900&#x2F;1000 Cost: 0.021888</span><br><span class="line">Epoch 1000&#x2F;1000 Cost: 0.019852</span><br><span class="line">tensor([[3.2530],</span><br><span class="line">        [1.5179]], requires_grad&#x3D;True)</span><br><span class="line">tensor([-14.4819], requires_grad&#x3D;True)</span><br></pre></td></tr></table></figure>]]></content:encoded>
      
      <comments>https://katie0809.github.io/2020/02/17/ai-start4/#disqus_thread</comments>
    </item>
    
    <item>
      <title>[딥러닝 기초] 선형회귀 모델의 개선</title>
      <link>https://katie0809.github.io/2020/02/17/ai-start3/</link>
      <guid>https://katie0809.github.io/2020/02/17/ai-start3/</guid>
      <pubDate>Mon, 17 Feb 2020 00:36:45 GMT</pubDate>
      <description>
      
        &lt;p&gt;다음의 책을 공부하며 정리한 내용입니다&lt;/p&gt;
&lt;ul&gt;
&lt;li&gt;&lt;a href=&quot;https://wikidocs.net/book/2788&quot; rel=&quot;external nofollow noopener noreferrer&quot; target=&quot;_blank&quot;&gt;https://wikidocs.net/book/2788&lt;/a&gt;&lt;/li&gt;
&lt;/ul&gt;
&lt;h2 id=&quot;nn-Module-을-사용한-선형회귀-모델의-개선&quot;&gt;&lt;a href=&quot;#nn-Module-을-사용한-선형회귀-모델의-개선&quot; class=&quot;headerlink&quot; title=&quot;nn.Module 을 사용한 선형회귀 모델의 개선&quot;&gt;&lt;/a&gt;nn.Module 을 사용한 선형회귀 모델의 개선&lt;/h2&gt;&lt;p&gt;: 파이토치에서 일부 모델(ex. 선형회귀모델)들은 이미 nn.Module의 형태로 편리하게 쓸 수 있도록 구현되어있다.&lt;/p&gt;
      
      </description>
      
      
      <content:encoded><![CDATA[<p>다음의 책을 공부하며 정리한 내용입니다</p><ul><li><a href="https://wikidocs.net/book/2788" rel="external nofollow noopener noreferrer" target="_blank">https://wikidocs.net/book/2788</a></li></ul><h2 id="nn-Module-을-사용한-선형회귀-모델의-개선"><a href="#nn-Module-을-사용한-선형회귀-모델의-개선" class="headerlink" title="nn.Module 을 사용한 선형회귀 모델의 개선"></a>nn.Module 을 사용한 선형회귀 모델의 개선</h2><p>: 파이토치에서 일부 모델(ex. 선형회귀모델)들은 이미 nn.Module의 형태로 편리하게 쓸 수 있도록 구현되어있다.</p><a id="more"></a><p>즉, 우리가 기존에 구현했던 선형회귀 수식</p><ul><li>y_hat = (w * x_train) + b</li><li>y_hat = x_train.matmul(w) + b</li></ul><p>이는 아래와 같이 변경할 수 있다.</p><ul><li><strong>model = nn.Linear(1, 1)</strong></li><li><strong>model = nn.Linear(3, 1)</strong></li></ul><p>: 이때 <strong>nn.Linear()</strong>은 <u>선형회귀 모델</u>을 의미하며 왼쪽부터 순서대로 <code>input dimension</code>, <code>output dimension</code>이다.</p><blockquote><p>input_dim : 가중치 w의 개수</p><p>output_dim : 가중치 w의 길이</p></blockquote><h3 id="nn-Linear-모델-사용해보기"><a href="#nn-Linear-모델-사용해보기" class="headerlink" title="nn.Linear 모델 사용해보기"></a>nn.Linear 모델 사용해보기</h3><ul><li>nn.Linear( )에는 <u>가중치w와 편향b가 저장</u>되어있다. </li><li>이는 <strong>model.parameters( )</strong>로 불러올 수 있다.</li></ul><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br></pre></td><td class="code"><pre><span class="line"><span class="comment"># 모델을 선언 및 초기화. 단순 선형 회귀이므로 input_dim=1, output_dim=1.</span></span><br><span class="line"><span class="keyword">import</span> torch</span><br><span class="line"><span class="keyword">import</span> torch.nn <span class="keyword">as</span> nn</span><br><span class="line"></span><br><span class="line">model = nn.Linear(<span class="number">1</span>,<span class="number">1</span>)</span><br><span class="line"></span><br><span class="line"><span class="comment"># 출력되는 첫번째값이 w, 두번째값이 b. 랜덤 초기화되어있는 상태이다.</span></span><br><span class="line">print(list(model.parameters()))</span><br></pre></td></tr></table></figure><figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br></pre></td><td class="code"><pre><span class="line">[결과]</span><br><span class="line">[Parameter containing:</span><br><span class="line">tensor([[0.5153]], requires_grad&#x3D;True), Parameter containing:</span><br><span class="line">tensor([-0.4414], requires_grad&#x3D;True)]</span><br></pre></td></tr></table></figure><hr><h3 id="기존-선형회귀-코드의-개선"><a href="#기존-선형회귀-코드의-개선" class="headerlink" title="기존 선형회귀 코드의 개선"></a>기존 선형회귀 코드의 개선</h3><p>: 기존 선형회귀 코드를 다음과 같이 개선할 수 있다.</p><h4 id="개선-1"><a href="#개선-1" class="headerlink" title="개선 1"></a>개선 1</h4><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br></pre></td><td class="code"><pre><span class="line"><span class="comment"># 최종코드</span></span><br><span class="line"><span class="keyword">import</span> torch</span><br><span class="line">torch.manual_seed(<span class="number">1</span>)</span><br></pre></td></tr></table></figure><blockquote><p>torch.nn 까지 import</p></blockquote><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br></pre></td><td class="code"><pre><span class="line"><span class="comment"># 최종코드</span></span><br><span class="line"><span class="keyword">import</span> torch</span><br><span class="line"><span class="keyword">import</span> torch.nn</span><br><span class="line">torch.manual_seed(<span class="number">1</span>)</span><br></pre></td></tr></table></figure><h4 id="개선-2"><a href="#개선-2" class="headerlink" title="개선 2"></a>개선 2</h4><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br></pre></td><td class="code"><pre><span class="line"><span class="keyword">for</span> epoch <span class="keyword">in</span> range(<span class="number">1000</span>):</span><br><span class="line"> </span><br><span class="line">  <span class="comment"># 새 학습값</span></span><br><span class="line">  y_hat = (w * x_train) + b</span><br><span class="line"> </span><br><span class="line">  <span class="comment"># MSE함수통한 비용 계산</span></span><br><span class="line">  cost = torch.mean((y_train - y_hat) ** <span class="number">2</span>)</span><br><span class="line"> </span><br><span class="line">  <span class="comment"># optimizer로 w,b 학습시킴으로서 y_hat 개선</span></span><br><span class="line">  <span class="comment"># gradient를 0으로 초기화</span></span><br><span class="line">  optimizer.zero_grad() </span><br><span class="line">  <span class="comment"># 비용 함수를 미분하여 gradient 계산</span></span><br><span class="line">  cost.backward() </span><br><span class="line">  <span class="comment"># W와 b를 업데이트</span></span><br><span class="line">  optimizer.step()</span><br></pre></td></tr></table></figure><blockquote><p>nn.Linear( )로 선언한 model로 y_hat 계산</p><p>F.mse_loss(prediction, y_train) 파이토치에서 제공하는 평균제곱함수로 cost 계산</p></blockquote><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br><span class="line">23</span><br><span class="line">24</span><br><span class="line">25</span><br><span class="line">26</span><br></pre></td><td class="code"><pre><span class="line">model = nn.Linear(<span class="number">1</span>, <span class="number">1</span>)</span><br><span class="line"> </span><br><span class="line"><span class="keyword">for</span> epoch <span class="keyword">in</span> range(<span class="number">1000</span>):</span><br><span class="line"> </span><br><span class="line">  <span class="comment"># 새 학습값</span></span><br><span class="line">  y_hat = model(x_train)</span><br><span class="line"> </span><br><span class="line">  <span class="comment"># MSE함수통한 비용 계산</span></span><br><span class="line">  cost = F.mse_loss(y_hat, y_train)</span><br><span class="line"> </span><br><span class="line">  <span class="comment"># optimizer로 w,b 학습시킴으로서 y_hat 개선</span></span><br><span class="line">  <span class="comment"># gradient를 0으로 초기화</span></span><br><span class="line">  optimizer.zero_grad() </span><br><span class="line">  <span class="comment"># 비용 함수를 미분하여 gradient 계산</span></span><br><span class="line">  cost.backward() </span><br><span class="line">  <span class="comment"># W와 b를 업데이트</span></span><br><span class="line">  optimizer.step() </span><br><span class="line">    </span><br><span class="line"><span class="comment"># 임의의 입력 4를 선언</span></span><br><span class="line">new_var =  torch.FloatTensor([[<span class="number">4.0</span>]]) </span><br><span class="line"></span><br><span class="line"><span class="comment"># 입력한 값 4에 대해서 예측값 y를 리턴받아서 pred_y에 저장</span></span><br><span class="line">pred_y = model(new_var) <span class="comment"># forward 연산</span></span><br><span class="line"></span><br><span class="line"><span class="comment"># y = 2x 이므로 입력이 4라면 y가 8에 가까운 값이 나와야 제대로 학습이 된 것</span></span><br><span class="line">print(<span class="string">"훈련 후 입력이 4일 때의 예측값 :"</span>, pred_y)</span><br></pre></td></tr></table></figure><figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br></pre></td><td class="code"><pre><span class="line">[결과]</span><br><span class="line">훈련 후 입력이 4일 때의 예측값 : tensor([[7.9989]], grad_fn&#x3D;&lt;AddmmBackward&gt;)</span><br></pre></td></tr></table></figure><ul><li>위의 코드로 학습한 모델 model은 x_train, y_train에 대해 학습된 값 w,b 를 저장하고 있다.</li><li>학습된 모델 model을 활용해 새로운 값 x_new 에 대한 예측값 y_pred를 얻을 수 있다.</li></ul>]]></content:encoded>
      
      <comments>https://katie0809.github.io/2020/02/17/ai-start3/#disqus_thread</comments>
    </item>
    
    <item>
      <title>[딥러닝 기초] Linear Regression</title>
      <link>https://katie0809.github.io/2020/02/17/ai-start2/</link>
      <guid>https://katie0809.github.io/2020/02/17/ai-start2/</guid>
      <pubDate>Mon, 17 Feb 2020 00:18:25 GMT</pubDate>
      <description>
      
        &lt;p&gt;다음의 책을 공부하며 정리한 내용입니다&lt;/p&gt;
&lt;ul&gt;
&lt;li&gt;&lt;a href=&quot;https://wikidocs.net/book/2788&quot; rel=&quot;external nofollow noopener noreferrer&quot; target=&quot;_blank&quot;&gt;https://wikidocs.net/book/2788&lt;/a&gt;&lt;/li&gt;
&lt;/ul&gt;
&lt;h2 id=&quot;선형회귀란&quot;&gt;&lt;a href=&quot;#선형회귀란&quot; class=&quot;headerlink&quot; title=&quot;선형회귀란&quot;&gt;&lt;/a&gt;선형회귀란&lt;/h2&gt;&lt;p&gt;: 선형 회귀란 &lt;code&gt;학습 데이터와 가장 잘 맞는 하나의 직선&lt;/code&gt;을 찾는 일.&lt;/p&gt;
&lt;ul&gt;
&lt;li&gt;하나의 &lt;code&gt;직선&lt;/code&gt;은 &lt;code&gt;W와 b&lt;/code&gt;로 정의할 수 있다.&lt;/li&gt;
&lt;li&gt;선형 회귀의 목표: &lt;code&gt;가장 잘 맞는 직선&lt;/code&gt;을 정의하는 &lt;code&gt;W와 b&lt;/code&gt;의 값을 찾는 것.&lt;/li&gt;
&lt;/ul&gt;
      
      </description>
      
      
      <content:encoded><![CDATA[<p>다음의 책을 공부하며 정리한 내용입니다</p><ul><li><a href="https://wikidocs.net/book/2788" rel="external nofollow noopener noreferrer" target="_blank">https://wikidocs.net/book/2788</a></li></ul><h2 id="선형회귀란"><a href="#선형회귀란" class="headerlink" title="선형회귀란"></a>선형회귀란</h2><p>: 선형 회귀란 <code>학습 데이터와 가장 잘 맞는 하나의 직선</code>을 찾는 일.</p><ul><li>하나의 <code>직선</code>은 <code>W와 b</code>로 정의할 수 있다.</li><li>선형 회귀의 목표: <code>가장 잘 맞는 직선</code>을 정의하는 <code>W와 b</code>의 값을 찾는 것.</li></ul><a id="more"></a><h3 id="파이토치에서의-선형회귀"><a href="#파이토치에서의-선형회귀" class="headerlink" title="파이토치에서의 선형회귀"></a>파이토치에서의 선형회귀</h3><ul><li><p>선형 회귀 모델: nn.Linear()</p></li><li><p>평균 제곱오차: nn.functional.mse_loss()</p><blockquote><p>torch.manual_seed() : 현재의 코드를 재실행해도 다음에도 같은 결과가 나오도록 랜덤 시드(random seed)를 준다.</p></blockquote></li></ul><p>선형회귀 훈련을 위한 기본적인 코드의 뼈대는 아래와 같다</p><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br><span class="line">23</span><br><span class="line">24</span><br><span class="line">25</span><br><span class="line">26</span><br><span class="line">27</span><br></pre></td><td class="code"><pre><span class="line"><span class="comment"># 1.훈련데이터의 선언</span></span><br><span class="line"><span class="comment"># x_train의 벡터가 y_train이 되도록 하는 w와 b를 찾는다</span></span><br><span class="line"><span class="keyword">import</span> torch</span><br><span class="line"></span><br><span class="line">x_train = torch.FloatTensor([[<span class="number">1</span>], [<span class="number">2</span>], [<span class="number">3</span>]])</span><br><span class="line">y_train = torch.FloatTensor([[<span class="number">2</span>], [<span class="number">4</span>], [<span class="number">6</span>]])</span><br><span class="line"></span><br><span class="line"></span><br><span class="line"><span class="comment"># 2.가중치 W와 편향 b를 0으로 초기화</span></span><br><span class="line"><span class="comment"># requires_grad: 학습을 통해 값이 변경되는 변수임을 명시.</span></span><br><span class="line">W = torch.zeros(<span class="number">1</span>, requires_grad=<span class="literal">True</span>)</span><br><span class="line">b = torch.zeros(<span class="number">1</span>, requires_grad=<span class="literal">True</span>)</span><br><span class="line"></span><br><span class="line"></span><br><span class="line"><span class="comment"># 3.직선의 방정식(=선형회귀)에 해당되는 가설을 선언한다.</span></span><br><span class="line"><span class="comment"># 가설 = 시스템이 학습한 w,b로 예측한 y_hat값</span></span><br><span class="line">hypothesis = x_train * W + b</span><br><span class="line"><span class="comment"># print(hypothesis)</span></span><br><span class="line"></span><br><span class="line"></span><br><span class="line"><span class="comment"># 4.사용할 비용함수 선언(MSE)</span></span><br><span class="line">cost = torch.mean((hypothesis - y_train) ** <span class="number">2</span>) </span><br><span class="line"></span><br><span class="line"></span><br><span class="line"><span class="comment"># 5.W와 b를 SGD(경사하강법: Stochastic Gradient Descent)로 훈련시킨다</span></span><br><span class="line"><span class="comment"># lr: learning rate</span></span><br><span class="line">optimizer = torch.optim.SGD([W, b], lr=<span class="number">0.01</span>)</span><br></pre></td></tr></table></figure><p>이를 바탕으로 실제 동작하는 선형회귀 모델을 제작해 학습을 진행해보면 아래와 같다.</p><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br><span class="line">23</span><br><span class="line">24</span><br><span class="line">25</span><br><span class="line">26</span><br><span class="line">27</span><br><span class="line">28</span><br><span class="line">29</span><br><span class="line">30</span><br><span class="line">31</span><br><span class="line">32</span><br><span class="line">33</span><br><span class="line">34</span><br><span class="line">35</span><br><span class="line">36</span><br><span class="line">37</span><br><span class="line">38</span><br><span class="line">39</span><br><span class="line">40</span><br></pre></td><td class="code"><pre><span class="line"><span class="comment"># 최종코드</span></span><br><span class="line"><span class="keyword">import</span> torch</span><br><span class="line">torch.manual_seed(<span class="number">1</span>)</span><br><span class="line"></span><br><span class="line"><span class="comment"># 훈련데이터 선언</span></span><br><span class="line">x_train = torch.FloatTensor([[<span class="number">1</span>], [<span class="number">2</span>], [<span class="number">3</span>]])</span><br><span class="line">y_train = torch.FloatTensor([[<span class="number">2</span>], [<span class="number">4</span>], [<span class="number">6</span>]])</span><br><span class="line"></span><br><span class="line"><span class="comment"># 학습데이터 w,b 선언. 둘다 값이 1인 임의의 스칼라 텐서</span></span><br><span class="line">w = torch.zeros(<span class="number">1</span>, requires_grad=<span class="literal">True</span>)</span><br><span class="line">b = torch.zeros(<span class="number">1</span>, requires_grad=<span class="literal">True</span>)</span><br><span class="line"></span><br><span class="line"></span><br><span class="line"><span class="comment"># SGD방식을 사용한 최적화 선언</span></span><br><span class="line">optimizer = torch.optim.SGD([w, b], lr=<span class="number">0.01</span>)</span><br><span class="line"></span><br><span class="line"><span class="comment"># 학습 횟수는 1000+1회</span></span><br><span class="line">tot_epoch = <span class="number">1000</span></span><br><span class="line"></span><br><span class="line"><span class="keyword">for</span> cur_epoch <span class="keyword">in</span> range(tot_epoch + <span class="number">1</span>):</span><br><span class="line">  </span><br><span class="line">  <span class="comment"># 새 학습값</span></span><br><span class="line">  y_hat = (w * x_train) + b</span><br><span class="line"></span><br><span class="line">  <span class="comment"># MSE함수통한 비용 계산</span></span><br><span class="line">  cost = torch.mean((y_train - y_hat) ** <span class="number">2</span>)</span><br><span class="line"></span><br><span class="line">  <span class="comment"># optimizer로 w,b 학습시킴으로서 y_hat 개선</span></span><br><span class="line">  <span class="comment"># gradient를 0으로 초기화</span></span><br><span class="line">  optimizer.zero_grad() </span><br><span class="line">  <span class="comment"># 비용 함수를 미분하여 gradient 계산</span></span><br><span class="line">  cost.backward() </span><br><span class="line">  <span class="comment"># W와 b를 업데이트</span></span><br><span class="line">  optimizer.step() </span><br><span class="line"></span><br><span class="line">  <span class="comment"># 100번마다 로그 출력</span></span><br><span class="line">  <span class="keyword">if</span> cur_epoch % <span class="number">100</span> == <span class="number">0</span>:</span><br><span class="line">      print(<span class="string">'Epoch &#123;:4d&#125;/&#123;&#125; w: &#123;:.3f&#125;, b: &#123;:.3f&#125; Cost: &#123;:.6f&#125;'</span>.format(</span><br><span class="line">          cur_epoch, tot_epoch, w.item(), b.item(), cost.item()</span><br><span class="line">      ))</span><br></pre></td></tr></table></figure><figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br></pre></td><td class="code"><pre><span class="line">[결과]</span><br><span class="line">Epoch    0&#x2F;1000 w: 0.187, b: 0.080 Cost: 18.666666</span><br><span class="line">Epoch  100&#x2F;1000 w: 1.746, b: 0.578 Cost: 0.048171</span><br><span class="line">Epoch  200&#x2F;1000 w: 1.800, b: 0.454 Cost: 0.029767</span><br><span class="line">Epoch  300&#x2F;1000 w: 1.843, b: 0.357 Cost: 0.018394</span><br><span class="line">Epoch  400&#x2F;1000 w: 1.876, b: 0.281 Cost: 0.011366</span><br><span class="line">Epoch  500&#x2F;1000 w: 1.903, b: 0.221 Cost: 0.007024</span><br><span class="line">Epoch  600&#x2F;1000 w: 1.924, b: 0.174 Cost: 0.004340</span><br><span class="line">Epoch  700&#x2F;1000 w: 1.940, b: 0.136 Cost: 0.002682</span><br><span class="line">Epoch  800&#x2F;1000 w: 1.953, b: 0.107 Cost: 0.001657</span><br><span class="line">Epoch  900&#x2F;1000 w: 1.963, b: 0.084 Cost: 0.001024</span><br><span class="line">Epoch 1000&#x2F;1000 w: 1.971, b: 0.066 Cost: 0.000633</span><br></pre></td></tr></table></figure><p>위의 코드와 결과는 다음을 의미한다.</p><ul><li>x가 [1, 2, 3]일때 y가 [2,4,6]이 되는 w와 b는 2, 0이다.</li><li>학습을 통해 최종적으로 찾은 결과 w,b는 1.971, 0.066이므로 어느정도 답을 찾아냈다고 볼 수 있다.</li></ul><hr><h3 id="다중선형회귀"><a href="#다중선형회귀" class="headerlink" title="다중선형회귀"></a>다중선형회귀</h3><p>: 기존의 선형회귀가 y = wx + b의 <u>w,b를 찾는 모델</u>이었다면, 다중선형회귀는 <strong>y = w1x1 + w2x2 + w3x3 + b</strong>의 <code>w1, w2, w3, b를 찾는 모델</code>이다.</p><p>선형회귀 훈련을 위한 기본적인 코드의 뼈대는 아래와 같다.</p><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br></pre></td><td class="code"><pre><span class="line"><span class="comment"># N개의 x_train 벡터와 가중치 w는 행렬로 표현할 수 있다.</span></span><br><span class="line"><span class="comment"># 5 * 3 학습벡터 =&gt; 길이 5의 x_train벡터 3개</span></span><br><span class="line">x_train  =  torch.FloatTensor([[<span class="number">73</span>,  <span class="number">80</span>,  <span class="number">75</span>], </span><br><span class="line">                               [<span class="number">93</span>,  <span class="number">88</span>,  <span class="number">93</span>], </span><br><span class="line">                               [<span class="number">89</span>,  <span class="number">91</span>,  <span class="number">90</span>], </span><br><span class="line">                               [<span class="number">96</span>,  <span class="number">98</span>,  <span class="number">100</span>],   </span><br><span class="line">                               [<span class="number">73</span>,  <span class="number">66</span>,  <span class="number">70</span>]])  </span><br><span class="line"></span><br><span class="line"><span class="comment"># y_train벡터의 길이 = x_train벡터의 길이(5)</span></span><br><span class="line">y_train  =  torch.FloatTensor([[<span class="number">152</span>],  [<span class="number">185</span>],  [<span class="number">180</span>],  [<span class="number">196</span>],  [<span class="number">142</span>]])</span><br><span class="line"></span><br><span class="line"><span class="comment"># 가중치 w의 개수 = x_train벡터의 개수(3) ** 길이는 1 **</span></span><br><span class="line">w = torch.zeros((<span class="number">3</span>, <span class="number">1</span>) ,requires_grad=<span class="literal">True</span>)</span><br><span class="line">b = torch.zeros(<span class="number">1</span>, requires_grad=<span class="literal">True</span>)</span><br><span class="line"></span><br><span class="line"><span class="comment"># x_train과 y_train벡터가 각각 행렬이므로</span></span><br><span class="line"><span class="comment"># 가설은 파이토치 행렬곱을 활용해 정의해준다. </span></span><br><span class="line">hypothesis = x_train.matmul(w) + b</span><br></pre></td></tr></table></figure><p>이를 바탕으로 실제 동작하는 다중선형회귀 모델을 제작해 학습을 진행해보면 아래와 같다.</p><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br><span class="line">23</span><br><span class="line">24</span><br><span class="line">25</span><br><span class="line">26</span><br><span class="line">27</span><br><span class="line">28</span><br><span class="line">29</span><br><span class="line">30</span><br><span class="line">31</span><br><span class="line">32</span><br><span class="line">33</span><br></pre></td><td class="code"><pre><span class="line"><span class="comment"># 학습을 위한 코드는 기존과 동일하다.</span></span><br><span class="line"><span class="keyword">import</span> torch</span><br><span class="line">torch.manual_seed(<span class="number">1</span>)</span><br><span class="line"></span><br><span class="line">x_train  =  torch.FloatTensor([[<span class="number">73</span>,  <span class="number">80</span>,  <span class="number">75</span>], </span><br><span class="line">                               [<span class="number">93</span>,  <span class="number">88</span>,  <span class="number">93</span>], </span><br><span class="line">                               [<span class="number">89</span>,  <span class="number">91</span>,  <span class="number">90</span>], </span><br><span class="line">                               [<span class="number">96</span>,  <span class="number">98</span>,  <span class="number">100</span>],   </span><br><span class="line">                               [<span class="number">73</span>,  <span class="number">66</span>,  <span class="number">70</span>]])  </span><br><span class="line">y_train  =  torch.FloatTensor([[<span class="number">152</span>],  [<span class="number">185</span>],  [<span class="number">180</span>],  [<span class="number">196</span>],  [<span class="number">142</span>]])</span><br><span class="line"></span><br><span class="line"><span class="comment"># 모델 초기화</span></span><br><span class="line">w = torch.zeros((<span class="number">3</span>, <span class="number">1</span>) ,requires_grad=<span class="literal">True</span>)</span><br><span class="line">b = torch.zeros(<span class="number">1</span>, requires_grad=<span class="literal">True</span>)</span><br><span class="line"></span><br><span class="line"><span class="comment"># SGD방식을 사용한 최적화 선언</span></span><br><span class="line">optimizer = torch.optim.SGD([w, b], lr=<span class="number">1e-5</span>)</span><br><span class="line"></span><br><span class="line">tot_epoch = <span class="number">1000</span></span><br><span class="line"><span class="keyword">for</span> cur_epoch <span class="keyword">in</span> range(tot_epoch + <span class="number">1</span>):</span><br><span class="line"></span><br><span class="line">  y_hat = x_train.matmul(w) + b</span><br><span class="line">  cost = torch.mean((y_train - y_hat) ** <span class="number">2</span>)</span><br><span class="line"></span><br><span class="line">  optimizer.zero_grad()</span><br><span class="line">  cost.backward()</span><br><span class="line">  optimizer.step()</span><br><span class="line"></span><br><span class="line">  <span class="comment"># 100번마다 로그 출력</span></span><br><span class="line">  <span class="keyword">if</span> cur_epoch % <span class="number">100</span> == <span class="number">0</span>:</span><br><span class="line">      print(<span class="string">'Epoch &#123;:4d&#125;/&#123;&#125; Cost: &#123;:.6f&#125;'</span>.format(</span><br><span class="line">          cur_epoch, tot_epoch, cost.item()</span><br><span class="line">      ))</span><br></pre></td></tr></table></figure><figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br></pre></td><td class="code"><pre><span class="line">[결과]</span><br><span class="line">Epoch    0&#x2F;1000 Cost: 29661.800781</span><br><span class="line">Epoch  100&#x2F;1000 Cost: 1.563628</span><br><span class="line">Epoch  200&#x2F;1000 Cost: 1.497595</span><br><span class="line">Epoch  300&#x2F;1000 Cost: 1.435044</span><br><span class="line">Epoch  400&#x2F;1000 Cost: 1.375726</span><br><span class="line">Epoch  500&#x2F;1000 Cost: 1.319507</span><br><span class="line">Epoch  600&#x2F;1000 Cost: 1.266222</span><br><span class="line">Epoch  700&#x2F;1000 Cost: 1.215703</span><br><span class="line">Epoch  800&#x2F;1000 Cost: 1.167810</span><br><span class="line">Epoch  900&#x2F;1000 Cost: 1.122429</span><br><span class="line">Epoch 1000&#x2F;1000 Cost: 1.079390</span><br></pre></td></tr></table></figure>]]></content:encoded>
      
      <comments>https://katie0809.github.io/2020/02/17/ai-start2/#disqus_thread</comments>
    </item>
    
    <item>
      <title>[딥러닝 기초] Pytorch 기본연산</title>
      <link>https://katie0809.github.io/2020/02/17/ai-start1/</link>
      <guid>https://katie0809.github.io/2020/02/17/ai-start1/</guid>
      <pubDate>Mon, 17 Feb 2020 00:08:25 GMT</pubDate>
      <description>
      
        &lt;p&gt;다음의 책을 공부하며 정리한 내용입니다&lt;/p&gt;
&lt;ul&gt;
&lt;li&gt;&lt;a href=&quot;https://wikidocs.net/book/2788&quot; rel=&quot;external nofollow noopener noreferrer&quot; target=&quot;_blank&quot;&gt;https://wikidocs.net/book/2788&lt;/a&gt;&lt;/li&gt;
&lt;/ul&gt;
&lt;h2 id=&quot;Pytorch-기본-연산&quot;&gt;&lt;a href=&quot;#Pytorch-기본-연산&quot; class=&quot;headerlink&quot; title=&quot;Pytorch 기본 연산&quot;&gt;&lt;/a&gt;Pytorch 기본 연산&lt;/h2&gt;&lt;p&gt;: pytorch를 활용한 신경망 구성을 위해 필수적인 &lt;strong&gt;딥러닝 기본 연산단위&lt;/strong&gt;를 알아보자.&lt;/p&gt;
      
      </description>
      
      
      <content:encoded><![CDATA[<p>다음의 책을 공부하며 정리한 내용입니다</p><ul><li><a href="https://wikidocs.net/book/2788" rel="external nofollow noopener noreferrer" target="_blank">https://wikidocs.net/book/2788</a></li></ul><h2 id="Pytorch-기본-연산"><a href="#Pytorch-기본-연산" class="headerlink" title="Pytorch 기본 연산"></a>Pytorch 기본 연산</h2><p>: pytorch를 활용한 신경망 구성을 위해 필수적인 <strong>딥러닝 기본 연산단위</strong>를 알아보자.</p><a id="more"></a><h3 id="Tensor"><a href="#Tensor" class="headerlink" title="Tensor"></a>Tensor</h3><ul><li>딥러닝의 가장 기본적인 연산단위 : <code>벡터</code>, <code>행렬</code>, <code>텐서</code><ul><li>0차원 : 스칼라</li><li>1차원 : 벡터</li><li>2차원 : 행렬</li><li>3차원 이상 : 텐서</li></ul></li></ul><h4 id="2D-Tensor-행렬"><a href="#2D-Tensor-행렬" class="headerlink" title="2D Tensor : 행렬"></a><strong>2D Tenso</strong>r : 행렬</h4><p><img src="https://wikidocs.net/images/page/52460/tensor3.PNG" alt></p><p>2차원 텐서는 말그대로 ‘<strong>행렬</strong>‘이다. 따라서 2차원 텐서 t는 다음과 같이 나타낼 수 있다.</p><blockquote><p>|t| = (배치 사이즈, 차원)</p></blockquote><ul><li><p>행렬의 행의 개수 = 배치사이즈</p></li><li><p>행렬의 열의 개수 = 차원(dimension)</p></li></ul><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br></pre></td><td class="code"><pre><span class="line">t = tor.FloatTensor([<span class="number">0</span>,<span class="number">1</span>,<span class="number">2</span>,<span class="number">3</span>])</span><br><span class="line">print(t)</span><br><span class="line">print(t.dim(), t.size()) <span class="comment"># rank(차원), 원소개수</span></span><br><span class="line"></span><br><span class="line"><span class="comment"># 인덱스로 접근 가능하다</span></span><br><span class="line">print(t[<span class="number">1</span>])</span><br><span class="line"></span><br><span class="line"><span class="comment"># 정수 텐서</span></span><br><span class="line">lt = tor.LongTensor([[<span class="number">1</span>, <span class="number">2</span>, <span class="number">3</span>],</span><br><span class="line">                       [<span class="number">4</span>, <span class="number">5</span>, <span class="number">6</span>],</span><br><span class="line">                       [<span class="number">7</span>, <span class="number">8</span>, <span class="number">9</span>],</span><br><span class="line">                       [<span class="number">10</span>, <span class="number">11</span>, <span class="number">12</span>]</span><br><span class="line">                      ])</span><br><span class="line">print(lt)</span><br><span class="line">print(lt[<span class="number">2</span>], lt[<span class="number">2</span>].size())</span><br></pre></td></tr></table></figure><figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br></pre></td><td class="code"><pre><span class="line">tensor([0., 1., 2., 3.])</span><br><span class="line">1 torch.Size([4])</span><br><span class="line">tensor(1.)</span><br><span class="line">tensor([[ 1,  2,  3],</span><br><span class="line">        [ 4,  5,  6],</span><br><span class="line">        [ 7,  8,  9],</span><br><span class="line">        [10, 11, 12]])</span><br><span class="line">tensor([7, 8, 9]) torch.Size([3])</span><br></pre></td></tr></table></figure><h4 id="3D-Tensor"><a href="#3D-Tensor" class="headerlink" title="3D Tensor"></a><strong>3D Tensor</strong></h4><p><img src="https://wikidocs.net/images/page/52460/tensor5.PNG" alt></p><p>이미지/영상처리 분야에서는 보다 복잡한 텐서를 다룬다.</p><p>이미지는 가로/세로가 존재하며, 따라서 여러장의 이미지는 자연스레 <code>(가로, 세로, 배치 크기)</code> 가 됨을 연상할 수 있다.</p><h4 id="3D-Tensor-in-NLP"><a href="#3D-Tensor-in-NLP" class="headerlink" title="3D Tensor in NLP"></a><strong>3D Tensor in NLP</strong></h4><p>Natural Language Processing(자연어처리)에서는 보통 <code>(문장길이, 차원, 배치 크기)</code> 라는 3차원 텐서를 사용한다.</p>]]></content:encoded>
      
      <comments>https://katie0809.github.io/2020/02/17/ai-start1/#disqus_thread</comments>
    </item>
    
    <item>
      <title>YAMLException: can not read a block mapping entry, Hexo</title>
      <link>https://katie0809.github.io/2020/02/16/yaml-exception/</link>
      <guid>https://katie0809.github.io/2020/02/16/yaml-exception/</guid>
      <pubDate>Sun, 16 Feb 2020 09:32:45 GMT</pubDate>
      <description>
      
        &lt;p&gt;How to solve problem&lt;/p&gt;
&lt;blockquote&gt;
&lt;p&gt;YAMLException: can not read a block mapping entry&lt;/p&gt;
&lt;/blockquote&gt;
      
      </description>
      
      
      <content:encoded><![CDATA[<p>How to solve problem</p><blockquote><p>YAMLException: can not read a block mapping entry</p></blockquote><a id="more"></a><p>포스팅을 하면서 다음과 같은 에러가 발생했다.</p><p><img src="/image/screenshot1.png" alt="스크린샷 2020-02-16 오후 6.35.44"></p><p>이는 아래와 같이 Hexo로 신규 게시물 작성 시 title에 [] 대괄호를 사용하면 발생하는 에러이다. (,나 :같은 기호를 사용해도 발생한다.)</p><p>The error above occurs because of the [] square brackets used in a title of your post.</p><p><img src="/image/screenshot2.png" alt></p><p>따라서 위의 타이틀을 “” 로 묶어주면 에러를 해결할 수 있다.</p><p>Just add a title with double quotation marks to solve the problem.</p><figure class="highlight bash"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line">title : <span class="string">"[딥러닝 스터디] 임베딩이란"</span></span><br></pre></td></tr></table></figure>]]></content:encoded>
      
      <comments>https://katie0809.github.io/2020/02/16/yaml-exception/#disqus_thread</comments>
    </item>
    
    <item>
      <title>[딥러닝 스터디] 자연어의 계산과 이해</title>
      <link>https://katie0809.github.io/2020/02/16/ai-study2/</link>
      <guid>https://katie0809.github.io/2020/02/16/ai-study2/</guid>
      <pubDate>Sun, 16 Feb 2020 07:42:21 GMT</pubDate>
      <description>
      
        &lt;p&gt;다음의 책을 공부하며 정리한 내용입니다&lt;/p&gt;
&lt;ul&gt;
&lt;li&gt;한국어 임베딩 - 이기창&lt;/li&gt;
&lt;li&gt;&lt;a href=&quot;https://wikidocs.net/21668&quot; rel=&quot;external nofollow noopener noreferrer&quot; target=&quot;_blank&quot;&gt;https://wikidocs.net/21668&lt;/a&gt;&lt;/li&gt;
&lt;li&gt;&lt;a href=&quot;https://wikidocs.net/21687&quot; rel=&quot;external nofollow noopener noreferrer&quot; target=&quot;_blank&quot;&gt;https://wikidocs.net/21687&lt;/a&gt;&lt;/li&gt;
&lt;li&gt;&lt;a href=&quot;https://wikidocs.net/21692&quot; rel=&quot;external nofollow noopener noreferrer&quot; target=&quot;_blank&quot;&gt;https://wikidocs.net/21692&lt;/a&gt;&lt;/li&gt;
&lt;/ul&gt;
&lt;h2 id=&quot;2장-언어모델이란&quot;&gt;&lt;a href=&quot;#2장-언어모델이란&quot; class=&quot;headerlink&quot; title=&quot;2장. 언어모델이란&quot;&gt;&lt;/a&gt;2장. 언어모델이란&lt;/h2&gt;&lt;p&gt;: 자연어의 의미를 임베딩에 어떻게 녹여낼 수 있는가? 그 비결은 &lt;code&gt;자연어의 통계적 패턴&lt;/code&gt; 을 통째로 임베딩에 넣는 것이다.&lt;/p&gt;
      
      </description>
      
      
      <content:encoded><![CDATA[<p>다음의 책을 공부하며 정리한 내용입니다</p><ul><li>한국어 임베딩 - 이기창</li><li><a href="https://wikidocs.net/21668" rel="external nofollow noopener noreferrer" target="_blank">https://wikidocs.net/21668</a></li><li><a href="https://wikidocs.net/21687" rel="external nofollow noopener noreferrer" target="_blank">https://wikidocs.net/21687</a></li><li><a href="https://wikidocs.net/21692" rel="external nofollow noopener noreferrer" target="_blank">https://wikidocs.net/21692</a></li></ul><h2 id="2장-언어모델이란"><a href="#2장-언어모델이란" class="headerlink" title="2장. 언어모델이란"></a>2장. 언어모델이란</h2><p>: 자연어의 의미를 임베딩에 어떻게 녹여낼 수 있는가? 그 비결은 <code>자연어의 통계적 패턴</code> 을 통째로 임베딩에 넣는 것이다.</p><a id="more"></a><p>임베딩을 만드는 세가지 철학</p><table><thead><tr><th>구분</th><th>bag of words 가정</th><th>언어 모델</th><th>분포 가정</th></tr></thead><tbody><tr><td>내용</td><td>어떤 단어가 (많이) 쓰였는가</td><td>단어가 어떤 순서로 쓰였는가</td><td>어떤 단어가 같이 쓰였는가</td></tr><tr><td>대표 통계량</td><td>TF-IDF</td><td>-</td><td>PMI</td></tr><tr><td>대표 모델</td><td>Deep Averaging Network</td><td>ELMo, GPT</td><td>Word2Vec</td></tr></tbody></table><p>자연어의 의미는 해당 언어 사용자들이 실제 사용하는 일상 언어에서 드러난다. 따라서 실제 사람이 사용하는 자연어의 통계적 패턴정보를 임베딩에 넣는다면 임베딩에 자연어의 의미를 함축해 넣을 수 있다.</p><p>임베딩을 만들때 쓰는 통계정보는 크게 3가지가 있다.</p><p>첫째, 문장에 어떤 단어가 <code>(많이) 쓰였</code>는가</p><p>둘째, 단어가 어떤 <code>순서</code>로 등장하는가</p><p>셋째, 어떤 단어가 <code>같이</code> 나타났는가</p><hr><h3 id="2-1-어떤-단어가-많이-쓰였는가"><a href="#2-1-어떤-단어가-많이-쓰였는가" class="headerlink" title="2-1. 어떤 단어가 많이 쓰였는가"></a>2-1. 어떤 단어가 많이 쓰였는가</h3><ol><li><p>Bag of words 가정</p><ul><li>단어의 <em>등장 순서에 관계없이</em> 문서 내 <em>단어의 등장 빈도</em> 를 임베딩으로 쓰는 기법</li><li>저자가 생각한 <code>주제</code> 가 <code>분서에서의 단어 사용</code> 에 녹아들어있다는 가정을 바탕으로 한다.</li><li>정보 검색 분야에서 많이 사용한다. 사용자 질의를 백오브워즈 임베딩으로 변환 후 코사인 유사도가 가장 높은 문서를 사용자에게 노출한다.</li></ul></li><li><p>TF - IDF</p><ul><li>문서에 단순히 <strong>많이 나타나는</strong> 단어만으로 주제를 판단하기 어려울 수 있다.(예: 한국어 문서에는 조사 ‘을/를’이 많이 등장하지만 이를 통해 주제 파악은 어려움.)</li><li>이를 보안하기 위해 나타난 기법이 TF-IDF(Term Frequency-Inverse Document Frequency)</li><li><code>TF</code> : 어떤 단어가 특정 문서에 얼마나 쓰였는지의 빈도</li><li><code>DF</code> : 특정 단어가 나타난 문서의 수 </li><li><code>IDF</code> : log( 전체 문서의 수 / 특정 단어의 DF ) <ul><li>값이 클수록 특이한 단어임을 의미</li><li>단어가 <em>문서의 주제와 연관있을 정도</em>(주제 예측능력) 와 관련있다.</li></ul></li><li>단어의 주제 예측능력이 클수록 TF-IDF 값이 커진다.</li></ul></li><li><p>Deep Averaging Network</p><ul><li>백오브워즈 가정의 신경망 버전</li></ul></li></ol><hr><h3 id="2-2-단어가-어떤-순서로-쓰였는가"><a href="#2-2-단어가-어떤-순서로-쓰였는가" class="headerlink" title="2-2. 단어가 어떤 순서로 쓰였는가"></a>2-2. 단어가 어떤 순서로 쓰였는가</h3><p><img src="https://wikidocs.net/images/page/21668/%EB%94%A5_%EB%9F%AC%EB%8B%9D%EC%9D%84_%EC%9D%B4%EC%9A%A9%ED%95%9C.PNG" alt></p><ol start="0"><li>언어 모델이란?<ul><li>언어 모델이란 딥러닝과 관계없이 이전부터 있었던 개념으로, 언어를 모델링하고자 <code>단어 시퀀스</code> 에 <code>확률을 부여</code> 하는 모델이다. </li><li>단어의 등장 순서를 무시하는 백오브 워즈와 달리 <code>시퀀스 정보를 명시적으로 학습</code> 한다. <u>따라서 백오브 워즈의 대척점</u>에 언어모델이 있다고 할 수 있다.</li><li>언어모델을 만드는 방법으로는 크게 1)통계를 이용한 방법과 2)신경망을 이용한 방법이 있다. </li></ul></li></ol><blockquote><p>잘 학습된 언어모델은 <u>어떤 문장이 더 자연스러운지</u>, 또한 주어진 단어 시퀀스 <u>다음에는 무엇이 오는게 자연스러운지</u>를 알수있다.</p></blockquote><p>이와 유사한 맥락에서 일각에서는 언어 모델을 <strong>문법(grammar)</strong> 이라 비유하기도 한다. 단어간의 조합이 얼마나 적절한지, 특정 문장이 얼마나 자연스러운지를 알려주는 언어모델의 역할이 마치 문법의 기능과 유사하기 때문이다.</p><p>단어가 n개 주어진 상황이라면 <code>언어모델은 n개 단어가 동시에 나타날 확률</code> , 즉 P(w1, w2… wn)을 반환한다.(단어 시퀀스에의 확률 할당)</p><p>이는 다음과 같이 사용할 수 있다. </p><h4 id="a-기계-번역-Machine-Translation"><a href="#a-기계-번역-Machine-Translation" class="headerlink" title="a. 기계 번역(Machine Translation):"></a><strong>a. 기계 번역(Machine Translation):</strong></h4><blockquote><p>P(나는 버스를 탔다) &gt; P(나는 버스를 태운다)</p></blockquote><p>: 언어 모델은 두 문장을 비교하여 좌측의 문장의 확률이 더 높다고 판단합니다. </p><h4 id="b-오타-교정-Spell-Correction"><a href="#b-오타-교정-Spell-Correction" class="headerlink" title="b. 오타 교정(Spell Correction)"></a><strong>b. 오타 교정(Spell Correction)</strong></h4><p>선생님이 교실로 부리나케</p><blockquote><p>P(달려갔다) &gt; P(잘려갔다)</p></blockquote><p>: 언어 모델은 두 문장을 비교하여 좌측의 문장의 확률이 더 높다고 판단합니다. </p><h4 id="c-음성-인식-Speech-Recognition"><a href="#c-음성-인식-Speech-Recognition" class="headerlink" title="c. 음성 인식(Speech Recognition)"></a><strong>c. 음성 인식(Speech Recognition)</strong></h4><blockquote><p>P(나는 메롱을 먹는다) &lt; P(나는 메론을 먹는다)</p></blockquote><p>: 언어 모델은 두 문장을 비교하여 우측의 문장의 확률이 더 높다고 판단합니다.</p><p>언어 모델은 위와 같이 확률을 통해 <strong>보다 적절한 문장을 판단한다.</strong></p><hr><h3 id="정리"><a href="#정리" class="headerlink" title="정리"></a>정리</h3><ul><li><p>언어를 모델링하고자 단어 시퀀스에 확률을 부여하는 모델로, 잘 학습된 언어모델은 어떤 시퀀스가 자연스러운지를 판단해낸다.</p></li><li><p>인간이 쓰는 자연어는 레이블이 없는 비지도 학습. 이를 지도학습과 같이 학습하도록 여러 방법을 사용</p></li><li><p>문제1) 비지도학습인 자연어를 어떻게 학습할 것인가?</p><p>: 이전 단어들이 주어졌을 때 다음 단어를 예측하도록 한다.</p></li><li><p>문제2) 각 단어시퀀스에 확률은 어떻게 할당할 것인가?</p><p>: 카운트 기반 접근방식 사용한다.</p></li></ul><hr><h3 id="언어모델의-종류"><a href="#언어모델의-종류" class="headerlink" title="언어모델의 종류"></a>언어모델의 종류</h3><ol><li><p>통계 기반 언어 모델(SLM: Statistical Language Model)</p><ul><li>통계 기반 언어모델은 말뭉치에서 해당 <code>단어 시퀀스가 얼마나 자주 등장하는지</code> 의 빈도를 세어서 학습한다.</li><li>문장은 문맥이라는 관계 내에서 단어들이 관계를 갖고 완성해낸 시퀀스이다. 따라서 특정 문장의 확률은 각 단어들의 <code>이전 단어가 주어졌을때 다음 단어로 등장할 확률의 곱</code> 으로 계산된다.</li></ul></li></ol><p>   즉, “나는 사과를 먹었다” 라는 문장의 확률은 다음과 같이 표현할 수 있다. (문장의 확률을 구하기 위해 다음 단어에 대한 예측 확률을 모두 곱한다.)</p><p>   <code>P(나는 사과를 먹었다) = P(나는) * P(사과를|나는) * P(먹었다|나는, 사과를)</code></p><blockquote><p>조건부 확률은 두개의 확률 P(A), P(B)에 대해 다음의 관계를 갖는다.</p><p>P(B|A)=P(A,B)/P(A)</p><p>P(A,B)=P(A)P(B|A)</p><p>더 많은 확률에 대해 일반화하면 다음과 같이 표현할 수 있다.</p><p>P(x1,x2,x3…xn)=P(x1)P(x2|x1)P(x3|x1,x2)…P(xn|x1…xn−1)</p><p>이를 조건부 확률의 연쇄법칙(chain rule) 이라고 한다.</p></blockquote><ul><li><p>이때 특정 시퀀스로부터 다음 단어가 나올 확률은 <strong>카운트에 기반해 계산</strong>할 수 있다.</p><p><img src="/image/screenshot3.png" alt></p></li></ul><ol start="2"><li><p>N-gram 언어모델</p><ul><li><p>n-gram 언어모델은 통계 기반 언어모델의 일종으로 전통적 SLM과 같이 카운트에 기반한 통계적 접근을 사용한다. </p><p>: 이전의 <u>n-1개의 단어</u>를 보고 <strong>n번째 단어를 예측</strong>하는 방식</p></li><li><p>전통적 SLM과 달리 이전에 등장한 모든 단어가 아닌 <code>일부 단어만 고려</code> 하는 방법을 사용한다.</p></li><li><p>즉, n-gram에서 n은 n개의 단어, 혹은 n-gram에 기반한 언어모델을 의미한다. 말뭉치(corpus) 내 단어들을 <strong>n개씩 묶어서</strong> 그 빈도를 학습했다는 의미이다. </p></li></ul></li></ol><p>   <strong>한국어 언어 모델 예시</strong></p><table><thead><tr><th>문장</th><th>확률</th></tr></thead><tbody><tr><td>진이는 이 책을 세 번 읽었다</td><td>0.47</td></tr><tr><td>이 책이 진이한테 세 번 읽혔다</td><td>0.23</td></tr><tr><td>세 번이 진이한테 이 책을 읽혔다</td><td>0.07</td></tr></tbody></table><p>   : 자연스러운 한국어 문장에 높은 확률값을 부여한다.</p><table><thead><tr><th>표현</th><th>빈도</th></tr></thead><tbody><tr><td>영원히</td><td>104</td></tr><tr><td>기억될</td><td>29</td></tr><tr><td>최고의</td><td>3503</td></tr><tr><td>명작이다</td><td>298</td></tr><tr><td>영원히 기억될</td><td>7</td></tr><tr><td>기억될 최고의</td><td>1</td></tr><tr><td>최고의 명작이다</td><td>23</td></tr><tr><td>기억될 최고의 명작이다</td><td>17</td></tr><tr><td>영원히 기억될 최고의 명작이다</td><td>0</td></tr></tbody></table><ul><li>전통적 SLM에서는 문법적으로 전혀 문제가 없는 ‘영원히 기억될 최고의 명작이다’ 라는 문장에 0의 확률을 부여하게 된다. =&gt; 이 말뭉치에 한번도 나타난 적 없기 때문.</li><li>n-gram모델을 통해 이 문제를 일부 해결할 수 있다.</li><li><code>직전 n-1개 단어의 등장확률</code> 로 <strong>전체 단어 시퀀스 등장 확률</strong> 을 <strong><code>근사</code></strong> 하는 것이다!</li></ul><p>   ‘<strong>영원히 기억될 최고의</strong>‘ 시퀀스 뒤에 <code>명작이다</code> 라는 단어가 올 확률을 trigram으로 근사해보면 얼마일까?</p><p>   P(명작이다 | 영원히, 기억될, 최고의) <del>(유사)</del> P(명작이다 | 기억될, 최고의)</p><p>   = Freq(기억될, 최고의, 명작이다) / Freq(명작이다)</p><p>   = 17 / 298</p><ul><li>위와 같이 단어를 슬라이딩 해가면서 수식을 풀어 생각한다면 전체 문장 ‘영원히 기억될 최고의 작품이다’를 trigram으로 계산하기 위한 수식은 다음과 같다.</li><li><code>영원히 기억될</code> 이 등장할 확률 * <code>영원히 기억될</code> 뒤에 <code>최고의</code> 가 등장할 확률 * <code>기억될 최고의</code> 뒤에 <code>명작이다</code> 가 등장할 확률</li></ul><p>   [ 카운트 기반 접근방식의 <strong>본질적 한계</strong> ]</p><ul><li><p><strong><code>희소문제</code></strong> : Sparcity Problem</p><ul><li><p>여전히 희소문제는 존재한다. 코퍼스 내에 단어시퀀스가 없을(카운트하지 못할) 확률은 여전히 존재.</p></li><li><p>n의 선택은 trade-off : n의 크기를 키우면 예측의 정확도는 높아지지만, 코퍼스에서 해당 n개의 시퀀스를 카운트할 확률은 낮아짐(희소문제 증가), 모델사이즈 커짐. </p><p>반대로 n을 작게 선택하면 훈련 코퍼스에서 카운트는 잘 되겠지만 근사의 정확도는 현실의 확률분포와 멀어짐.</p><p>=&gt; <u>n은 최대 5</u>를 넘어서는 안된다고 권장됨.</p></li><li><p>NNLM(Neural Net Language Model)을 통해 <code>언어 모델 또한 단어의 유사도를 학습</code> 할 수 있도록 설계.</p><ul><li>훈련 코퍼스에 없는 단어 시퀀스도 예측을 통해 유사한 단어 시퀀스를 참고하여 확률을 계산해낸다.</li><li>워드 임베딩의 아이디어이기도 한다.</li><li>단어를 continuous한 밀집벡터의 형태로 표현하여 희소문제를 해결(?)</li></ul></li></ul></li><li><p><a href="https://wikidocs.net/22147" rel="external nofollow noopener noreferrer" target="_blank"><strong><code>long-term dependency</code></strong></a> : 정해진 개수의 전 토큰만을 보기 때문에 볼 수 있는 시퀀스의 범위가 한정됨.</p><ul><li>이는 모델의 정확도와 연관</li></ul><blockquote><p><u>An adorable little</u> boy is spreading (   ?   )</p></blockquote><p>위와 같은 문장이 있을 때 정답은 insults, smile 둘 중 하나라고 해보자. 문맥상 “<u>작고 사랑스러운</u> 아이가 (미소)를 퍼뜨렸다” 가 적절함을 알 수 있다.</p><p>하지만 만약 <u>작고 사랑스러운</u> 이라는 밑줄 친 부분을 예측에 고려하지 않는다면 엉뚱한 답 insults를 고를 수 있다.</p></li></ul><ol start="3"><li><p>신경망 기반 언어모델</p><ul><li>카운트 기반 접근은 그 방식상 본질적인 한계를 갖는다. </li><li>이를 극복하기 위해 다양한 방법 시도되었지만(백오프, 스무딩) 본질적인 n-gram 언어모델(고정된 개수의 단어만을 입력으로 받아야한다)에 대한 취약점은 해결하지 못함.</li></ul></li></ol><p><strong>RNNLM</strong>(Recurrent Neural Network Language Model)</p><p><img src="https://wikidocs.net/images/page/46496/rnnlm1_final_final.PNG" alt></p><p>예문 <em>what will the fat cat sit on</em> 의 RNNLM 학습/사용 과정을 보자.</p><p>1) 훈련이 끝난 모델을 사용할 경우 </p><ul><li>RNNLM은 예측 과정에서 <strong>이전 시점의 출력을 현재 시점의 입력</strong>으로 한다. </li><li>RNNLM은 what을 입력받으면, will을 예`하고 이 will은 다음 시점의 입력이 되어 the를 예측한다. 이것이 반복되어 <u>네번째 시점의 cat</u>은 앞서 나온 <strong>what, will, the, fat이라는 시퀀스로 인해 결정된 단어</strong>가 된다.</li></ul><p>2) 모델을 훈련시키는 경우</p><ul><li>위의 샘플에 대해 <code>what will the fat cat sit 시퀀스를 모델의 입력</code>으로 넣으면, <strong>will the fat cat sit on를 예측하도록 훈련</strong>한다.</li><li>이때 <em>will, the, fat, cat, sit, on</em>은 <u>각 시점의 레이블(<strong><code>yt</code></strong>)</u>가 된다.</li><li>이러한 RNN 훈련 기법을 <strong>교사 강요(teacher forcing)</strong> 라고 한다.</li></ul><blockquote><p>교사 강요(teacher forcing) : 테스트(실제 모델 사용) 과정에서 t 시점의 출력이 t+1 시점의 입력으로 사용되는 RNN 모델을 훈련시킬 때 사용하는 <strong>훈련 기법</strong>입니다. </p><p>훈련할 때 교사 강요를 사용할 경우, 모델이 <u>t 시점에서 예측한 값</u>을 <strong>t+1 시점에 입력으로 사용하지 않고</strong>, t 시점의 레이블. 즉, <strong><code>실제 알고있는 정답</code></strong>을 t+1 시점의 입력으로 사용합니다.</p></blockquote><p><strong>RNNLM의 학습 구조</strong></p><p><img src="https://wikidocs.net/images/page/46496/rnnlm2_final_final.PNG" alt></p><p><img src="https://wikidocs.net/images/page/46496/rnnlm4_final.PNG" alt></p><ul><li>임베딩층 : et=lookup(xt)</li><li>은닉층 : ht=tanh(Wxet+Whht−1+b)</li><li>출력층 : yt^=softmax(Wyht+b)</li><li>사용되는 손실함수 : cross-entropy</li><li>학습 과정에서 학습되는 가중치 행렬 : E(임베딩 행렬), Wx, Wh, Wy</li></ul><hr><h3 id="2-3-어떤-단어가-같이-쓰였는가"><a href="#2-3-어떤-단어가-같이-쓰였는가" class="headerlink" title="2-3. 어떤 단어가 같이 쓰였는가"></a>2-3. 어떤 단어가 같이 쓰였는가</h3><h4 id="분포가정"><a href="#분포가정" class="headerlink" title="분포가정"></a>분포가정</h4><p>: 자연어 처리에서 <strong>분포</strong>란 특정 범위(=<strong>윈도우</strong>)내에 <code>동시에 등장하는 단어 또는 문맥의 집합</code> 을 가리킨다.</p><blockquote><p>어떤 단어 쌍이 비슷한 문맥 환경에서 자주 등장한다면 그 의미 또한 유사할 것이라는게 분포가정의 전제이다.</p></blockquote><p>즉, 자연어를 사용하는 화자들이 특정 단어를 <code>실제 어떻게 사용하는지 관찰</code> 함으로서 <strong>단어의 의미를 밝힐</strong> 수 있다는 의미.</p>]]></content:encoded>
      
      <comments>https://katie0809.github.io/2020/02/16/ai-study2/#disqus_thread</comments>
    </item>
    
    <item>
      <title>[딥러닝 스터디] 임베딩이란</title>
      <link>https://katie0809.github.io/2020/02/14/ai-study1/</link>
      <guid>https://katie0809.github.io/2020/02/14/ai-study1/</guid>
      <pubDate>Fri, 14 Feb 2020 11:25:26 GMT</pubDate>
      <description>
      
        &lt;p&gt;다음의 책을 공부하며 정리한 내용입니다&lt;/p&gt;
&lt;ul&gt;
&lt;li&gt;한국어 임베딩 - 이기창&lt;/li&gt;
&lt;/ul&gt;
&lt;h2 id=&quot;1장-임베딩이란&quot;&gt;&lt;a href=&quot;#1장-임베딩이란&quot; class=&quot;headerlink&quot; title=&quot;1장. 임베딩이란&quot;&gt;&lt;/a&gt;1장. 임베딩이란&lt;/h2&gt;&lt;p&gt;: 임베딩이란 &lt;code&gt;자연어를 벡터로 바꾼 결과&lt;/code&gt; 혹은 &lt;code&gt;그 일련의 과정&lt;/code&gt; 전체를 의미하는 용어이다.&lt;/p&gt;
      
      </description>
      
      
      <content:encoded><![CDATA[<p>다음의 책을 공부하며 정리한 내용입니다</p><ul><li>한국어 임베딩 - 이기창</li></ul><h2 id="1장-임베딩이란"><a href="#1장-임베딩이란" class="headerlink" title="1장. 임베딩이란"></a>1장. 임베딩이란</h2><p>: 임베딩이란 <code>자연어를 벡터로 바꾼 결과</code> 혹은 <code>그 일련의 과정</code> 전체를 의미하는 용어이다.</p><a id="more"></a><p>임베딩에는..</p><ol><li>말뭉치(corpus)의 의미, 문법 정보가 응축되어있다.</li><li>벡터이기 때문에 사칙연산이 가능하다</li><li>단어/문서 관련도를 계산할 수 있다.</li></ol><p>대규모 말뭉치(corpus)를 미리 학습한 임베딩을 다른 문제를 푸는 데에 재사용 할 수 있다(<strong><em>전이학습</em></strong>)</p><p>임베딩 품질이 좋으면 단순한 모델로도 원하는 성능을 낼 수 있다. 따라서 자연어 처리 모델의 구성과 서비스에 있어 가장 중요한 구성요소 중 하나는 임베딩이라고 꼽을 수 있다.</p><p><a href="https://ratsgo.github.io" rel="external nofollow noopener noreferrer" target="_blank">임베딩 소스코드 내려받기</a> : 다양한 논문 저자들이 공개한 실제 임베딩 코드를 통해 자신만의 임베딩을 구축할 수 있다.</p><hr><h3 id="1-1-임베딩이란"><a href="#1-1-임베딩이란" class="headerlink" title="1-1. 임베딩이란"></a>1-1. 임베딩이란</h3><blockquote><p>임베딩이란 사람이 쓰는 <code>자연어</code> 를 기계가 이해할 수 있는 숫자의 나열인 <code>벡터</code> 로 <code>바꾼 결과/일련의 과정</code> 을 의미한다.</p></blockquote><p>이는 단어나 문장 각각을 벡터로 변환해 벡터공간으로 <em>끼워넣는다</em> 는 의미에서 임베딩이란 이름이 붙게 되었다.</p><p>가장 간단한 임베딩은 단어의 빈도를 벡터로 사용하는 것이다. 근대 소설 작품 몇 편에 나오는 단어 <code>기차</code> , <code>막걸리</code> , <code>선술집</code> 의 예시를 통해 알아보자.</p><table><thead><tr><th>구분</th><th>메밀꽃 필 무렵</th><th>운수좋은 날</th><th>사랑손님과 어머니</th><th>삼포가는 길</th></tr></thead><tbody><tr><td>기차</td><td>0</td><td>2</td><td>10</td><td>7</td></tr><tr><td>막걸리</td><td>0</td><td>1</td><td>0</td><td>0</td></tr><tr><td>선술집</td><td>0</td><td>1</td><td>1</td><td>0</td></tr></tbody></table><p><img src="/image/3D_Vector_Plotter___Academo_org_-_Free__interactive__education_-1833943.png" alt></p><p>기차의 임베딩은 [0,2,10,7], 막걸리의 임베딩은 [0,1,0,0], 선술집의 임베딩은 [0,1,1,0]이다. 이를 바탕으로 우리는 기차(blue)-막걸리(red)간 의미차이가 선술집(orange)-막걸리(red)간 의미 차이보다 크다는 것을 알 수 있다.</p><hr><h3 id="1-2-임베딩의-역할"><a href="#1-2-임베딩의-역할" class="headerlink" title="1-2. 임베딩의 역할"></a>1-2. 임베딩의 역할</h3><p>임베딩의 역할은 위에서 언급한 것과 같이 크게 3가지로 분류할 수 있다.</p><ul><li><p>단어/문장 간 관련도 계산</p><p>: 임베딩된 단어(벡터)는 단어 간 유사도를 계산할 수 있다.(코사인 유사도)</p></li><li><p>의미적/문법적 정보 함축</p><p>: <code>임베딩은 벡터</code> 인 만큼 <strong>사칙연산이 가능하다</strong>. 따라서 임베딩된 단어는 사칙 연산을 통해 단어간의 의미적/문법적 관계를 도출해낼 수 있다. 예를 들어 품질이 좋은 임베딩은 다음의 관계를 도출해낼 수 있다.</p><blockquote><p>아들 - 딸 + 소녀 = 소년</p></blockquote></li></ul><ul><li><p>전이 학습</p><p>: 임베딩은 자주 다른  <code>딥러닝 모델의 입력값</code>으로 쓰인다. 이를 <strong>전이학습</strong>이라고 한다.</p></li></ul><hr><h3 id="1-3-임베딩-기법의-역사와-종류"><a href="#1-3-임베딩-기법의-역사와-종류" class="headerlink" title="1-3. 임베딩 기법의 역사와 종류"></a>1-3. 임베딩 기법의 역사와 종류</h3><p>임베딩 기법의 발전흐름과 종류는 다음과 같이 정리할 수 있다.</p><p>1) 통계기반에서 뉴럴 네트워크 기반으로</p><ul><li>초기 임베딩 기법은 <u>말뭉치의 통계량을 직접적으로 활용</u></li><li>최근에는 <u>신경망 기반의 임베딩</u> 기법이 사용된다 : 다음/이전/중간 단어의 예측을 해내는 과정에서 학습</li></ul><p>2) 단어 수준에서 문장 수준으로</p><ul><li>2017년 이전의 임베딩 기법은 대게 단어수준 모델이었다 : NPLM, Word2Vec, GloVe, FastText, Swivel 등</li><li>이는 동음이의어를 분간하기 어렵다는 문제가 있다 : <code>사람의 눈</code> 과 <code>하늘에서 내리는 눈</code> 은 엄연히 다르지만 임베딩 벡터는 하나.</li><li>2018년 이후 <u>문장수준 임베딩</u> 기법들이 주목받았다 : BERT, GPT, ELMo</li><li>문장수준 임베딩 기법은 개별 단어가 아닌 단어 시퀀스 전체의 문맥적 의미를 함축한다. 따라서 단어임베딩보다 학습 효과가 좋다.</li></ul><p><img src="https://camo.githubusercontent.com/f4e93eb8c9bee39be6fb6081925b7832f1c87970/68747470733a2f2f63646e2d696d616765732d312e6d656469756d2e636f6d2f6d61782f3830302f312a2d78546f32592d315679766a59797768646479544f672e706e67" alt="&#39;bank&#39;의 임베딩 시각화"></p><blockquote><p>다의어 ‘bank’를 문맥에 따라 시각화한 모습. 의미가 다른 단어를 분리해 이해할 수 있다.</p></blockquote><p>3) Pre-train/Fine-tuning 모델로</p><ul><li><p>90년대 자연어 처리 모델 : 사람이 직접 모델의 입력값을 선정.</p></li><li><p>2000년대 이후 : 데이터를 통째로 모델에 넣고 <code>입출력 사이의 관계</code> 를 <code>사람의 개입없이</code> 모델 스스로 이해해내도록 유도한다.</p></li><li><p>이러한 기법을 <em>엔드 투 엔드 모델</em> 이라고 한다. 대표적으로 <em>시퀀스 투 시퀀스</em> 모델이 있다.</p></li><li><p>2018년 이후 : 엔드투 엔드 방식에서 벗어나 <em>pretrain/fine tuning</em> 방식으로 발전해나가고 있다.</p><ul><li><p>대규모 말뭉치로 임베딩을 만든다(프리트레인)</p><p>: 이 말뭉치에는 단어의 의미/문법적 맥락이 포함되어있다.</p></li><li><p>임베딩을 입력으로 하는 새로운 딥러닝 모델을 만들고, 풀고자 하는 문제에 맞춰 <code>임베딩을 포함한 모델 전체를 업데이트</code> 한다.(파인 튜닝, 전이 학습)</p><p>: ELMo, GPT, BERT 등이 해당</p></li></ul></li></ul><blockquote><p>[용어 이해하기]</p><ul><li><strong>다운스트림 태스크</strong> : 풀고자 하는 구체적 자연어처리 문제들. 품사판별, 개채명 인식, 의미역 분석 등이 있다.</li><li><strong>업스트림 태스크</strong> : 다운스트림 태스크에 앞서 해결해야할 과제. 단어/문장 임베딩을 프리트레인하는 과정이 이에 해당.</li></ul></blockquote><p>4) 임베딩의 종류와 성능</p><p>: 임베딩 기법은 크게 3가지로 나뉜다.</p><ul><li>행렬분해 기반 방법<ul><li>말뭉치 정보가 들어있는 기존의 행렬을 두 개 이상의 작은 행렬로 쪼개는 임베딩 기법</li><li>GloVe, Sweivel등이 이에 해당</li></ul></li><li>예측 기반 방법<ul><li>어떤 단어 주변에 특정 단어가 나타날지 예측하거나, 이전/다음/중간의 단어가 무엇일지 맞추는 과정에서 학습하는 임베딩 기법</li><li>신경망 기반 임베딩 기법이 이러한 예측 기반 방법에 속한다 : Word2Vec, FastText, BERT, ELMo, GPT 등이 이에 해당</li></ul></li><li>토픽 기반 방법<ul><li>주어진 문서에 <u>잠재된 주제를 추론하는 방식</u> 의 임베딩 기법</li><li>잠재 디리클레 할당이 대표적 기법이다.</li></ul></li></ul>]]></content:encoded>
      
      <comments>https://katie0809.github.io/2020/02/14/ai-study1/#disqus_thread</comments>
    </item>
    
    <item>
      <title>GithubPage 세팅하기</title>
      <link>https://katie0809.github.io/2020/02/14/how-to-start-githubPage/</link>
      <guid>https://katie0809.github.io/2020/02/14/how-to-start-githubPage/</guid>
      <pubDate>Fri, 14 Feb 2020 02:20:38 GMT</pubDate>
      <description>
      
        &lt;p&gt;온갖 블로그 사이트를 전전하다가 드디어 뭔가 쌈박한 친구를 발견했다…&lt;br&gt;드디어 나는 정착할 블로그를 찾은 것인가..?!&lt;/p&gt;
      
      </description>
      
      
      <content:encoded><![CDATA[<p>온갖 블로그 사이트를 전전하다가 드디어 뭔가 쌈박한 친구를 발견했다…<br>드디어 나는 정착할 블로그를 찾은 것인가..?!</p><a id="more"></a><h1 id="참고한-사이트"><a href="#참고한-사이트" class="headerlink" title="참고한 사이트"></a>참고한 사이트</h1><ul><li><a href="https://pages.github.com/" rel="external nofollow noopener noreferrer" target="_blank">https://pages.github.com/</a> : GithubPage Official Guide</li><li><a href="https://www.holaxprogramming.com/2017/04/16/github-page-and-hexo/" rel="external nofollow noopener noreferrer" target="_blank">https://www.holaxprogramming.com/2017/04/16/github-page-and-hexo/</a> : GithubPage와 Hexo 설치하기</li><li><a href="https://blog.zhangruipeng.me/hexo-theme-icarus/" rel="external nofollow noopener noreferrer" target="_blank">https://blog.zhangruipeng.me/hexo-theme-icarus/</a> : Hexo Official Guide Blog</li><li><a href="https://github.com/ppoffice/hexo-theme-icarus" rel="external nofollow noopener noreferrer" target="_blank">https://github.com/ppoffice/hexo-theme-icarus</a> : Hexo Official Github</li><li><a href="https://alleyful.github.io/categories/Tools/Hexo/" rel="external nofollow noopener noreferrer" target="_blank">https://alleyful.github.io/categories/Tools/Hexo/</a> : Hexo 설치하기</li><li><a href="https://guides.github.com/features/mastering-markdown/" rel="external nofollow noopener noreferrer" target="_blank">https://guides.github.com/features/mastering-markdown/</a> : Markdown 마스터하기</li><li><a href="https://swtpumpkin.github.io/git/hexo/hexoImg/" rel="external nofollow noopener noreferrer" target="_blank">https://swtpumpkin.github.io/git/hexo/hexoImg/</a> : Hexo 이미지 올리기</li><li><a href="https://mishka.kr/2019/06/10/hexo-writing/" rel="external nofollow noopener noreferrer" target="_blank">https://mishka.kr/2019/06/10/hexo-writing/</a> : Hexo 글쓰기</li></ul><h1 id="Default-Guide"><a href="#Default-Guide" class="headerlink" title="Default Guide"></a>Default Guide</h1><h2 id="Hexo-명령어"><a href="#Hexo-명령어" class="headerlink" title="Hexo 명령어"></a>Hexo 명령어</h2><ol><li>새 테마 적용<br>: 새 테마 적용시에 일단 한번 클린 후 deploy</li></ol><figure class="highlight bash"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br></pre></td><td class="code"><pre><span class="line">hexo clean</span><br><span class="line">hexo deploy --generate</span><br></pre></td></tr></table></figure><ol start="2"><li>새 글쓰기<br>: <a href="https://mishka.kr/2019/06/10/hexo-writing/" rel="external nofollow noopener noreferrer" target="_blank">https://mishka.kr/2019/06/10/hexo-writing/</a> 링크를 꼭 참고한다.</li></ol><figure class="highlight bash"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br></pre></td><td class="code"><pre><span class="line"><span class="comment"># layout은 post(default), draft, page가 있다.</span></span><br><span class="line">hexo new [layout] &lt;post_name&gt;</span><br><span class="line">hexo deploy --generate</span><br><span class="line"></span><br><span class="line"><span class="comment"># draft로 작성시 publish 명령어 사용</span></span><br><span class="line">hexo publish [layout] &lt;post_name&gt;</span><br></pre></td></tr></table></figure><blockquote><p><em>submodule update</em>를 하는경우 theme폴더 내의 <code>_config.yml</code>파일이 지워지는 문제가 있다. 해당 파일은 항상 백업해두어야 한다ㅠ</p></blockquote><ol start="3"><li>로컬 서버 확인</li></ol><figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line">hexo s</span><br></pre></td></tr></table></figure><h2 id="Serch-Engine-Optimization"><a href="#Serch-Engine-Optimization" class="headerlink" title="Serch Engine Optimization"></a>Serch Engine Optimization</h2><p>깃헙 페이지는 기본적으로 검색이 안되므로.. 검색엔진 최적화 작업을 따로 해주어야 구글/네이버 등 검색엔진에서 보일 수 있다.</p><p>Hexo 검색엔진 최적화를 위해 참고한 사이트<br><a href="https://alleyful.github.io/2019/08/10/tools/hexo/hexo-guide-03/" rel="external nofollow noopener noreferrer" target="_blank">https://alleyful.github.io/2019/08/10/tools/hexo/hexo-guide-03/</a><br><a href="https://jeyolog.github.io/2018/08/02/hexo-검색엔진-최적화-플로그인/" rel="external nofollow noopener noreferrer" target="_blank">https://jeyolog.github.io/2018/08/02/hexo-검색엔진-최적화-플로그인/</a></p><h2 id="블로그-꾸미기"><a href="#블로그-꾸미기" class="headerlink" title="블로그 꾸미기"></a>블로그 꾸미기</h2><ul><li>로고 만들기<ul><li>테마폴더의 이미지 파일들을 대체해준다.</li><li>icarus테마 기준 \themes\icarus\source\images 내의 favicon, logo 등의 이미지를 변경해준다.</li><li><a href="https://logohub.io/" rel="external nofollow noopener noreferrer" target="_blank">https://logohub.io/</a> : 로고 제작 사이트</li><li><a href="https://www.aconvert.com/image/png-to-svg/" rel="external nofollow noopener noreferrer" target="_blank">https://www.aconvert.com/image/png-to-svg/</a> : png파일 svg로 변환</li></ul></li><li>변경한 이미지는 hexo clean후 배포해주어야 적용된다(테마 적용하듯이 적용!)</li></ul><hr><h2 id="기타-팁"><a href="#기타-팁" class="headerlink" title="기타 팁"></a>기타 팁</h2><ul><li>생각보다 시간이 오래 걸린다. 세팅하고 익숙해지는데에 거의 반나절이 걸렸다.</li><li>hexo deploy로 새 글을 발행하는 경우 내 깃헙페이지를 관리하는 실제 레포지토리에는 완성된 글/글과 관련된 블로그 파일들만 올라간다. </li><li>여튼 무슨소리냐면 <strong>내가 로컬에서 글 쓰는 환경</strong> 그 자체는 업로드되지 않는다는 얘기다.</li><li>따라서 내가 글쓰는 환경은 따로 백업을 해줘야하는데 이게 또 글 쓰는 폴더 내의 themes 폴더는 다 별도의 깃헙 레포라서 그냥 통으로 올리면 제대로 백업이 안된다.</li><li><a href="https://mishka.kr/2019/06/13/backup/" rel="external nofollow noopener noreferrer" target="_blank">https://mishka.kr/2019/06/13/backup/</a> 이분이 굉장히 잘 설명해주심.<ol><li>내 원격 레포를 두개 만든다. 하나는 블로그 쓰는환경 전체파일 백업용(레포1), 다른 하나는 테마 폴더만 백업용(레포2)</li><li>테마 폴더의 원격 저장소 위치를 내 원격레포1로 변경해두고 커밋</li><li>테마 폴더를 지우고 나머지 블로그 글쓰는 환경 폴더를 깃 레포로 만든다.</li><li>테마 폴더를 submodule로 현재 로컬 레포에 추가한다.</li><li>블로그 전체 글쓰는 환경 폴더를 원격레포2로 전체 커밋</li><li>이후 테마가 추가되면 git submodule add로 추가해준다. 테마 관련 설정이 변경돼도 이렇게 해주면 될 듯.</li></ol></li></ul><p><strong>Warning</strong> 로컬 테마폴더의 깃 레포 url을 바꾸지 않으면 기존 icarus  레포에 푸시를 하게 된다!!ㅋㅋ.. 대담한 한국인이 되고싶다면 시도해볼것..</p><ul><li><p>마크다운 에디터가 있으면 편할 것 같아서 또 서치를 했다.</p><ul><li>Typora 로 현재 작성중. 가볍고 심플하니 나쁘지 않다.</li></ul></li></ul><hr><p>Welcome to <a href="https://hexo.io/" rel="external nofollow noopener noreferrer" target="_blank">Hexo</a>! This is your very first post. Check <a href="https://hexo.io/docs/" rel="external nofollow noopener noreferrer" target="_blank">documentation</a> for more info. If you get any problems when using Hexo, you can find the answer in <a href="https://hexo.io/docs/troubleshooting.html" rel="external nofollow noopener noreferrer" target="_blank">troubleshooting</a> or you can ask me on <a href="https://github.com/hexojs/hexo/issues" rel="external nofollow noopener noreferrer" target="_blank">GitHub</a>.</p><h2 id="Quick-Start"><a href="#Quick-Start" class="headerlink" title="Quick Start"></a>Quick Start</h2><h3 id="Create-a-new-post"><a href="#Create-a-new-post" class="headerlink" title="Create a new post"></a>Create a new post</h3><figure class="highlight bash"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line">$ hexo new <span class="string">"My New Post"</span></span><br></pre></td></tr></table></figure><p>More info: <a href="https://hexo.io/docs/writing.html" rel="external nofollow noopener noreferrer" target="_blank">Writing</a></p><h3 id="Run-server"><a href="#Run-server" class="headerlink" title="Run server"></a>Run server</h3><figure class="highlight bash"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line">$ hexo server</span><br></pre></td></tr></table></figure><p>More info: <a href="https://hexo.io/docs/server.html" rel="external nofollow noopener noreferrer" target="_blank">Server</a></p><h3 id="Generate-static-files"><a href="#Generate-static-files" class="headerlink" title="Generate static files"></a>Generate static files</h3><figure class="highlight bash"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line">$ hexo generate</span><br></pre></td></tr></table></figure><p>More info: <a href="https://hexo.io/docs/generating.html" rel="external nofollow noopener noreferrer" target="_blank">Generating</a></p><h3 id="Deploy-to-remote-sites"><a href="#Deploy-to-remote-sites" class="headerlink" title="Deploy to remote sites"></a>Deploy to remote sites</h3><figure class="highlight bash"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line">$ hexo deploy</span><br></pre></td></tr></table></figure><p>More info: <a href="https://hexo.io/docs/one-command-deployment.html" rel="external nofollow noopener noreferrer" target="_blank">Deployment</a></p>]]></content:encoded>
      
      <comments>https://katie0809.github.io/2020/02/14/how-to-start-githubPage/#disqus_thread</comments>
    </item>
    
  </channel>
</rss>
