{"pages":[{"title":"Downloads","text":"인공지능자료 자연어처리 발표 참고자료 기타자료","link":"/downloads/index.html"}],"posts":[{"title":"[딥러닝 기초] Pytorch 기본연산","text":"다음의 책을 공부하며 정리한 내용입니다 https://wikidocs.net/book/2788 Pytorch 기본 연산: pytorch를 활용한 신경망 구성을 위해 필수적인 딥러닝 기본 연산단위를 알아보자. Tensor 딥러닝의 가장 기본적인 연산단위 : 벡터, 행렬, 텐서 0차원 : 스칼라 1차원 : 벡터 2차원 : 행렬 3차원 이상 : 텐서 2D Tensor : 행렬 2차원 텐서는 말그대로 ‘행렬‘이다. 따라서 2차원 텐서 t는 다음과 같이 나타낼 수 있다. |t| = (배치 사이즈, 차원) 행렬의 행의 개수 = 배치사이즈 행렬의 열의 개수 = 차원(dimension) 123456789101112131415t = tor.FloatTensor([0,1,2,3])print(t)print(t.dim(), t.size()) # rank(차원), 원소개수# 인덱스로 접근 가능하다print(t[1])# 정수 텐서lt = tor.LongTensor([[1, 2, 3], [4, 5, 6], [7, 8, 9], [10, 11, 12] ])print(lt)print(lt[2], lt[2].size()) 12345678tensor([0., 1., 2., 3.])1 torch.Size([4])tensor(1.)tensor([[ 1, 2, 3], [ 4, 5, 6], [ 7, 8, 9], [10, 11, 12]])tensor([7, 8, 9]) torch.Size([3]) 3D Tensor 이미지/영상처리 분야에서는 보다 복잡한 텐서를 다룬다. 이미지는 가로/세로가 존재하며, 따라서 여러장의 이미지는 자연스레 (가로, 세로, 배치 크기) 가 됨을 연상할 수 있다. 3D Tensor in NLPNatural Language Processing(자연어처리)에서는 보통 (문장길이, 차원, 배치 크기) 라는 3차원 텐서를 사용한다.","link":"/2020/02/17/ai-start1/"},{"title":"[딥러닝 기초] Linear Regression","text":"다음의 책을 공부하며 정리한 내용입니다 https://wikidocs.net/book/2788 선형회귀란: 선형 회귀란 학습 데이터와 가장 잘 맞는 하나의 직선을 찾는 일. 하나의 직선은 W와 b로 정의할 수 있다. 선형 회귀의 목표: 가장 잘 맞는 직선을 정의하는 W와 b의 값을 찾는 것. 파이토치에서의 선형회귀 선형 회귀 모델: nn.Linear() 평균 제곱오차: nn.functional.mse_loss() torch.manual_seed() : 현재의 코드를 재실행해도 다음에도 같은 결과가 나오도록 랜덤 시드(random seed)를 준다. 선형회귀 훈련을 위한 기본적인 코드의 뼈대는 아래와 같다 123456789101112131415161718192021222324252627# 1.훈련데이터의 선언# x_train의 벡터가 y_train이 되도록 하는 w와 b를 찾는다import torchx_train = torch.FloatTensor([[1], [2], [3]])y_train = torch.FloatTensor([[2], [4], [6]])# 2.가중치 W와 편향 b를 0으로 초기화# requires_grad: 학습을 통해 값이 변경되는 변수임을 명시.W = torch.zeros(1, requires_grad=True)b = torch.zeros(1, requires_grad=True)# 3.직선의 방정식(=선형회귀)에 해당되는 가설을 선언한다.# 가설 = 시스템이 학습한 w,b로 예측한 y_hat값hypothesis = x_train * W + b# print(hypothesis)# 4.사용할 비용함수 선언(MSE)cost = torch.mean((hypothesis - y_train) ** 2) # 5.W와 b를 SGD(경사하강법: Stochastic Gradient Descent)로 훈련시킨다# lr: learning rateoptimizer = torch.optim.SGD([W, b], lr=0.01) 이를 바탕으로 실제 동작하는 선형회귀 모델을 제작해 학습을 진행해보면 아래와 같다. 12345678910111213141516171819202122232425262728293031323334353637383940# 최종코드import torchtorch.manual_seed(1)# 훈련데이터 선언x_train = torch.FloatTensor([[1], [2], [3]])y_train = torch.FloatTensor([[2], [4], [6]])# 학습데이터 w,b 선언. 둘다 값이 1인 임의의 스칼라 텐서w = torch.zeros(1, requires_grad=True)b = torch.zeros(1, requires_grad=True)# SGD방식을 사용한 최적화 선언optimizer = torch.optim.SGD([w, b], lr=0.01)# 학습 횟수는 1000+1회tot_epoch = 1000for cur_epoch in range(tot_epoch + 1): # 새 학습값 y_hat = (w * x_train) + b # MSE함수통한 비용 계산 cost = torch.mean((y_train - y_hat) ** 2) # optimizer로 w,b 학습시킴으로서 y_hat 개선 # gradient를 0으로 초기화 optimizer.zero_grad() # 비용 함수를 미분하여 gradient 계산 cost.backward() # W와 b를 업데이트 optimizer.step() # 100번마다 로그 출력 if cur_epoch % 100 == 0: print('Epoch {:4d}/{} w: {:.3f}, b: {:.3f} Cost: {:.6f}'.format( cur_epoch, tot_epoch, w.item(), b.item(), cost.item() )) 123456789101112[결과]Epoch 0/1000 w: 0.187, b: 0.080 Cost: 18.666666Epoch 100/1000 w: 1.746, b: 0.578 Cost: 0.048171Epoch 200/1000 w: 1.800, b: 0.454 Cost: 0.029767Epoch 300/1000 w: 1.843, b: 0.357 Cost: 0.018394Epoch 400/1000 w: 1.876, b: 0.281 Cost: 0.011366Epoch 500/1000 w: 1.903, b: 0.221 Cost: 0.007024Epoch 600/1000 w: 1.924, b: 0.174 Cost: 0.004340Epoch 700/1000 w: 1.940, b: 0.136 Cost: 0.002682Epoch 800/1000 w: 1.953, b: 0.107 Cost: 0.001657Epoch 900/1000 w: 1.963, b: 0.084 Cost: 0.001024Epoch 1000/1000 w: 1.971, b: 0.066 Cost: 0.000633 위의 코드와 결과는 다음을 의미한다. x가 [1, 2, 3]일때 y가 [2,4,6]이 되는 w와 b는 2, 0이다. 학습을 통해 최종적으로 찾은 결과 w,b는 1.971, 0.066이므로 어느정도 답을 찾아냈다고 볼 수 있다. 다중선형회귀: 기존의 선형회귀가 y = wx + b의 w,b를 찾는 모델이었다면, 다중선형회귀는 y = w1x1 + w2x2 + w3x3 + b의 w1, w2, w3, b를 찾는 모델이다. 선형회귀 훈련을 위한 기본적인 코드의 뼈대는 아래와 같다. 123456789101112131415161718# N개의 x_train 벡터와 가중치 w는 행렬로 표현할 수 있다.# 5 * 3 학습벡터 =&gt; 길이 5의 x_train벡터 3개x_train = torch.FloatTensor([[73, 80, 75], [93, 88, 93], [89, 91, 90], [96, 98, 100], [73, 66, 70]]) # y_train벡터의 길이 = x_train벡터의 길이(5)y_train = torch.FloatTensor([[152], [185], [180], [196], [142]])# 가중치 w의 개수 = x_train벡터의 개수(3) ** 길이는 1 **w = torch.zeros((3, 1) ,requires_grad=True)b = torch.zeros(1, requires_grad=True)# x_train과 y_train벡터가 각각 행렬이므로# 가설은 파이토치 행렬곱을 활용해 정의해준다. hypothesis = x_train.matmul(w) + b 이를 바탕으로 실제 동작하는 다중선형회귀 모델을 제작해 학습을 진행해보면 아래와 같다. 123456789101112131415161718192021222324252627282930313233# 학습을 위한 코드는 기존과 동일하다.import torchtorch.manual_seed(1)x_train = torch.FloatTensor([[73, 80, 75], [93, 88, 93], [89, 91, 90], [96, 98, 100], [73, 66, 70]]) y_train = torch.FloatTensor([[152], [185], [180], [196], [142]])# 모델 초기화w = torch.zeros((3, 1) ,requires_grad=True)b = torch.zeros(1, requires_grad=True)# SGD방식을 사용한 최적화 선언optimizer = torch.optim.SGD([w, b], lr=1e-5)tot_epoch = 1000for cur_epoch in range(tot_epoch + 1): y_hat = x_train.matmul(w) + b cost = torch.mean((y_train - y_hat) ** 2) optimizer.zero_grad() cost.backward() optimizer.step() # 100번마다 로그 출력 if cur_epoch % 100 == 0: print('Epoch {:4d}/{} Cost: {:.6f}'.format( cur_epoch, tot_epoch, cost.item() )) 123456789101112[결과]Epoch 0/1000 Cost: 29661.800781Epoch 100/1000 Cost: 1.563628Epoch 200/1000 Cost: 1.497595Epoch 300/1000 Cost: 1.435044Epoch 400/1000 Cost: 1.375726Epoch 500/1000 Cost: 1.319507Epoch 600/1000 Cost: 1.266222Epoch 700/1000 Cost: 1.215703Epoch 800/1000 Cost: 1.167810Epoch 900/1000 Cost: 1.122429Epoch 1000/1000 Cost: 1.079390","link":"/2020/02/17/ai-start2/"},{"title":"[딥러닝 기초] Logistic Regression","text":"다음의 책을 공부하며 정리한 내용입니다 https://wikidocs.net/book/2788 로지스틱 회귀란: 로지스틱 회귀는 이진분류(Binary Classification) 문제의 해결에 사용되는 대표적인 알고리즘. 이름은 ‘회귀‘이지만 ‘분류‘에 쓰인다 이진분류의 모델 점수(x) 결과(y) 45 불합격 50 불합격 55 불합격 60 합격 65 합격 70 합격 위와 같은 데이터가 있다고 하자. 조건은 아래와 같다. 합격 커트라인은 알려져있지 않다 임의의 점수 x의 합격여부를 예측하고 싶다. 이 경우 주어진 데이터에 대해 합격(1), 불합격(0)으로 그래프를 그리면 아래와 같이 표현할 수 있다. 위의 간단한 예시를 통해 이진분류의 문제를 풀기위한 x, y의 관계는 S 형태의 그래프로 나타내야 함을 알 수 있다. 따라서 다음과 같은 결론을 얻을 수 있다. 로지스틱 회귀의 가설은 선형 회귀 때의 H(x)=Wx+b가 아니다. 위처럼 S자 모양의 그래프를 만들 수 있는 어떤 특정 함수 f를 추가적으로 사용하여 H(x)=f(Wx+b)의 가설을 사용한다. 어떤 함수 f는 이미 널리 알려져있다. =&gt; 시그모이드 함수 즉, 로지스틱 회귀의 가설이자 이진분류 문제를 풀기위한 함수 f는 Sigmoid function이다 Sigmoid 수식 H(x)=sigmoid(Wx+b)=1+e−(Wx+b)=σ(Wx+b) 가중치(w)의 변화에 따른 Sigmoid 함수 red: w값이 0.5 ~ blue: w값이 2 편향(b)의 변화에 따른 Sigmoid 함수 red: b값이 0.5 ~ blue: b값이 2 Sigmoid 함수의 특성 시그모이드 함수는 입력값이 한없이 커지면 1에 수렴하고, 입력값이 한없이 작아지면 0에 수렴한다. 시그모이드 함수의 출력은 0~1 위의 특성을 이용하여 분류 작업에 사용. 임계값 x(0 =&lt; x =&lt; 1)를 넘으면 1, 넘지 못하면 0으로 분류 결론 로지스틱 회귀의 가설/모델은 H(x)=sigmoid(Wx+b) 이다. 로지스틱 회귀 실습12345678910111213141516171819202122232425262728293031323334# 최종코드import torchimport torch.nn.functional as Ftorch.manual_seed(1)x_train = torch.FloatTensor([[1, 2], [2, 3], [3, 1], [4, 3], [5, 3], [6, 2]])y_train = torch.FloatTensor([[0], [0], [0], [1], [1], [1]])w = torch.zeros([2, 1], requires_grad=True)b = torch.zeros([1], requires_grad=True)# hypothesis = 1 / (1 + torch.exp(-(x_train.matmul(w) + b)))hypothesis = torch.sigmoid(x_train.matmul(w) + b)optimizer = torch.optim.SGD([w, b], lr=1)tot_epoch = 1000for cur_epoch in range(tot_epoch + 1): y_hat = torch.sigmoid(x_train.matmul(w) + b) cost = F.binary_cross_entropy(y_hat, y_train) optimizer.zero_grad() cost.backward() optimizer.step() # 100번마다 로그 출력 if cur_epoch % 100 == 0: print('Epoch {:4d}/{} Cost: {:.6f}'.format(cur_epoch, tot_epoch, cost.item())) # 제대로 학습됐는지 확인print(w)print(b) 123456789101112131415[결과]Epoch 0/1000 Cost: 0.693147Epoch 100/1000 Cost: 0.134722Epoch 200/1000 Cost: 0.080643Epoch 300/1000 Cost: 0.057900Epoch 400/1000 Cost: 0.045300Epoch 500/1000 Cost: 0.037261Epoch 600/1000 Cost: 0.031672Epoch 700/1000 Cost: 0.027556Epoch 800/1000 Cost: 0.024394Epoch 900/1000 Cost: 0.021888Epoch 1000/1000 Cost: 0.019852tensor([[3.2530], [1.5179]], requires_grad=True)tensor([-14.4819], requires_grad=True)","link":"/2020/02/17/ai-start4/"},{"title":"[딥러닝 기초] 선형회귀 모델의 개선","text":"다음의 책을 공부하며 정리한 내용입니다 https://wikidocs.net/book/2788 nn.Module 을 사용한 선형회귀 모델의 개선: 파이토치에서 일부 모델(ex. 선형회귀모델)들은 이미 nn.Module의 형태로 편리하게 쓸 수 있도록 구현되어있다. 즉, 우리가 기존에 구현했던 선형회귀 수식 y_hat = (w * x_train) + b y_hat = x_train.matmul(w) + b 이는 아래와 같이 변경할 수 있다. model = nn.Linear(1, 1) model = nn.Linear(3, 1) : 이때 nn.Linear()은 선형회귀 모델을 의미하며 왼쪽부터 순서대로 input dimension, output dimension이다. input_dim : 가중치 w의 개수 output_dim : 가중치 w의 길이 nn.Linear 모델 사용해보기 nn.Linear( )에는 가중치w와 편향b가 저장되어있다. 이는 model.parameters( )로 불러올 수 있다. 12345678# 모델을 선언 및 초기화. 단순 선형 회귀이므로 input_dim=1, output_dim=1.import torchimport torch.nn as nnmodel = nn.Linear(1,1)# 출력되는 첫번째값이 w, 두번째값이 b. 랜덤 초기화되어있는 상태이다.print(list(model.parameters())) 1234[결과][Parameter containing:tensor([[0.5153]], requires_grad=True), Parameter containing:tensor([-0.4414], requires_grad=True)] 기존 선형회귀 코드의 개선: 기존 선형회귀 코드를 다음과 같이 개선할 수 있다. 개선 1123# 최종코드import torchtorch.manual_seed(1) torch.nn 까지 import 1234# 최종코드import torchimport torch.nntorch.manual_seed(1) 개선 2123456789101112131415for epoch in range(1000): # 새 학습값 y_hat = (w * x_train) + b # MSE함수통한 비용 계산 cost = torch.mean((y_train - y_hat) ** 2) # optimizer로 w,b 학습시킴으로서 y_hat 개선 # gradient를 0으로 초기화 optimizer.zero_grad() # 비용 함수를 미분하여 gradient 계산 cost.backward() # W와 b를 업데이트 optimizer.step() nn.Linear( )로 선언한 model로 y_hat 계산 F.mse_loss(prediction, y_train) 파이토치에서 제공하는 평균제곱함수로 cost 계산 1234567891011121314151617181920212223242526model = nn.Linear(1, 1) for epoch in range(1000): # 새 학습값 y_hat = model(x_train) # MSE함수통한 비용 계산 cost = F.mse_loss(y_hat, y_train) # optimizer로 w,b 학습시킴으로서 y_hat 개선 # gradient를 0으로 초기화 optimizer.zero_grad() # 비용 함수를 미분하여 gradient 계산 cost.backward() # W와 b를 업데이트 optimizer.step() # 임의의 입력 4를 선언new_var = torch.FloatTensor([[4.0]]) # 입력한 값 4에 대해서 예측값 y를 리턴받아서 pred_y에 저장pred_y = model(new_var) # forward 연산# y = 2x 이므로 입력이 4라면 y가 8에 가까운 값이 나와야 제대로 학습이 된 것print(\"훈련 후 입력이 4일 때의 예측값 :\", pred_y) 12[결과]훈련 후 입력이 4일 때의 예측값 : tensor([[7.9989]], grad_fn=&lt;AddmmBackward&gt;) 위의 코드로 학습한 모델 model은 x_train, y_train에 대해 학습된 값 w,b 를 저장하고 있다. 학습된 모델 model을 활용해 새로운 값 x_new 에 대한 예측값 y_pred를 얻을 수 있다.","link":"/2020/02/17/ai-start3/"},{"title":"[딥러닝 스터디] 임베딩이란","text":"다음의 책을 공부하며 정리한 내용입니다 한국어 임베딩 - 이기창 1장. 임베딩이란: 임베딩이란 자연어를 벡터로 바꾼 결과 혹은 그 일련의 과정 전체를 의미하는 용어이다. 임베딩에는.. 말뭉치(corpus)의 의미, 문법 정보가 응축되어있다. 벡터이기 때문에 사칙연산이 가능하다 단어/문서 관련도를 계산할 수 있다. 대규모 말뭉치(corpus)를 미리 학습한 임베딩을 다른 문제를 푸는 데에 재사용 할 수 있다(전이학습) 임베딩 품질이 좋으면 단순한 모델로도 원하는 성능을 낼 수 있다. 따라서 자연어 처리 모델의 구성과 서비스에 있어 가장 중요한 구성요소 중 하나는 임베딩이라고 꼽을 수 있다. 임베딩 소스코드 내려받기 : 다양한 논문 저자들이 공개한 실제 임베딩 코드를 통해 자신만의 임베딩을 구축할 수 있다. 1-1. 임베딩이란 임베딩이란 사람이 쓰는 자연어 를 기계가 이해할 수 있는 숫자의 나열인 벡터 로 바꾼 결과/일련의 과정 을 의미한다. 이는 단어나 문장 각각을 벡터로 변환해 벡터공간으로 끼워넣는다 는 의미에서 임베딩이란 이름이 붙게 되었다. 가장 간단한 임베딩은 단어의 빈도를 벡터로 사용하는 것이다. 근대 소설 작품 몇 편에 나오는 단어 기차 , 막걸리 , 선술집 의 예시를 통해 알아보자. 구분 메밀꽃 필 무렵 운수좋은 날 사랑손님과 어머니 삼포가는 길 기차 0 2 10 7 막걸리 0 1 0 0 선술집 0 1 1 0 기차의 임베딩은 [0,2,10,7], 막걸리의 임베딩은 [0,1,0,0], 선술집의 임베딩은 [0,1,1,0]이다. 이를 바탕으로 우리는 기차(blue)-막걸리(red)간 의미차이가 선술집(orange)-막걸리(red)간 의미 차이보다 크다는 것을 알 수 있다. 1-2. 임베딩의 역할임베딩의 역할은 위에서 언급한 것과 같이 크게 3가지로 분류할 수 있다. 단어/문장 간 관련도 계산 : 임베딩된 단어(벡터)는 단어 간 유사도를 계산할 수 있다.(코사인 유사도) 의미적/문법적 정보 함축 : 임베딩은 벡터 인 만큼 사칙연산이 가능하다. 따라서 임베딩된 단어는 사칙 연산을 통해 단어간의 의미적/문법적 관계를 도출해낼 수 있다. 예를 들어 품질이 좋은 임베딩은 다음의 관계를 도출해낼 수 있다. 아들 - 딸 + 소녀 = 소년 전이 학습 : 임베딩은 자주 다른 딥러닝 모델의 입력값으로 쓰인다. 이를 전이학습이라고 한다. 1-3. 임베딩 기법의 역사와 종류임베딩 기법의 발전흐름과 종류는 다음과 같이 정리할 수 있다. 1) 통계기반에서 뉴럴 네트워크 기반으로 초기 임베딩 기법은 말뭉치의 통계량을 직접적으로 활용 최근에는 신경망 기반의 임베딩 기법이 사용된다 : 다음/이전/중간 단어의 예측을 해내는 과정에서 학습 2) 단어 수준에서 문장 수준으로 2017년 이전의 임베딩 기법은 대게 단어수준 모델이었다 : NPLM, Word2Vec, GloVe, FastText, Swivel 등 이는 동음이의어를 분간하기 어렵다는 문제가 있다 : 사람의 눈 과 하늘에서 내리는 눈 은 엄연히 다르지만 임베딩 벡터는 하나. 2018년 이후 문장수준 임베딩 기법들이 주목받았다 : BERT, GPT, ELMo 문장수준 임베딩 기법은 개별 단어가 아닌 단어 시퀀스 전체의 문맥적 의미를 함축한다. 따라서 단어임베딩보다 학습 효과가 좋다. 다의어 ‘bank’를 문맥에 따라 시각화한 모습. 의미가 다른 단어를 분리해 이해할 수 있다. 3) Pre-train/Fine-tuning 모델로 90년대 자연어 처리 모델 : 사람이 직접 모델의 입력값을 선정. 2000년대 이후 : 데이터를 통째로 모델에 넣고 입출력 사이의 관계 를 사람의 개입없이 모델 스스로 이해해내도록 유도한다. 이러한 기법을 엔드 투 엔드 모델 이라고 한다. 대표적으로 시퀀스 투 시퀀스 모델이 있다. 2018년 이후 : 엔드투 엔드 방식에서 벗어나 pretrain/fine tuning 방식으로 발전해나가고 있다. 대규모 말뭉치로 임베딩을 만든다(프리트레인) : 이 말뭉치에는 단어의 의미/문법적 맥락이 포함되어있다. 임베딩을 입력으로 하는 새로운 딥러닝 모델을 만들고, 풀고자 하는 문제에 맞춰 임베딩을 포함한 모델 전체를 업데이트 한다.(파인 튜닝, 전이 학습) : ELMo, GPT, BERT 등이 해당 [용어 이해하기] 다운스트림 태스크 : 풀고자 하는 구체적 자연어처리 문제들. 품사판별, 개채명 인식, 의미역 분석 등이 있다. 업스트림 태스크 : 다운스트림 태스크에 앞서 해결해야할 과제. 단어/문장 임베딩을 프리트레인하는 과정이 이에 해당. 4) 임베딩의 종류와 성능 : 임베딩 기법은 크게 3가지로 나뉜다. 행렬분해 기반 방법 말뭉치 정보가 들어있는 기존의 행렬을 두 개 이상의 작은 행렬로 쪼개는 임베딩 기법 GloVe, Sweivel등이 이에 해당 예측 기반 방법 어떤 단어 주변에 특정 단어가 나타날지 예측하거나, 이전/다음/중간의 단어가 무엇일지 맞추는 과정에서 학습하는 임베딩 기법 신경망 기반 임베딩 기법이 이러한 예측 기반 방법에 속한다 : Word2Vec, FastText, BERT, ELMo, GPT 등이 이에 해당 토픽 기반 방법 주어진 문서에 잠재된 주제를 추론하는 방식 의 임베딩 기법 잠재 디리클레 할당이 대표적 기법이다.","link":"/2020/02/14/ai-study1/"},{"title":"YAMLException: can not read a block mapping entry, Hexo","text":"How to solve problem YAMLException: can not read a block mapping entry 포스팅을 하면서 다음과 같은 에러가 발생했다. 이는 아래와 같이 Hexo로 신규 게시물 작성 시 title에 [] 대괄호를 사용하면 발생하는 에러이다. (,나 :같은 기호를 사용해도 발생한다.) The error above occurs because of the [] square brackets used in a title of your post. 따라서 위의 타이틀을 “” 로 묶어주면 에러를 해결할 수 있다. Just add a title with double quotation marks to solve the problem. 1title : \"[딥러닝 스터디] 임베딩이란\"","link":"/2020/02/16/yaml-exception/"},{"title":"[딥러닝 스터디] 자연어의 계산과 이해","text":"다음의 책을 공부하며 정리한 내용입니다 한국어 임베딩 - 이기창 https://wikidocs.net/21668 https://wikidocs.net/21687 https://wikidocs.net/21692 2장. 언어모델이란: 자연어의 의미를 임베딩에 어떻게 녹여낼 수 있는가? 그 비결은 자연어의 통계적 패턴 을 통째로 임베딩에 넣는 것이다. 임베딩을 만드는 세가지 철학 구분 bag of words 가정 언어 모델 분포 가정 내용 어떤 단어가 (많이) 쓰였는가 단어가 어떤 순서로 쓰였는가 어떤 단어가 같이 쓰였는가 대표 통계량 TF-IDF - PMI 대표 모델 Deep Averaging Network ELMo, GPT Word2Vec 자연어의 의미는 해당 언어 사용자들이 실제 사용하는 일상 언어에서 드러난다. 따라서 실제 사람이 사용하는 자연어의 통계적 패턴정보를 임베딩에 넣는다면 임베딩에 자연어의 의미를 함축해 넣을 수 있다. 임베딩을 만들때 쓰는 통계정보는 크게 3가지가 있다. 첫째, 문장에 어떤 단어가 (많이) 쓰였는가 둘째, 단어가 어떤 순서로 등장하는가 셋째, 어떤 단어가 같이 나타났는가 2-1. 어떤 단어가 많이 쓰였는가 Bag of words 가정 단어의 등장 순서에 관계없이 문서 내 단어의 등장 빈도 를 임베딩으로 쓰는 기법 저자가 생각한 주제 가 분서에서의 단어 사용 에 녹아들어있다는 가정을 바탕으로 한다. 정보 검색 분야에서 많이 사용한다. 사용자 질의를 백오브워즈 임베딩으로 변환 후 코사인 유사도가 가장 높은 문서를 사용자에게 노출한다. TF - IDF 문서에 단순히 많이 나타나는 단어만으로 주제를 판단하기 어려울 수 있다.(예: 한국어 문서에는 조사 ‘을/를’이 많이 등장하지만 이를 통해 주제 파악은 어려움.) 이를 보안하기 위해 나타난 기법이 TF-IDF(Term Frequency-Inverse Document Frequency) TF : 어떤 단어가 특정 문서에 얼마나 쓰였는지의 빈도 DF : 특정 단어가 나타난 문서의 수 IDF : log( 전체 문서의 수 / 특정 단어의 DF ) 값이 클수록 특이한 단어임을 의미 단어가 문서의 주제와 연관있을 정도(주제 예측능력) 와 관련있다. 단어의 주제 예측능력이 클수록 TF-IDF 값이 커진다. Deep Averaging Network 백오브워즈 가정의 신경망 버전 2-2. 단어가 어떤 순서로 쓰였는가 언어 모델이란? 언어 모델이란 언어를 모델리아하고자 단어 시퀀스 에 확률을 부여 하는 모델이다. 단어의 등장 순서를 무시하는 백오브 워즈와 달리 시퀀스 정보를 명시적으로 학습 한다. 따라서 백오브 워즈의 대척점에 언어모델이 있다고 할 수 있다. 언어모델을 만드는 방법으로는 크게 1)통계를 이용한 방법과 2)신경망을 이용한 방법이 있다. 잘 학습된 언어모델은 어떤 문장이 더 자연스러운지, 또한 주어진 단어 시퀀스 다음에는 무엇이 오는게 자연스러운지를 알수있다. 이와 유사한 맥락에서 일각에서는 언어 모델을 문법(grammar) 이라 비유하기도 한다. 단어간의 조합이 얼마나 적절한지, 특정 문장이 얼마나 자연스러운지를 알려주는 언어모델의 역할이 마치 문법의 기능과 유사하기 때문이다. 단어가 n개 주어진 상황이라면 언어모델은 n개 단어가 동시에 나타날 확률 , 즉 P(w1, w2… wn)을 반환한다.(단어 시퀀스에의 확률 할당) 이는 다음과 같이 사용할 수 있다. a. 기계 번역(Machine Translation): P(나는 버스를 탔다) &gt; P(나는 버스를 태운다) : 언어 모델은 두 문장을 비교하여 좌측의 문장의 확률이 더 높다고 판단합니다. b. 오타 교정(Spell Correction)선생님이 교실로 부리나케 P(달려갔다) &gt; P(잘려갔다) : 언어 모델은 두 문장을 비교하여 좌측의 문장의 확률이 더 높다고 판단합니다. c. 음성 인식(Speech Recognition) P(나는 메롱을 먹는다) &lt; P(나는 메론을 먹는다) : 언어 모델은 두 문장을 비교하여 우측의 문장의 확률이 더 높다고 판단합니다. 언어 모델은 위와 같이 확률을 통해 보다 적절한 문장을 판단한다. 통계 기반 언어 모델(SLM: Statistical Language Model) 통계 기반 언어모델은 말뭉치에서 해당 단어 시퀀스가 얼마나 자주 등장하는지 의 빈도를 세어서 학습한다. 문장은 문맥이라는 관계 내에서 단어들이 관계를 갖고 완성해낸 시퀀스이다. 따라서 특정 문장의 확률은 각 단어들의 이전 단어가 주어졌을때 다음 단어로 등장할 확률의 곱 으로 계산된다. 즉, “나는 사과를 먹었다” 라는 문장의 확률은 다음과 같이 표현할 수 있다. (문장의 확률을 구하기 위해 다음 단어에 대한 예측 확률을 모두 곱한다.) P(나는 사과를 먹었다) = P(나는) * P(사과를|나는) * P(먹었다|나는, 사과를) 조건부 확률은 두개의 확률 P(A), P(B)에 대해 다음의 관계를 갖는다. P(B|A)=P(A,B)/P(A) P(A,B)=P(A)P(B|A) 더 많은 확률에 대해 일반화하면 다음과 같이 표현할 수 있다. P(x1,x2,x3…xn)=P(x1)P(x2|x1)P(x3|x1,x2)…P(xn|x1…xn−1) 이를 조건부 확률의 연쇄법칙(chain rule) 이라고 한다. 이때 특정 시퀀스로부터 다음 단어가 나올 확률은 카운트에 기반해 계산할 수 있다. N-gram 언어모델 n-gram 언어모델은 통계 기반 언어모델의 일종으로 전통적 SLM과 같이 카운트에 기반한 통계적 접근을 사용한다. 전통적 SLM과 달리 이전에 등장한 모든 단어가 아닌 일부 단어만 고려 하는 방법을 사용한다. 즉, n-gram에서 n은 n개의 단어, 혹은 n-gram에 기반한 언어모델을 의미한다. 말뭉치(corpus) 내 단어들을 n개씩 묶어서 그 빈도를 학습했다는 의미이다. 신경망 기반 언어모델 카운트 기반 접근은 그 방식상 본질적인 한계를 갖는다.","link":"/2020/02/16/ai-study2/"},{"title":"GithubPage 세팅하기","text":"온갖 블로그 사이트를 전전하다가 드디어 뭔가 쌈박한 친구를 발견했다…드디어 나는 정착할 블로그를 찾은 것인가..?! 참고한 사이트 https://pages.github.com/ : GithubPage Official Guide https://www.holaxprogramming.com/2017/04/16/github-page-and-hexo/ : GithubPage와 Hexo 설치하기 https://blog.zhangruipeng.me/hexo-theme-icarus/ : Hexo Official Guide Blog https://github.com/ppoffice/hexo-theme-icarus : Hexo Official Github https://alleyful.github.io/categories/Tools/Hexo/ : Hexo 설치하기 https://guides.github.com/features/mastering-markdown/ : Markdown 마스터하기 https://swtpumpkin.github.io/git/hexo/hexoImg/ : Hexo 이미지 올리기 https://mishka.kr/2019/06/10/hexo-writing/ : Hexo 글쓰기 Default GuideHexo 명령어 새 테마 적용: 새 테마 적용시에 일단 한번 클린 후 deploy 12hexo cleanhexo deploy --generate 새 글쓰기: https://mishka.kr/2019/06/10/hexo-writing/ 링크를 꼭 참고한다. 123456# layout은 post(default), draft, page가 있다.hexo new [layout] &lt;post_name&gt;hexo deploy --generate# draft로 작성시 publish 명령어 사용hexo publish [layout] &lt;post_name&gt; submodule update를 하는경우 theme폴더 내의 _config.yml파일이 지워지는 문제가 있다. 해당 파일은 항상 백업해두어야 한다ㅠ 로컬 서버 확인 1hexo s 블로그 꾸미기 로고 만들기 테마폴더의 이미지 파일들을 대체해준다. icarus테마 기준 \\themes\\icarus\\source\\images 내의 favicon, logo 등의 이미지를 변경해준다. https://logohub.io/ : 로고 제작 사이트 https://www.aconvert.com/image/png-to-svg/ : png파일 svg로 변환 변경한 이미지는 hexo clean후 배포해주어야 적용된다(테마 적용하듯이 적용!) 기타 팁 생각보다 시간이 오래 걸린다. 세팅하고 익숙해지는데에 거의 반나절이 걸렸다. hexo deploy로 새 글을 발행하는 경우 내 깃헙페이지를 관리하는 실제 레포지토리에는 완성된 글/글과 관련된 블로그 파일들만 올라간다. 여튼 무슨소리냐면 내가 로컬에서 글 쓰는 환경 그 자체는 업로드되지 않는다는 얘기다. 따라서 내가 글쓰는 환경은 따로 백업을 해줘야하는데 이게 또 글 쓰는 폴더 내의 themes 폴더는 다 별도의 깃헙 레포라서 그냥 통으로 올리면 제대로 백업이 안된다. https://mishka.kr/2019/06/13/backup/ 이분이 굉장히 잘 설명해주심. 내 원격 레포를 두개 만든다. 하나는 블로그 쓰는환경 전체파일 백업용(레포1), 다른 하나는 테마 폴더만 백업용(레포2) 테마 폴더의 원격 저장소 위치를 내 원격레포1로 변경해두고 커밋 테마 폴더를 지우고 나머지 블로그 글쓰는 환경 폴더를 깃 레포로 만든다. 테마 폴더를 submodule로 현재 로컬 레포에 추가한다. 블로그 전체 글쓰는 환경 폴더를 원격레포2로 전체 커밋 이후 테마가 추가되면 git submodule add로 추가해준다. 테마 관련 설정이 변경돼도 이렇게 해주면 될 듯. Warning 로컬 테마폴더의 깃 레포 url을 바꾸지 않으면 기존 icarus 레포에 푸시를 하게 된다!!ㅋㅋ.. 대담한 한국인이 되고싶다면 시도해볼것.. 마크다운 에디터가 있으면 편할 것 같아서 또 서치를 했다. Typora 로 현재 작성중. 가볍고 심플하니 나쁘지 않다. Welcome to Hexo! This is your very first post. Check documentation for more info. If you get any problems when using Hexo, you can find the answer in troubleshooting or you can ask me on GitHub. Quick StartCreate a new post1$ hexo new \"My New Post\" More info: Writing Run server1$ hexo server More info: Server Generate static files1$ hexo generate More info: Generating Deploy to remote sites1$ hexo deploy More info: Deployment","link":"/2020/02/14/how-to-start-githubPage/"}],"tags":[{"name":"pytorch","slug":"pytorch","link":"/tags/pytorch/"},{"name":"딥러닝기초","slug":"딥러닝기초","link":"/tags/%EB%94%A5%EB%9F%AC%EB%8B%9D%EA%B8%B0%EC%B4%88/"},{"name":"선형회귀","slug":"선형회귀","link":"/tags/%EC%84%A0%ED%98%95%ED%9A%8C%EA%B7%80/"},{"name":"linear regression","slug":"linear-regression","link":"/tags/linear-regression/"},{"name":"로지스틱 회귀","slug":"로지스틱-회귀","link":"/tags/%EB%A1%9C%EC%A7%80%EC%8A%A4%ED%8B%B1-%ED%9A%8C%EA%B7%80/"},{"name":"logistic regression","slug":"logistic-regression","link":"/tags/logistic-regression/"},{"name":"자연어처리","slug":"자연어처리","link":"/tags/%EC%9E%90%EC%97%B0%EC%96%B4%EC%B2%98%EB%A6%AC/"},{"name":"딥러닝","slug":"딥러닝","link":"/tags/%EB%94%A5%EB%9F%AC%EB%8B%9D/"},{"name":"언어모델","slug":"언어모델","link":"/tags/%EC%96%B8%EC%96%B4%EB%AA%A8%EB%8D%B8/"},{"name":"한국어 임베딩","slug":"한국어-임베딩","link":"/tags/%ED%95%9C%EA%B5%AD%EC%96%B4-%EC%9E%84%EB%B2%A0%EB%94%A9/"},{"name":"YAMLException","slug":"YAMLException","link":"/tags/YAMLException/"},{"name":"Hexo","slug":"Hexo","link":"/tags/Hexo/"},{"name":"githubpage","slug":"githubpage","link":"/tags/githubpage/"},{"name":"hexo","slug":"hexo","link":"/tags/hexo/"}],"categories":[{"name":"개발자 공부","slug":"개발자-공부","link":"/categories/%EA%B0%9C%EB%B0%9C%EC%9E%90-%EA%B3%B5%EB%B6%80/"},{"name":"인공지능","slug":"개발자-공부/인공지능","link":"/categories/%EA%B0%9C%EB%B0%9C%EC%9E%90-%EA%B3%B5%EB%B6%80/%EC%9D%B8%EA%B3%B5%EC%A7%80%EB%8A%A5/"},{"name":"그냥 그런 일상","slug":"그냥-그런-일상","link":"/categories/%EA%B7%B8%EB%83%A5-%EA%B7%B8%EB%9F%B0-%EC%9D%BC%EC%83%81/"},{"name":"그렇게 바보는 아님","slug":"그냥-그런-일상/그렇게-바보는-아님","link":"/categories/%EA%B7%B8%EB%83%A5-%EA%B7%B8%EB%9F%B0-%EC%9D%BC%EC%83%81/%EA%B7%B8%EB%A0%87%EA%B2%8C-%EB%B0%94%EB%B3%B4%EB%8A%94-%EC%95%84%EB%8B%98/"},{"name":"끄적끄적","slug":"그냥-그런-일상/끄적끄적","link":"/categories/%EA%B7%B8%EB%83%A5-%EA%B7%B8%EB%9F%B0-%EC%9D%BC%EC%83%81/%EB%81%84%EC%A0%81%EB%81%84%EC%A0%81/"}]}