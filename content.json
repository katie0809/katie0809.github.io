{"pages":[{"title":"Downloads","text":"인공지능자료 자연어처리 발표 참고자료 자연어처리 발표자료 자연어처리 발표자료2 자연어처리 발표자료3","link":"/downloads/index.html"}],"posts":[{"title":"[딥러닝 기초] Pytorch 기본연산","text":"다음의 책을 공부하며 정리한 내용입니다 https://wikidocs.net/book/2788 Pytorch 기본 연산: pytorch를 활용한 신경망 구성을 위해 필수적인 딥러닝 기본 연산단위를 알아보자. Tensor 딥러닝의 가장 기본적인 연산단위 : 벡터, 행렬, 텐서 0차원 : 스칼라 1차원 : 벡터 2차원 : 행렬 3차원 이상 : 텐서 2D Tensor : 행렬 2차원 텐서는 말그대로 ‘행렬‘이다. 따라서 2차원 텐서 t는 다음과 같이 나타낼 수 있다. |t| = (배치 사이즈, 차원) 행렬의 행의 개수 = 배치사이즈 행렬의 열의 개수 = 차원(dimension) 123456789101112131415t = tor.FloatTensor([0,1,2,3])print(t)print(t.dim(), t.size()) # rank(차원), 원소개수# 인덱스로 접근 가능하다print(t[1])# 정수 텐서lt = tor.LongTensor([[1, 2, 3], [4, 5, 6], [7, 8, 9], [10, 11, 12] ])print(lt)print(lt[2], lt[2].size()) 12345678tensor([0., 1., 2., 3.])1 torch.Size([4])tensor(1.)tensor([[ 1, 2, 3], [ 4, 5, 6], [ 7, 8, 9], [10, 11, 12]])tensor([7, 8, 9]) torch.Size([3]) 3D Tensor 이미지/영상처리 분야에서는 보다 복잡한 텐서를 다룬다. 이미지는 가로/세로가 존재하며, 따라서 여러장의 이미지는 자연스레 (가로, 세로, 배치 크기) 가 됨을 연상할 수 있다. 3D Tensor in NLPNatural Language Processing(자연어처리)에서는 보통 (문장길이, 차원, 배치 크기) 라는 3차원 텐서를 사용한다.","link":"/2020/02/17/ai-start1/"},{"title":"[딥러닝 기초] Linear Regression","text":"다음의 책을 공부하며 정리한 내용입니다 https://wikidocs.net/book/2788 선형회귀란: 선형 회귀란 학습 데이터와 가장 잘 맞는 하나의 직선을 찾는 일. 하나의 직선은 W와 b로 정의할 수 있다. 선형 회귀의 목표: 가장 잘 맞는 직선을 정의하는 W와 b의 값을 찾는 것. 파이토치에서의 선형회귀 선형 회귀 모델: nn.Linear() 평균 제곱오차: nn.functional.mse_loss() torch.manual_seed() : 현재의 코드를 재실행해도 다음에도 같은 결과가 나오도록 랜덤 시드(random seed)를 준다. 선형회귀 훈련을 위한 기본적인 코드의 뼈대는 아래와 같다 123456789101112131415161718192021222324252627# 1.훈련데이터의 선언# x_train의 벡터가 y_train이 되도록 하는 w와 b를 찾는다import torchx_train = torch.FloatTensor([[1], [2], [3]])y_train = torch.FloatTensor([[2], [4], [6]])# 2.가중치 W와 편향 b를 0으로 초기화# requires_grad: 학습을 통해 값이 변경되는 변수임을 명시.W = torch.zeros(1, requires_grad=True)b = torch.zeros(1, requires_grad=True)# 3.직선의 방정식(=선형회귀)에 해당되는 가설을 선언한다.# 가설 = 시스템이 학습한 w,b로 예측한 y_hat값hypothesis = x_train * W + b# print(hypothesis)# 4.사용할 비용함수 선언(MSE)cost = torch.mean((hypothesis - y_train) ** 2) # 5.W와 b를 SGD(경사하강법: Stochastic Gradient Descent)로 훈련시킨다# lr: learning rateoptimizer = torch.optim.SGD([W, b], lr=0.01) 이를 바탕으로 실제 동작하는 선형회귀 모델을 제작해 학습을 진행해보면 아래와 같다. 12345678910111213141516171819202122232425262728293031323334353637383940# 최종코드import torchtorch.manual_seed(1)# 훈련데이터 선언x_train = torch.FloatTensor([[1], [2], [3]])y_train = torch.FloatTensor([[2], [4], [6]])# 학습데이터 w,b 선언. 둘다 값이 1인 임의의 스칼라 텐서w = torch.zeros(1, requires_grad=True)b = torch.zeros(1, requires_grad=True)# SGD방식을 사용한 최적화 선언optimizer = torch.optim.SGD([w, b], lr=0.01)# 학습 횟수는 1000+1회tot_epoch = 1000for cur_epoch in range(tot_epoch + 1): # 새 학습값 y_hat = (w * x_train) + b # MSE함수통한 비용 계산 cost = torch.mean((y_train - y_hat) ** 2) # optimizer로 w,b 학습시킴으로서 y_hat 개선 # gradient를 0으로 초기화 optimizer.zero_grad() # 비용 함수를 미분하여 gradient 계산 cost.backward() # W와 b를 업데이트 optimizer.step() # 100번마다 로그 출력 if cur_epoch % 100 == 0: print('Epoch {:4d}/{} w: {:.3f}, b: {:.3f} Cost: {:.6f}'.format( cur_epoch, tot_epoch, w.item(), b.item(), cost.item() )) 123456789101112[결과]Epoch 0/1000 w: 0.187, b: 0.080 Cost: 18.666666Epoch 100/1000 w: 1.746, b: 0.578 Cost: 0.048171Epoch 200/1000 w: 1.800, b: 0.454 Cost: 0.029767Epoch 300/1000 w: 1.843, b: 0.357 Cost: 0.018394Epoch 400/1000 w: 1.876, b: 0.281 Cost: 0.011366Epoch 500/1000 w: 1.903, b: 0.221 Cost: 0.007024Epoch 600/1000 w: 1.924, b: 0.174 Cost: 0.004340Epoch 700/1000 w: 1.940, b: 0.136 Cost: 0.002682Epoch 800/1000 w: 1.953, b: 0.107 Cost: 0.001657Epoch 900/1000 w: 1.963, b: 0.084 Cost: 0.001024Epoch 1000/1000 w: 1.971, b: 0.066 Cost: 0.000633 위의 코드와 결과는 다음을 의미한다. x가 [1, 2, 3]일때 y가 [2,4,6]이 되는 w와 b는 2, 0이다. 학습을 통해 최종적으로 찾은 결과 w,b는 1.971, 0.066이므로 어느정도 답을 찾아냈다고 볼 수 있다. 다중선형회귀: 기존의 선형회귀가 y = wx + b의 w,b를 찾는 모델이었다면, 다중선형회귀는 y = w1x1 + w2x2 + w3x3 + b의 w1, w2, w3, b를 찾는 모델이다. 선형회귀 훈련을 위한 기본적인 코드의 뼈대는 아래와 같다. 123456789101112131415161718# N개의 x_train 벡터와 가중치 w는 행렬로 표현할 수 있다.# 5 * 3 학습벡터 =&gt; 길이 5의 x_train벡터 3개x_train = torch.FloatTensor([[73, 80, 75], [93, 88, 93], [89, 91, 90], [96, 98, 100], [73, 66, 70]]) # y_train벡터의 길이 = x_train벡터의 길이(5)y_train = torch.FloatTensor([[152], [185], [180], [196], [142]])# 가중치 w의 개수 = x_train벡터의 개수(3) ** 길이는 1 **w = torch.zeros((3, 1) ,requires_grad=True)b = torch.zeros(1, requires_grad=True)# x_train과 y_train벡터가 각각 행렬이므로# 가설은 파이토치 행렬곱을 활용해 정의해준다. hypothesis = x_train.matmul(w) + b 이를 바탕으로 실제 동작하는 다중선형회귀 모델을 제작해 학습을 진행해보면 아래와 같다. 123456789101112131415161718192021222324252627282930313233# 학습을 위한 코드는 기존과 동일하다.import torchtorch.manual_seed(1)x_train = torch.FloatTensor([[73, 80, 75], [93, 88, 93], [89, 91, 90], [96, 98, 100], [73, 66, 70]]) y_train = torch.FloatTensor([[152], [185], [180], [196], [142]])# 모델 초기화w = torch.zeros((3, 1) ,requires_grad=True)b = torch.zeros(1, requires_grad=True)# SGD방식을 사용한 최적화 선언optimizer = torch.optim.SGD([w, b], lr=1e-5)tot_epoch = 1000for cur_epoch in range(tot_epoch + 1): y_hat = x_train.matmul(w) + b cost = torch.mean((y_train - y_hat) ** 2) optimizer.zero_grad() cost.backward() optimizer.step() # 100번마다 로그 출력 if cur_epoch % 100 == 0: print('Epoch {:4d}/{} Cost: {:.6f}'.format( cur_epoch, tot_epoch, cost.item() )) 123456789101112[결과]Epoch 0/1000 Cost: 29661.800781Epoch 100/1000 Cost: 1.563628Epoch 200/1000 Cost: 1.497595Epoch 300/1000 Cost: 1.435044Epoch 400/1000 Cost: 1.375726Epoch 500/1000 Cost: 1.319507Epoch 600/1000 Cost: 1.266222Epoch 700/1000 Cost: 1.215703Epoch 800/1000 Cost: 1.167810Epoch 900/1000 Cost: 1.122429Epoch 1000/1000 Cost: 1.079390","link":"/2020/02/17/ai-start2/"},{"title":"[딥러닝 기초] 선형회귀 모델의 개선","text":"다음의 책을 공부하며 정리한 내용입니다 https://wikidocs.net/book/2788 nn.Module 을 사용한 선형회귀 모델의 개선: 파이토치에서 일부 모델(ex. 선형회귀모델)들은 이미 nn.Module의 형태로 편리하게 쓸 수 있도록 구현되어있다. 즉, 우리가 기존에 구현했던 선형회귀 수식 y_hat = (w * x_train) + b y_hat = x_train.matmul(w) + b 이는 아래와 같이 변경할 수 있다. model = nn.Linear(1, 1) model = nn.Linear(3, 1) : 이때 nn.Linear()은 선형회귀 모델을 의미하며 왼쪽부터 순서대로 input dimension, output dimension이다. input_dim : 가중치 w의 개수 output_dim : 가중치 w의 길이 nn.Linear 모델 사용해보기 nn.Linear( )에는 가중치w와 편향b가 저장되어있다. 이는 model.parameters( )로 불러올 수 있다. 12345678# 모델을 선언 및 초기화. 단순 선형 회귀이므로 input_dim=1, output_dim=1.import torchimport torch.nn as nnmodel = nn.Linear(1,1)# 출력되는 첫번째값이 w, 두번째값이 b. 랜덤 초기화되어있는 상태이다.print(list(model.parameters())) 1234[결과][Parameter containing:tensor([[0.5153]], requires_grad=True), Parameter containing:tensor([-0.4414], requires_grad=True)] 기존 선형회귀 코드의 개선: 기존 선형회귀 코드를 다음과 같이 개선할 수 있다. 개선 1123# 최종코드import torchtorch.manual_seed(1) torch.nn 까지 import 1234# 최종코드import torchimport torch.nntorch.manual_seed(1) 개선 2123456789101112131415for epoch in range(1000): # 새 학습값 y_hat = (w * x_train) + b # MSE함수통한 비용 계산 cost = torch.mean((y_train - y_hat) ** 2) # optimizer로 w,b 학습시킴으로서 y_hat 개선 # gradient를 0으로 초기화 optimizer.zero_grad() # 비용 함수를 미분하여 gradient 계산 cost.backward() # W와 b를 업데이트 optimizer.step() nn.Linear( )로 선언한 model로 y_hat 계산 F.mse_loss(prediction, y_train) 파이토치에서 제공하는 평균제곱함수로 cost 계산 1234567891011121314151617181920212223242526model = nn.Linear(1, 1) for epoch in range(1000): # 새 학습값 y_hat = model(x_train) # MSE함수통한 비용 계산 cost = F.mse_loss(y_hat, y_train) # optimizer로 w,b 학습시킴으로서 y_hat 개선 # gradient를 0으로 초기화 optimizer.zero_grad() # 비용 함수를 미분하여 gradient 계산 cost.backward() # W와 b를 업데이트 optimizer.step() # 임의의 입력 4를 선언new_var = torch.FloatTensor([[4.0]]) # 입력한 값 4에 대해서 예측값 y를 리턴받아서 pred_y에 저장pred_y = model(new_var) # forward 연산# y = 2x 이므로 입력이 4라면 y가 8에 가까운 값이 나와야 제대로 학습이 된 것print(\"훈련 후 입력이 4일 때의 예측값 :\", pred_y) 12[결과]훈련 후 입력이 4일 때의 예측값 : tensor([[7.9989]], grad_fn=&lt;AddmmBackward&gt;) 위의 코드로 학습한 모델 model은 x_train, y_train에 대해 학습된 값 w,b 를 저장하고 있다. 학습된 모델 model을 활용해 새로운 값 x_new 에 대한 예측값 y_pred를 얻을 수 있다.","link":"/2020/02/17/ai-start3/"},{"title":"[딥러닝 기초] Logistic Regression","text":"다음의 책을 공부하며 정리한 내용입니다 https://wikidocs.net/book/2788 로지스틱 회귀란: 로지스틱 회귀는 이진분류(Binary Classification) 문제의 해결에 사용되는 대표적인 알고리즘. 이름은 ‘회귀‘이지만 ‘분류‘에 쓰인다 이진분류의 모델 점수(x) 결과(y) 45 불합격 50 불합격 55 불합격 60 합격 65 합격 70 합격 위와 같은 데이터가 있다고 하자. 조건은 아래와 같다. 합격 커트라인은 알려져있지 않다 임의의 점수 x의 합격여부를 예측하고 싶다. 이 경우 주어진 데이터에 대해 합격(1), 불합격(0)으로 그래프를 그리면 아래와 같이 표현할 수 있다. 위의 간단한 예시를 통해 이진분류의 문제를 풀기위한 x, y의 관계는 S 형태의 그래프로 나타내야 함을 알 수 있다. 따라서 다음과 같은 결론을 얻을 수 있다. 로지스틱 회귀의 가설은 선형 회귀 때의 H(x)=Wx+b가 아니다. 위처럼 S자 모양의 그래프를 만들 수 있는 어떤 특정 함수 f를 추가적으로 사용하여 H(x)=f(Wx+b)의 가설을 사용한다. 어떤 함수 f는 이미 널리 알려져있다. =&gt; 시그모이드 함수 즉, 로지스틱 회귀의 가설이자 이진분류 문제를 풀기위한 함수 f는 Sigmoid function이다 Sigmoid 수식 H(x)=sigmoid(Wx+b)=1+e−(Wx+b)=σ(Wx+b) 가중치(w)의 변화에 따른 Sigmoid 함수 red: w값이 0.5 ~ blue: w값이 2 편향(b)의 변화에 따른 Sigmoid 함수 red: b값이 0.5 ~ blue: b값이 2 Sigmoid 함수의 특성 시그모이드 함수는 입력값이 한없이 커지면 1에 수렴하고, 입력값이 한없이 작아지면 0에 수렴한다. 시그모이드 함수의 출력은 0~1 위의 특성을 이용하여 분류 작업에 사용. 임계값 x(0 =&lt; x =&lt; 1)를 넘으면 1, 넘지 못하면 0으로 분류 결론 로지스틱 회귀의 가설/모델은 H(x)=sigmoid(Wx+b) 이다. 로지스틱 회귀 실습12345678910111213141516171819202122232425262728293031323334# 최종코드import torchimport torch.nn.functional as Ftorch.manual_seed(1)x_train = torch.FloatTensor([[1, 2], [2, 3], [3, 1], [4, 3], [5, 3], [6, 2]])y_train = torch.FloatTensor([[0], [0], [0], [1], [1], [1]])w = torch.zeros([2, 1], requires_grad=True)b = torch.zeros([1], requires_grad=True)# hypothesis = 1 / (1 + torch.exp(-(x_train.matmul(w) + b)))hypothesis = torch.sigmoid(x_train.matmul(w) + b)optimizer = torch.optim.SGD([w, b], lr=1)tot_epoch = 1000for cur_epoch in range(tot_epoch + 1): y_hat = torch.sigmoid(x_train.matmul(w) + b) cost = F.binary_cross_entropy(y_hat, y_train) optimizer.zero_grad() cost.backward() optimizer.step() # 100번마다 로그 출력 if cur_epoch % 100 == 0: print('Epoch {:4d}/{} Cost: {:.6f}'.format(cur_epoch, tot_epoch, cost.item())) # 제대로 학습됐는지 확인print(w)print(b) 123456789101112131415[결과]Epoch 0/1000 Cost: 0.693147Epoch 100/1000 Cost: 0.134722Epoch 200/1000 Cost: 0.080643Epoch 300/1000 Cost: 0.057900Epoch 400/1000 Cost: 0.045300Epoch 500/1000 Cost: 0.037261Epoch 600/1000 Cost: 0.031672Epoch 700/1000 Cost: 0.027556Epoch 800/1000 Cost: 0.024394Epoch 900/1000 Cost: 0.021888Epoch 1000/1000 Cost: 0.019852tensor([[3.2530], [1.5179]], requires_grad=True)tensor([-14.4819], requires_grad=True)","link":"/2020/02/17/ai-start4/"},{"title":"[딥러닝 스터디] 임베딩이란","text":"다음의 책을 공부하며 정리한 내용입니다 한국어 임베딩 - 이기창 1장. 임베딩이란: 임베딩이란 자연어를 벡터로 바꾼 결과 혹은 그 일련의 과정 전체를 의미하는 용어이다. 임베딩에는.. 말뭉치(corpus)의 의미, 문법 정보가 응축되어있다. 벡터이기 때문에 사칙연산이 가능하다 단어/문서 관련도를 계산할 수 있다. 대규모 말뭉치(corpus)를 미리 학습한 임베딩을 다른 문제를 푸는 데에 재사용 할 수 있다(전이학습) 임베딩 품질이 좋으면 단순한 모델로도 원하는 성능을 낼 수 있다. 따라서 자연어 처리 모델의 구성과 서비스에 있어 가장 중요한 구성요소 중 하나는 임베딩이라고 꼽을 수 있다. 임베딩 소스코드 내려받기 : 다양한 논문 저자들이 공개한 실제 임베딩 코드를 통해 자신만의 임베딩을 구축할 수 있다. 1-1. 임베딩이란 임베딩이란 사람이 쓰는 자연어 를 기계가 이해할 수 있는 숫자의 나열인 벡터 로 바꾼 결과/일련의 과정 을 의미한다. 이는 단어나 문장 각각을 벡터로 변환해 벡터공간으로 끼워넣는다 는 의미에서 임베딩이란 이름이 붙게 되었다. 가장 간단한 임베딩은 단어의 빈도를 벡터로 사용하는 것이다. 근대 소설 작품 몇 편에 나오는 단어 기차 , 막걸리 , 선술집 의 예시를 통해 알아보자. 구분 메밀꽃 필 무렵 운수좋은 날 사랑손님과 어머니 삼포가는 길 기차 0 2 10 7 막걸리 0 1 0 0 선술집 0 1 1 0 기차의 임베딩은 [0,2,10,7], 막걸리의 임베딩은 [0,1,0,0], 선술집의 임베딩은 [0,1,1,0]이다. 이를 바탕으로 우리는 기차(blue)-막걸리(red)간 의미차이가 선술집(orange)-막걸리(red)간 의미 차이보다 크다는 것을 알 수 있다. 1-2. 임베딩의 역할임베딩의 역할은 위에서 언급한 것과 같이 크게 3가지로 분류할 수 있다. 단어/문장 간 관련도 계산 : 임베딩된 단어(벡터)는 단어 간 유사도를 계산할 수 있다.(코사인 유사도) 의미적/문법적 정보 함축 : 임베딩은 벡터 인 만큼 사칙연산이 가능하다. 따라서 임베딩된 단어는 사칙 연산을 통해 단어간의 의미적/문법적 관계를 도출해낼 수 있다. 예를 들어 품질이 좋은 임베딩은 다음의 관계를 도출해낼 수 있다. 아들 - 딸 + 소녀 = 소년 전이 학습 : 임베딩은 자주 다른 딥러닝 모델의 입력값으로 쓰인다. 이를 전이학습이라고 한다. 1-3. 임베딩 기법의 역사와 종류임베딩 기법의 발전흐름과 종류는 다음과 같이 정리할 수 있다. 1) 통계기반에서 뉴럴 네트워크 기반으로 초기 임베딩 기법은 말뭉치의 통계량을 직접적으로 활용 최근에는 신경망 기반의 임베딩 기법이 사용된다 : 다음/이전/중간 단어의 예측을 해내는 과정에서 학습 2) 단어 수준에서 문장 수준으로 2017년 이전의 임베딩 기법은 대게 단어수준 모델이었다 : NPLM, Word2Vec, GloVe, FastText, Swivel 등 이는 동음이의어를 분간하기 어렵다는 문제가 있다 : 사람의 눈 과 하늘에서 내리는 눈 은 엄연히 다르지만 임베딩 벡터는 하나. 2018년 이후 문장수준 임베딩 기법들이 주목받았다 : BERT, GPT, ELMo 문장수준 임베딩 기법은 개별 단어가 아닌 단어 시퀀스 전체의 문맥적 의미를 함축한다. 따라서 단어임베딩보다 학습 효과가 좋다. 다의어 ‘bank’를 문맥에 따라 시각화한 모습. 의미가 다른 단어를 분리해 이해할 수 있다. 3) Pre-train/Fine-tuning 모델로 90년대 자연어 처리 모델 : 사람이 직접 모델의 입력값을 선정. 2000년대 이후 : 데이터를 통째로 모델에 넣고 입출력 사이의 관계 를 사람의 개입없이 모델 스스로 이해해내도록 유도한다. 이러한 기법을 엔드 투 엔드 모델 이라고 한다. 대표적으로 시퀀스 투 시퀀스 모델이 있다. 2018년 이후 : 엔드투 엔드 방식에서 벗어나 pretrain/fine tuning 방식으로 발전해나가고 있다. 대규모 말뭉치로 임베딩을 만든다(프리트레인) : 이 말뭉치에는 단어의 의미/문법적 맥락이 포함되어있다. 임베딩을 입력으로 하는 새로운 딥러닝 모델을 만들고, 풀고자 하는 문제에 맞춰 임베딩을 포함한 모델 전체를 업데이트 한다.(파인 튜닝, 전이 학습) : ELMo, GPT, BERT 등이 해당 [용어 이해하기] 다운스트림 태스크 : 풀고자 하는 구체적 자연어처리 문제들. 품사판별, 개채명 인식, 의미역 분석 등이 있다. 업스트림 태스크 : 다운스트림 태스크에 앞서 해결해야할 과제. 단어/문장 임베딩을 프리트레인하는 과정이 이에 해당. 4) 임베딩의 종류와 성능 : 임베딩 기법은 크게 3가지로 나뉜다. 행렬분해 기반 방법 말뭉치 정보가 들어있는 기존의 행렬을 두 개 이상의 작은 행렬로 쪼개는 임베딩 기법 GloVe, Sweivel등이 이에 해당 예측 기반 방법 어떤 단어 주변에 특정 단어가 나타날지 예측하거나, 이전/다음/중간의 단어가 무엇일지 맞추는 과정에서 학습하는 임베딩 기법 신경망 기반 임베딩 기법이 이러한 예측 기반 방법에 속한다 : Word2Vec, FastText, BERT, ELMo, GPT 등이 이에 해당 토픽 기반 방법 주어진 문서에 잠재된 주제를 추론하는 방식 의 임베딩 기법 잠재 디리클레 할당이 대표적 기법이다.","link":"/2020/02/14/ai-study1/"},{"title":"[딥러닝 스터디] 자연어의 계산과 이해","text":"다음의 책을 공부하며 정리한 내용입니다 한국어 임베딩 - 이기창 https://wikidocs.net/21668 https://wikidocs.net/21687 https://wikidocs.net/21692 2장. 언어모델이란: 자연어의 의미를 임베딩에 어떻게 녹여낼 수 있는가? 그 비결은 자연어의 통계적 패턴 을 통째로 임베딩에 넣는 것이다. 임베딩을 만드는 세가지 철학 구분 bag of words 가정 언어 모델 분포 가정 내용 어떤 단어가 (많이) 쓰였는가 단어가 어떤 순서로 쓰였는가 어떤 단어가 같이 쓰였는가 대표 통계량 TF-IDF - PMI 대표 모델 Deep Averaging Network ELMo, GPT Word2Vec 자연어의 의미는 해당 언어 사용자들이 실제 사용하는 일상 언어에서 드러난다. 따라서 실제 사람이 사용하는 자연어의 통계적 패턴정보를 임베딩에 넣는다면 임베딩에 자연어의 의미를 함축해 넣을 수 있다. 임베딩을 만들때 쓰는 통계정보는 크게 3가지가 있다. 첫째, 문장에 어떤 단어가 (많이) 쓰였는가 둘째, 단어가 어떤 순서로 등장하는가 셋째, 어떤 단어가 같이 나타났는가 2-1. 어떤 단어가 많이 쓰였는가 Bag of words 가정 단어의 등장 순서에 관계없이 문서 내 단어의 등장 빈도 를 임베딩으로 쓰는 기법 저자가 생각한 주제 가 분서에서의 단어 사용 에 녹아들어있다는 가정을 바탕으로 한다. 정보 검색 분야에서 많이 사용한다. 사용자 질의를 백오브워즈 임베딩으로 변환 후 코사인 유사도가 가장 높은 문서를 사용자에게 노출한다. TF - IDF 문서에 단순히 많이 나타나는 단어만으로 주제를 판단하기 어려울 수 있다.(예: 한국어 문서에는 조사 ‘을/를’이 많이 등장하지만 이를 통해 주제 파악은 어려움.) 이를 보안하기 위해 나타난 기법이 TF-IDF(Term Frequency-Inverse Document Frequency) TF : 어떤 단어가 특정 문서에 얼마나 쓰였는지의 빈도 DF : 특정 단어가 나타난 문서의 수 IDF : log( 전체 문서의 수 / 특정 단어의 DF ) 값이 클수록 특이한 단어임을 의미 단어가 문서의 주제와 연관있을 정도(주제 예측능력) 와 관련있다. 단어의 주제 예측능력이 클수록 TF-IDF 값이 커진다. Deep Averaging Network 백오브워즈 가정의 신경망 버전 2-2. 단어가 어떤 순서로 쓰였는가 언어 모델이란? 언어 모델이란 딥러닝과 관계없이 이전부터 있었던 개념으로, 언어를 모델링하고자 단어 시퀀스 에 확률을 부여 하는 모델이다. 단어의 등장 순서를 무시하는 백오브 워즈와 달리 시퀀스 정보를 명시적으로 학습 한다. 따라서 백오브 워즈의 대척점에 언어모델이 있다고 할 수 있다. 언어모델을 만드는 방법으로는 크게 1)통계를 이용한 방법과 2)신경망을 이용한 방법이 있다. 잘 학습된 언어모델은 어떤 문장이 더 자연스러운지, 또한 주어진 단어 시퀀스 다음에는 무엇이 오는게 자연스러운지를 알수있다. 이와 유사한 맥락에서 일각에서는 언어 모델을 문법(grammar) 이라 비유하기도 한다. 단어간의 조합이 얼마나 적절한지, 특정 문장이 얼마나 자연스러운지를 알려주는 언어모델의 역할이 마치 문법의 기능과 유사하기 때문이다. 단어가 n개 주어진 상황이라면 언어모델은 n개 단어가 동시에 나타날 확률 , 즉 P(w1, w2… wn)을 반환한다.(단어 시퀀스에의 확률 할당) 이는 다음과 같이 사용할 수 있다. a. 기계 번역(Machine Translation): P(나는 버스를 탔다) &gt; P(나는 버스를 태운다) : 언어 모델은 두 문장을 비교하여 좌측의 문장의 확률이 더 높다고 판단합니다. b. 오타 교정(Spell Correction)선생님이 교실로 부리나케 P(달려갔다) &gt; P(잘려갔다) : 언어 모델은 두 문장을 비교하여 좌측의 문장의 확률이 더 높다고 판단합니다. c. 음성 인식(Speech Recognition) P(나는 메롱을 먹는다) &lt; P(나는 메론을 먹는다) : 언어 모델은 두 문장을 비교하여 우측의 문장의 확률이 더 높다고 판단합니다. 언어 모델은 위와 같이 확률을 통해 보다 적절한 문장을 판단한다. 정리 언어를 모델링하고자 단어 시퀀스에 확률을 부여하는 모델로, 잘 학습된 언어모델은 어떤 시퀀스가 자연스러운지를 판단해낸다. 인간이 쓰는 자연어는 레이블이 없는 비지도 학습. 이를 지도학습과 같이 학습하도록 여러 방법을 사용 문제1) 비지도학습인 자연어를 어떻게 학습할 것인가? : 이전 단어들이 주어졌을 때 다음 단어를 예측하도록 한다. 문제2) 각 단어시퀀스에 확률은 어떻게 할당할 것인가? : 카운트 기반 접근방식 사용한다. 언어모델의 종류 통계 기반 언어 모델(SLM: Statistical Language Model) 통계 기반 언어모델은 말뭉치에서 해당 단어 시퀀스가 얼마나 자주 등장하는지 의 빈도를 세어서 학습한다. 문장은 문맥이라는 관계 내에서 단어들이 관계를 갖고 완성해낸 시퀀스이다. 따라서 특정 문장의 확률은 각 단어들의 이전 단어가 주어졌을때 다음 단어로 등장할 확률의 곱 으로 계산된다. 즉, “나는 사과를 먹었다” 라는 문장의 확률은 다음과 같이 표현할 수 있다. (문장의 확률을 구하기 위해 다음 단어에 대한 예측 확률을 모두 곱한다.) P(나는 사과를 먹었다) = P(나는) * P(사과를|나는) * P(먹었다|나는, 사과를) 조건부 확률은 두개의 확률 P(A), P(B)에 대해 다음의 관계를 갖는다. P(B|A)=P(A,B)/P(A) P(A,B)=P(A)P(B|A) 더 많은 확률에 대해 일반화하면 다음과 같이 표현할 수 있다. P(x1,x2,x3…xn)=P(x1)P(x2|x1)P(x3|x1,x2)…P(xn|x1…xn−1) 이를 조건부 확률의 연쇄법칙(chain rule) 이라고 한다. 이때 특정 시퀀스로부터 다음 단어가 나올 확률은 카운트에 기반해 계산할 수 있다. N-gram 언어모델 n-gram 언어모델은 통계 기반 언어모델의 일종으로 전통적 SLM과 같이 카운트에 기반한 통계적 접근을 사용한다. : 이전의 n-1개의 단어를 보고 n번째 단어를 예측하는 방식 전통적 SLM과 달리 이전에 등장한 모든 단어가 아닌 일부 단어만 고려 하는 방법을 사용한다. 즉, n-gram에서 n은 n개의 단어, 혹은 n-gram에 기반한 언어모델을 의미한다. 말뭉치(corpus) 내 단어들을 n개씩 묶어서 그 빈도를 학습했다는 의미이다. 한국어 언어 모델 예시 문장 확률 진이는 이 책을 세 번 읽었다 0.47 이 책이 진이한테 세 번 읽혔다 0.23 세 번이 진이한테 이 책을 읽혔다 0.07 : 자연스러운 한국어 문장에 높은 확률값을 부여한다. 표현 빈도 영원히 104 기억될 29 최고의 3503 명작이다 298 영원히 기억될 7 기억될 최고의 1 최고의 명작이다 23 기억될 최고의 명작이다 17 영원히 기억될 최고의 명작이다 0 전통적 SLM에서는 문법적으로 전혀 문제가 없는 ‘영원히 기억될 최고의 명작이다’ 라는 문장에 0의 확률을 부여하게 된다. =&gt; 이 말뭉치에 한번도 나타난 적 없기 때문. n-gram모델을 통해 이 문제를 일부 해결할 수 있다. 직전 n-1개 단어의 등장확률 로 전체 단어 시퀀스 등장 확률 을 근사 하는 것이다! ‘영원히 기억될 최고의‘ 시퀀스 뒤에 명작이다 라는 단어가 올 확률을 trigram으로 근사해보면 얼마일까? P(명작이다 | 영원히, 기억될, 최고의) (유사) P(명작이다 | 기억될, 최고의) = Freq(기억될, 최고의, 명작이다) / Freq(명작이다) = 17 / 298 위와 같이 단어를 슬라이딩 해가면서 수식을 풀어 생각한다면 전체 문장 ‘영원히 기억될 최고의 작품이다’를 trigram으로 계산하기 위한 수식은 다음과 같다. 영원히 기억될 이 등장할 확률 * 영원히 기억될 뒤에 최고의 가 등장할 확률 * 기억될 최고의 뒤에 명작이다 가 등장할 확률 [ 카운트 기반 접근방식의 본질적 한계 ] 희소문제 : Sparcity Problem 여전히 희소문제는 존재한다. 코퍼스 내에 단어시퀀스가 없을(카운트하지 못할) 확률은 여전히 존재. n의 선택은 trade-off : n의 크기를 키우면 예측의 정확도는 높아지지만, 코퍼스에서 해당 n개의 시퀀스를 카운트할 확률은 낮아짐(희소문제 증가), 모델사이즈 커짐. 반대로 n을 작게 선택하면 훈련 코퍼스에서 카운트는 잘 되겠지만 근사의 정확도는 현실의 확률분포와 멀어짐. =&gt; n은 최대 5를 넘어서는 안된다고 권장됨. NNLM(Neural Net Language Model)을 통해 언어 모델 또한 단어의 유사도를 학습 할 수 있도록 설계. 훈련 코퍼스에 없는 단어 시퀀스도 예측을 통해 유사한 단어 시퀀스를 참고하여 확률을 계산해낸다. 워드 임베딩의 아이디어이기도 한다. 단어를 continuous한 밀집벡터의 형태로 표현하여 희소문제를 해결(?) long-term dependency : 정해진 개수의 전 토큰만을 보기 때문에 볼 수 있는 시퀀스의 범위가 한정됨. 이는 모델의 정확도와 연관 An adorable little boy is spreading ( ? ) 위와 같은 문장이 있을 때 정답은 insults, smile 둘 중 하나라고 해보자. 문맥상 “작고 사랑스러운 아이가 (미소)를 퍼뜨렸다” 가 적절함을 알 수 있다. 하지만 만약 작고 사랑스러운 이라는 밑줄 친 부분을 예측에 고려하지 않는다면 엉뚱한 답 insults를 고를 수 있다. 신경망 기반 언어모델 카운트 기반 접근은 그 방식상 본질적인 한계를 갖는다. 이를 극복하기 위해 다양한 방법 시도되었지만(백오프, 스무딩) 본질적인 n-gram 언어모델(고정된 개수의 단어만을 입력으로 받아야한다)에 대한 취약점은 해결하지 못함. RNNLM(Recurrent Neural Network Language Model) 예문 what will the fat cat sit on 의 RNNLM 학습/사용 과정을 보자. 1) 훈련이 끝난 모델을 사용할 경우 RNNLM은 예측 과정에서 이전 시점의 출력을 현재 시점의 입력으로 한다. RNNLM은 what을 입력받으면, will을 예`하고 이 will은 다음 시점의 입력이 되어 the를 예측한다. 이것이 반복되어 네번째 시점의 cat은 앞서 나온 what, will, the, fat이라는 시퀀스로 인해 결정된 단어가 된다. 2) 모델을 훈련시키는 경우 위의 샘플에 대해 what will the fat cat sit 시퀀스를 모델의 입력으로 넣으면, will the fat cat sit on를 예측하도록 훈련한다. 이때 will, the, fat, cat, sit, on은 각 시점의 레이블(yt)가 된다. 이러한 RNN 훈련 기법을 교사 강요(teacher forcing) 라고 한다. 교사 강요(teacher forcing) : 테스트(실제 모델 사용) 과정에서 t 시점의 출력이 t+1 시점의 입력으로 사용되는 RNN 모델을 훈련시킬 때 사용하는 훈련 기법입니다. 훈련할 때 교사 강요를 사용할 경우, 모델이 t 시점에서 예측한 값을 t+1 시점에 입력으로 사용하지 않고, t 시점의 레이블. 즉, 실제 알고있는 정답을 t+1 시점의 입력으로 사용합니다. RNNLM의 학습 구조 임베딩층 : et=lookup(xt) 은닉층 : ht=tanh(Wxet+Whht−1+b) 출력층 : yt^=softmax(Wyht+b) 사용되는 손실함수 : cross-entropy 학습 과정에서 학습되는 가중치 행렬 : E(임베딩 행렬), Wx, Wh, Wy 2-3. 어떤 단어가 같이 쓰였는가분포가정: 자연어 처리에서 분포란 특정 범위(=윈도우)내에 동시에 등장하는 단어 또는 문맥의 집합 을 가리킨다. 어떤 단어 쌍이 비슷한 문맥 환경에서 자주 등장한다면 그 의미 또한 유사할 것이라는게 분포가정의 전제이다. 즉, 자연어를 사용하는 화자들이 특정 단어를 실제 어떻게 사용하는지 관찰 함으로서 단어의 의미를 밝힐 수 있다는 의미.","link":"/2020/02/16/ai-study2/"},{"title":"[딥러닝 스터디] 자연어 처리의 전처리","text":"다음의 책을 공부하며 정리한 내용입니다. https://wikidocs.net/22886 자연어 처리의 전처리 자연어 처리를 위해 자연어 데이터는 일반적으로 토큰화, 단어집합생성, 정수인코딩, 패딩, 벡터화의 과정을 거친다. 토큰화 주어진 텍스트를 단어/문자 단위로 자르는 것을 의미한다. 토큰화 도구 spaCy, NLTK: English Tokenization .split(): 파이썬 기본함수. 띄어쓰기 등으로 토큰화한다면.. 12345678# tokenizer 도구 사용해보기 - 상세 코드는 wikidocs.net/64157 참고import spacytext = \"A dog run back corner near bedrooms\"spacy_text = spacy.load('en')for token in spacy_text.tokenizer(text): print(token.text) 1234567Adogrunbackcornernearbedrooms 한국어 토큰화 영어와 달리 한국어는 띄어쓰기 단위로 토큰화 하면 ‘사과가 =/= 사과는’ 으로 인식되어 단어집합이 불필요하게 커진다. 형태소 토큰화 사용 : 형태소 분석기를 사용해 토큰화를 진행한다. 12345678# 형태소 분석기 중 mecab을 사용해 한국어 형태소 토큰화한다.# !git clone https://github.com/SOMJANG/Mecab-ko-for-Google-Colab.git# %cd Mecab-ko-for-Google-Colab# !bash install_mecab-ko_on_colab190912.shfrom konlpy.tag import Mecabtokenizer = Mecab()print(tokenizer.morphs(\"사과의 놀라운 효능이라는 글을 봤어. 그래서 오늘 사과를 먹으려고 했는데 사과가 썩어서 슈퍼에 가서 사과랑 오렌지 사왔어\")) 단어집합의 생성: 단어집합(vocabulary)란 중복을 제거한 텍스트 내 총 단어의 집합(set)을 의미한다. (실습) 네이버 영화 리뷰 데이터를 통해 단어집합 생성123456789101112# 20만개의 영화리뷰에 대해 긍정 1, 부정 0으로 레이블링한 네이버 데이터를 다운받는다.import urllib.requestimport pandas as pdimport numpy as npimport matplotlib.pyplot as plturllib.request.urlretrieve(\"https://raw.githubusercontent.com/e9t/nsmc/master/ratings.txt\", filename=\"ratings.txt\")data = pd.read_table('ratings.txt') # 데이터프레임에 저장print(data[15:20]) # 15번 ~ 20번까지의 리뷰 5개 뽑아보기# data는 해시맵으로 특정 리뷰의 내용에만 접근하기 위한 키는 document이다print(data['document'][15]) 12345678[결과] id document label15 9034036 평점 왜 낮아? 긴장감 스릴감 진짜 최고인데 진짜 전장에서 느끼는 공포를 생생하게 ... 116 979683 네고시에이터랑 소재만 같을 뿐.. 아무런 관련없음.. 117 165498 단연 최고 118 8703997 가면 갈수록 더욱 빠져드네요 밀회 화이팅!! 119 9468781 어?생각없이 봤는데 상당한 수작.일본영화 10년내 최고로 마음에 들었다.강렬한 임팩... 1평점 왜 낮아? 긴장감 스릴감 진짜 최고인데 진짜 전장에서 느끼는 공포를 생생하게 전해준다. 12345678910111213141516171819202122232425from konlpy.tag import Mecab# 임의의 10000개의 리뷰를 sample data로 사용한다.sample_data = data[:10000] # 한글과 공백을 제외하고 불필요한 문자는 모두 제거한다. - 정규표현식 사용sample_data['document'] = data['document'].str.replace(\"[^ㄱ-ㅎㅏ-ㅣ가-힣 ]\",\"\")# 불용어를 제거해준다. - 인터넷 검색 시 검색 용어로 사용하지 않는 단어. 관사, 전치사, 조사, 접속사 등은 검색 색인 단어로 의미가 없는 단어stopwords=['뭐','으면','을','의','가','이','은','들','는','좀','잘','걍','과','도','를','으로','자','에','와','한','하다']tokenizer = Mecab()res = []for sentence in sample_data['document']: tmp = [] tmp = tokenizer.morphs(sentence) tokenized = [] for token in tmp: if not token in stopwords: tokenized.append(token) res.append(tokenized)print(res[:2]) 12345678[결과]/usr/local/lib/python3.6/dist-packages/ipykernel_launcher.py:7: SettingWithCopyWarning: A value is trying to be set on a copy of a slice from a DataFrame.Try using .loc[row_indexer,col_indexer] = value insteadSee the caveats in the documentation: http://pandas.pydata.org/pandas-docs/stable/user_guide/indexing.html#returning-a-view-versus-a-copy import sys[['어릴', '때', '보', '고', '지금', '다시', '봐도', '재밌', '어요', 'ㅋㅋ'], ['디자인', '배우', '학생', '외국', '디자이너', '그', '일군', '전통', '통해', '발전', '해', '문화', '산업', '부러웠', '는데', '사실', '우리', '나라', '에서', '그', '어려운', '시절', '끝', '까지', '열정', '지킨', '노라노', '같', '전통', '있', '어', '저', '같', '사람', '꿈', '꾸', '고', '이뤄나갈', '수', '있', '다는', '것', '감사', '합니다']] 1234567891011import nltkfrom nltk import FreqDist# 단어-빈도수 조합으로 이루어진 단어집합 해시맵 vocab을 생성한다.# NLTK에서는 빈도수 계산 도구인 FreqDist()를 지원한다.vocab = FreqDist(np.hstack(res))print('단어 [별로]의 빈도수는? ', vocab['별로'], '번')# most_common(N) : 가장 빈도수가 높은 N개의 단어를 반환# 상위 500개의 단어만 보존vocab = vocab.most_common(500) 12[결과]단어 [별로]의 빈도수는? 37 번 각 단어에 고유한 정수 부여 각 토큰에 고유한 정수를 부여한다. 0과 1은 특수 인덱스로 사용한다. 인덱스 0 : 단어집합에 없는 토큰 인덱스 1 : 패딩 토큰(길이맞추기용) 12345678910111213141516171819202122232425word_to_index = {}# 단어들에 순차적으로 2~ 501까지의 인덱스를 부여한다word_to_index = {word[0] : index + 2 for index, word in enumerate(vocab)}# 특수 인덱스word_to_index['unk'] = 0word_to_index['pad'] = 1encoded = []for review in res: tmp = [] for word in review: try: # 각 글자를 해당하는 정수로 변환한다. tmp.append(word_to_index[word]) except KeyError: # 단어 집합에 없는 단어일 경우(=빈도수 상위 500 이외의 단어) unk로 대체된다. tmp.append(word_to_index['unk']) encoded.append(tmp)# 기존의 리뷰가 성공적으로 encoding 되었는지 확인해보기print(encoded[:2]) 1[[294, 51, 6, 4, 89, 63, 86, 11, 21, 34], [0, 79, 0, 0, 0, 54, 0, 0, 0, 0, 48, 0, 0, 0, 19, 314, 136, 319, 26, 54, 0, 278, 169, 72, 0, 0, 0, 32, 0, 8, 36, 140, 32, 68, 383, 0, 4, 0, 22, 8, 123, 29, 320, 103]] Padding: 길이가 다른 문장들을 모두 동일한 크기로 바꿔주는 작업 인코딩한 리뷰를 모두 일정한 길이로 변환해준다. 특정 길이로 모든 샘플들의 길이를 맞춰준다. 정한 길이보다 짧은 샘플들에는 ‘pad’ 토큰을 추가하여 길이를 맞춰준다. 리뷰의 길이를 그래프로 출력하는 코드 12345678max_len = max(len(l) for l in encoded)print('리뷰의 최대 길이 : %d' % max_len)print('리뷰의 최소 길이 : %d' % min(len(l) for l in encoded))print('리뷰의 평균 길이 : %f' % (sum(map(len, encoded))/len(encoded)))plt.hist([len(s) for s in encoded], bins=50)plt.xlabel('length of sample')plt.ylabel('number of sample')plt.show() 12max_len = max(len(l) for l in encoded)print('리뷰의 최대 길이 : %d' % max_len) 12[결과]리뷰의 최대 길이 : 81 123456789# 리뷰 최대 길이인 81로 모든 리뷰의 길이를 맞춰준다.pad_len = 81for review in encoded: if len(review) &lt; pad_len: review += [word_to_index['pad']] * (pad_len - len(review)) # 기존의 리뷰가 성공적으로 padding 되었는지 보기print(encoded[0]) 12[결과][294, 51, 6, 4, 89, 63, 86, 11, 21, 34, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1]","link":"/2020/02/17/ai-study4/"},{"title":"[딥러닝 스터디] 순환신경망(RNN)","text":"다음의 책을 공부하며 정리한 내용입니다. https://wikidocs.net/22886 순환신경망(RNN : Recurrent Neural Network): RNN은 입력과 출력을 시퀀스 단위로 처리하는 시퀀스 모델이다. 이때의 입력은 처리하고자 하는 문장, 즉 단어 시퀀스이며, 출력은 처리된 문장 단어 시퀀스이다. RNN의 가장 큰 특징은 은닉층의 노드에서 나온 결과값이 출력층 과 은닉층 노드의 다음 계산을 위한 입력으로, 둘 모두로 보내진다는 점이다. xt : 입력층의 입력 벡터 yt : 출력층의 출력 벡터 cell : 은닉층에서 결과를 두 방향으로 내보내는(출력층 &amp; 다음연산) 노드. 메모리 셀 혹은 RNN셀이라고 표현한다. 이때 메모리 셀이 두 방향으로 내보내는 결과를 은닉상태(hidden state) 라고 한다. 피드포워드 신경망에서는 기본적으로 뉴런이라는 단위를 사용했지만, RNN에서는 입력층/출력층 -&gt; 입력벡터/출력벡터 은닉층 -&gt; 은닉상태 의 표현을 일반적으로 사용한다. 피드포워드 신경망과 같이 뉴런 단위로 RNN을 시각화할 경우 아래와 같이 표현할 수 있다. 입력벡터 차원(입력층의 뉴런 수) : 4 은닉상태 크기(은닉층의 뉴런 수) : 2 출력벡터 차원(출력층의 뉴런 수) : 2 시점(timestep) : 2 RNN의 활용: RNN은 입력과 출력의 길이가 고정되어 있지 않다. 즉, 설계에 따라 다양한 용도로 신경망을 사용할 수 있다. 일대다 모델 하나의 이미지 입력에 대해서 사진의 제목을 출력하는 이미지 캡셔닝(Image Captioning) 작업에 사용할 수 있다. 사진의 제목은 단어들의 나열이므로 시퀀스 출력이다. 다대일 모델 단어 시퀀스에 대해서 하나의 출력(many-to-one)을 하는 모델. 입력 문서가 긍정적인지 부정적인지를 판별하는 감성 분류(sentiment classification), 또는 메일이 정상 메일인지 스팸 메일인지 판별하는 스팸 메일 분류(spam detection)에 사용할 수 있다. 위 그림은 RNN으로 스팸 메일을 분류할 때의 아키텍처를 보여줍니다. 다대다 모델 다 대 다(many-to-many)의 모델의 경우에는 입력 문장으로 부터 대답 문장을 출력하는 챗봇과 입력 문장으로부터 번역된 문장을 출력하는 번역기, 개체명 인식이나 품사 태깅과 같은 작업이 속한다. 위 그림은 개체명 인식을 수행할 때의 RNN 아키텍처를 보여줍니다. RNN의 수식 ht : 현재시점 t에서의 은닉 상태값 wx : 입력층의 입력값에 대한 가중치 wt : 이전시점 t-1의 은닉상태값 ht-1 에 대한 가중치 wh 따라서 ht를 계산하는 수식은 다음과 같다. ht = activation_func((wh * ht-1) + (wx * xt) + b) 이때 활성화 함수는 일반적으로 tanh함수를 사용한다. ReLU를 사용하기도 한다. 출력층 값 yt는 아래와 같이 계산한다. yt = activation_func((wy * ht) + b) 이때 비선형 활성화 함수 중 하나를 activation func으로 사용한다. (실습) 파이썬으로 RNN 구현하기1234567891011121314151617181920import torchimport torch.nn as nn# 입력과 은닉상태의 크기를 정의한다.input_size = 5hidden_size = 8# 입력텐서(=입력벡터)를 정의한다. # (배치크기 * 시점의 수 * 입력크기)를 인자로 받는다.input_vec = torch.Tensor(1, 10, input_size)# nn.RNN()으로 RNN셀을 정의한다.# (입력크기 * 은닉상태 크기)를 인자로 받는다. batch_first=True는 입력텐서의 첫번째 차원이 배치크기임을 알려준다.cell = nn.RNN(input_size, hidden_size, batch_first=True)# 입력텐서를 RNN셀에 넣어 출력값의 크기를 확인해본다.# (모든 시점의 은닉상태들, 마지막 시점의 은닉상태)를 반환한다.outputs, final_output = cell(input_vec)print(outputs.shape)print(final_output.shape) 123[결과]torch.Size([1, 10, 8])torch.Size([1, 1, 8]) 다양한 순환신경망 깊은 순환신경망(Deep Recurrent Neural Network) : RNN역시 다수의 은닉층을 가질 수 있다. 2개 이상의 은닉층을 가진 RNN을 Deep RNN이라고 한다. 깊은 순환 신경망은 nn.RNN()의 인자로 num_layers 파라미터를 추가해줌으로서 구현할 수 있다. 12# (입력텐서 크기, 은닉층 크기, 은닉층 개수)cell = nn.RNN(input_size = 5, hidden_size = 8, num_layers = 2, batch_first=True) 이때 마지막 시점의 은닉상태는 다음과 같이 바뀐다. 1234print(final_output.shape) # (층의 개수, 배치 크기, 은닉 상태의 크기)&gt;&gt; torch.Size([2, 1, 8]) 양방향 순환신경망(Bidirectional Recurrent Neural Network) : 양방향 순환신경망은 특정 시점 t에서 출력값 ht를 예측할 때 이전시점의 데이터 ht-1뿐만 아니라 이후시점의 데이터로도 예측할 수 있다는 아이디어에서 출발한다. (예제) 12345Exercise is very effective at [ ] belly fat. 1) reducing2) increasing3) multiplying 정답 reducing을 찾기 위해서는 이전에 나온 단어와 이후에 나온 단어 모두를 참고\\해야 결정할 수 있다. 즉, 양방향 RNN은 이전 시점의 데이터뿐만 아니라, 이후 시점의 데이터도 힌트로 활용하기 위해서 고안된 모델이다. 양방향 순환 신경망은 하나의 출력값 ht를 예측하기 위해 두개의 메모리 셀을 사용 한다. 첫번째 메모리 셀은 앞 시점의 은닉상태를 전달받아 계산한다. 두번째 메모리 셀은 뒤 시점의 은닉상태를 전달받아 계산한다. 양방향 RNN도 다수의 은닉층을 가질 수 있다. nn.RNN()의 인자로 bidirectional값을 True로 전달하여 구현할 수 있다. 12# (입력텐서 크기, 은닉층 크기, 은닉층 개수, 양방향 여부)cell = nn.RNN(input_size = 5, hidden_size = 8, num_layers = 2, batch_first=True, bidirectional=True) 이때 마지막 시점의 은닉상태는 다음과 같이 바뀐다. 1234print(final_output.shape) # (층의 개수 * 2, 배치 크기, 은닉 상태의 크기)&gt;&gt; torch.Size([4, 1, 8])","link":"/2020/02/17/ai-study3/"},{"title":"[딥러닝 스터디] 자연어 전처리 실습","text":"자연어 처리 라이브러리인 토치텍스트를 활용해 이전 포스트의 전처리 이론을 실제로 구현해보자. 토치텍스트: 텍스트에 대한 여러 추상화 기능을 제공하는 자연어 처리 라이브러리 토치텍스트 제공 기능 파일 로드하기(File Loading) : 다양한 포맷의 코퍼스를 로드한다. 토큰화(Tokenization) : 문장을 단어 단위로 분리한다. 단어 집합 생성(Vocab) : 단어 집합을 만든다. 정수 인코딩(Integer encoding) : 전체 코퍼스의 단어들을 각각의 고유한 정수로 맵핑한다. 단어 벡터(Word Vector) : 단어 집합의 단어들에 고유한 임베딩 벡터를 만들어줍니다. 랜덤값으로 초기화한 값일 수도 있고, 사전 훈련된 임베딩 벡터들을 로드할 수도 있습니다. 배치화(Batching) : 훈련 샘플들의 배치를 만들어줍니다. 이 과정에서 패딩 작업(Padding)도 이루어집니다. 실습 1. IMDB 리뷰데이터 분류하기(영어)데이터 다운 및 용도에 따른 분류 진행(훈련/테스트) 데이터는 text(리뷰데이터)와 sentiment(리뷰의 긍정:1/부정:0 여부) 1234567891011121314151617181920# 토치텍스트 설치pip install torchtextimport urllib.requestimport pandas as pd# IMDB 리뷰 데이터 다운로드urllib.request.urlretrieve(\"https://raw.githubusercontent.com/LawrenceDuan/IMDb-Review-Analysis/master/IMDb_Reviews.csv\", filename=\"IMDb_Reviews.csv\")# 데이터 csv파일로 저장, 상위 5개 행 출력해본다df = pd.read_csv('IMDb_Reviews.csv', encoding='latin1')df.head()# 훈련 데이터와 테스트 데이터로 분류(총 5만개)train_df = df[:40000]test_df = df[40001:]# 각 데이터를 csv 파일로 저장 (index=False :: 인덱스를 저장하지 않음)train_df.to_csv(\"train_data.csv\", index=False)test_df.to_csv(\"test_data.csv\", index=False) 필드 정의하기(torchtext.data) torchtext.data 의 Field함수를 활용해 진행할 자연어 전처리를 정의할 수 있다. sequential : 순차적인 데이터 여부. (True가 기본값) LABEL은 긍정/부정의 단순한 클래스를 나타내는 숫자값이지 순차적 데이터가 아니므로 False이다. use_vocab : 단어 집합을 만들 것인지 여부. (True가 기본값) tokenize : 어떤 토큰화 함수를 사용할 것인지 지정. (string.split이 기본값) lower : 영어 데이터를 전부 소문자화한다. (False가 기본값) batch_first : 신경망에 입력되는 텐서의 첫번째 차원값이 batch_size가 되도록 한다. (False가 기본값) is_target : 레이블 데이터 여부. (False가 기본값) fix_length : 최대 허용 길이. 이 길이에 맞춰서 패딩 작업(Padding)이 진행된다. 12345678910from torchtext.data import TabularDataset# 경로, 훈련데이터, 테스트데이터, 데이터포멧, 텍스트객체train_data, test_data = TabularDataset.splits( path='.', train='train_data.csv', test='test_data.csv', format='csv', fields=[('text', TEXT), ('label', LABEL)], skip_header=True)# 훈련 데이터의 샘플을 확인한다.print(vars(train_data[1])['text'])print(vars(train_data[1])['label']) 123[결과]['believe', 'it', 'or', 'not,', 'this', 'was', 'at', 'one', 'time', 'the', 'worst', 'movie', 'i', 'had', 'ever', 'seen.', 'since', 'that', 'time,', 'i', 'have', 'seen', 'many', 'more', 'movies', 'that', 'are', 'worse', '(how', 'is', 'it', 'possible??)', 'therefore,', 'to', 'be', 'fair,', 'i', 'had', 'to', 'give', 'this', 'movie', 'a', '2', 'out', 'of', '10.', 'but', 'it', 'was', 'a', 'tough', 'call.']0 단어집합을 생성한다. 전체 리뷰의 단어들 내에서 중복을 제거한 단어집합을 생성한다. 각 단어에 고유한 정수를 부여한다(정수 인코딩) 정의한 필드 객체의 .build_vocab() 함수를 활용해 단어집합을 생성할 수 있다. 1234567# 리뷰 데이터의 단어집합을 만든다.# min_freq : 단어집합에 추가되기 위한 최소 등장빈도조건# max_size : 단어집합의 최대 크기TEXT.build_vocab(train_data, min_freq=10, max_size=10000)print('Size of vocab : ', len(TEXT.vocab))print('Integer index of word [the] : ', TEXT.vocab.stoi['the']) 123[결과]Size of vocab : 10002Integer index of word [the] : 2 이때 단어집합의 크기는 기존에 정의한 10000이 아닌 10002임을 알 수 있다. 더해진 두개는 토치텍스트가 자동으로 추가한 특별토큰 unk\\와 pad\\이다. unk는 0, pad는 1의 정수가 부여된다. (+) 데이터로더 만들기 : 특정 배치크기로 데이터를 로드하도록 하는 데이터 로더를 만든다. torchtext.data의 Iterator를 사용해 만들 수 있다. 12345from torchtext.data import Iterator# 배치사이즈 100으로 훈련데이터에 대한 데이터로더를 만든다train_loader = Iterator(dataset=train_data, batch_size=100)print(\"len of train data : \", len(train_data), \" | num of batches : \", len(train_loader)) 12[결과]len of train data : 40000 | num of batches : 400 실습 2. 네이버 영화데이터 분류하기(한국어): 이전 IMDB데이터를 토치텍스트로 전처리 한 것과 마찬가지의 과정으로 진행한다. 네이버 영화리뷰데이터 다운 훈련데이터/테스트 데이터로 분류 필드 정의하기(전처리 방식 지정) 데이터셋 제작하기(전처리 수행) 단어집합 생성/정수인코딩 수행 1234567891011121314151617181920212223242526272829303132333435363738394041424344454647# 한국어 형태소 분석기 Mecab 설치!git clone https://github.com/SOMJANG/Mecab-ko-for-Google-Colab.git%cd Mecab-ko-for-Google-Colab!bash install_mecab-ko_on_colab190912.sh# 네이버 영화 리뷰데이터 다운import urllib.requestimport pandas as pdurllib.request.urlretrieve(\"https://raw.githubusercontent.com/e9t/nsmc/master/ratings_train.txt\", filename=\"ratings_train.txt\")urllib.request.urlretrieve(\"https://raw.githubusercontent.com/e9t/nsmc/master/ratings_test.txt\", filename=\"ratings_test.txt\")# 훈련 데이터와 테스트 데이터로 분리한다.train_data = pd.read_table('ratings_train.txt')test_data = pd.read_table('ratings_test.txt')from torchtext import datafrom konlpy.tag import Mecab# 필드 정의하기(전처리 방식 지정)TEXT = data.Field(sequential=True, use_vocab=True, # 단어집합을 만든다 tokenize=Mecab().morphs, # 토크나이저로는 Mecab 사용. lower=True, batch_first=True, fix_length=20) # 패딩 길이는 20LABEL = data.Field(sequential=False, use_vocab=False, is_target=True)# ID필드는 사용하지 않는다. # 디민 TabularDataset.splits()은 받은 데이터를 앞에서부터 순서대로 자르므로 필요함.# 네이버 영화리뷰 데이터는 [리뷰아이디, 리뷰, 라벨] 세가지로 이뤄져있기 때문ID = data.Field(sequential=False, use_vocab=False,) from torchtext.data import TabularDataset# 데이터셋 제작하기(전처리 수행)train_data, test_data = TabularDataset.splits( path='.', train='ratings_train.txt', test='ratings_test.txt', format='tsv', fields=[('id', ID), ('text', TEXT), ('label', LABEL)], skip_header=True)print('훈련 샘플의 개수 : {}'.format(len(train_data)))print('테스트 샘플의 개수 : {}'.format(len(test_data)))print('훈련 데이터 예제 : {}'.format(vars(train_data[0]))) 1234[결과]훈련 샘플의 개수 : 150000테스트 샘플의 개수 : 50000훈련 데이터 예제 : {'id': '9976970', 'text': ['아', '더', '빙', '.', '.', '진짜', '짜증', '나', '네요', '목소리'], 'label': '0'} 12345# 단어집합을 생성한다(정수인코딩 수행)TEXT.build_vocab(train_data, min_freq=10, max_size=10000)# 생성된 단어집합 내 단어 확인해보기print('단어 \"좋아\"의 인덱스는 [{}]'.format(TEXT.vocab.stoi['좋아'])) 12[결과]단어 &quot;좋아&quot;의 인덱스는 [8343]","link":"/2020/02/17/ai-study5/"},{"title":"GithubPage 세팅하기","text":"온갖 블로그 사이트를 전전하다가 드디어 뭔가 쌈박한 친구를 발견했다…드디어 나는 정착할 블로그를 찾은 것인가..?! 참고한 사이트 https://pages.github.com/ : GithubPage Official Guide https://www.holaxprogramming.com/2017/04/16/github-page-and-hexo/ : GithubPage와 Hexo 설치하기 https://blog.zhangruipeng.me/hexo-theme-icarus/ : Hexo Official Guide Blog https://github.com/ppoffice/hexo-theme-icarus : Hexo Official Github https://alleyful.github.io/categories/Tools/Hexo/ : Hexo 설치하기 https://guides.github.com/features/mastering-markdown/ : Markdown 마스터하기 https://swtpumpkin.github.io/git/hexo/hexoImg/ : Hexo 이미지 올리기 https://mishka.kr/2019/06/10/hexo-writing/ : Hexo 글쓰기 Default GuideHexo 명령어 새 테마 적용: 새 테마 적용시에 일단 한번 클린 후 deploy 12hexo cleanhexo deploy --generate 새 글쓰기: https://mishka.kr/2019/06/10/hexo-writing/ 링크를 꼭 참고한다. 123456# layout은 post(default), draft, page가 있다.hexo new [layout] &lt;post_name&gt;hexo deploy --generate# draft로 작성시 publish 명령어 사용hexo publish [layout] &lt;post_name&gt; submodule update를 하는경우 theme폴더 내의 _config.yml파일이 지워지는 문제가 있다. 해당 파일은 항상 백업해두어야 한다ㅠ 로컬 서버 확인 1hexo s Serch Engine Optimization깃헙 페이지는 기본적으로 검색이 안되므로.. 검색엔진 최적화 작업을 따로 해주어야 구글/네이버 등 검색엔진에서 보일 수 있다. Hexo 검색엔진 최적화를 위해 참고한 사이트https://alleyful.github.io/2019/08/10/tools/hexo/hexo-guide-03/https://jeyolog.github.io/2018/08/02/hexo-검색엔진-최적화-플로그인/ 블로그 꾸미기 로고 만들기 테마폴더의 이미지 파일들을 대체해준다. icarus테마 기준 \\themes\\icarus\\source\\images 내의 favicon, logo 등의 이미지를 변경해준다. https://logohub.io/ : 로고 제작 사이트 https://www.aconvert.com/image/png-to-svg/ : png파일 svg로 변환 변경한 이미지는 hexo clean후 배포해주어야 적용된다(테마 적용하듯이 적용!) 기타 팁 생각보다 시간이 오래 걸린다. 세팅하고 익숙해지는데에 거의 반나절이 걸렸다. hexo deploy로 새 글을 발행하는 경우 내 깃헙페이지를 관리하는 실제 레포지토리에는 완성된 글/글과 관련된 블로그 파일들만 올라간다. 여튼 무슨소리냐면 내가 로컬에서 글 쓰는 환경 그 자체는 업로드되지 않는다는 얘기다. 따라서 내가 글쓰는 환경은 따로 백업을 해줘야하는데 이게 또 글 쓰는 폴더 내의 themes 폴더는 다 별도의 깃헙 레포라서 그냥 통으로 올리면 제대로 백업이 안된다. https://mishka.kr/2019/06/13/backup/ 이분이 굉장히 잘 설명해주심. 내 원격 레포를 두개 만든다. 하나는 블로그 쓰는환경 전체파일 백업용(레포1), 다른 하나는 테마 폴더만 백업용(레포2) 테마 폴더의 원격 저장소 위치를 내 원격레포1로 변경해두고 커밋 테마 폴더를 지우고 나머지 블로그 글쓰는 환경 폴더를 깃 레포로 만든다. 테마 폴더를 submodule로 현재 로컬 레포에 추가한다. 블로그 전체 글쓰는 환경 폴더를 원격레포2로 전체 커밋 이후 테마가 추가되면 git submodule add로 추가해준다. 테마 관련 설정이 변경돼도 이렇게 해주면 될 듯. Warning 로컬 테마폴더의 깃 레포 url을 바꾸지 않으면 기존 icarus 레포에 푸시를 하게 된다!!ㅋㅋ.. 대담한 한국인이 되고싶다면 시도해볼것.. 마크다운 에디터가 있으면 편할 것 같아서 또 서치를 했다. Typora 로 현재 작성중. 가볍고 심플하니 나쁘지 않다. Welcome to Hexo! This is your very first post. Check documentation for more info. If you get any problems when using Hexo, you can find the answer in troubleshooting or you can ask me on GitHub. Quick StartCreate a new post1$ hexo new \"My New Post\" More info: Writing Run server1$ hexo server More info: Server Generate static files1$ hexo generate More info: Generating Deploy to remote sites1$ hexo deploy More info: Deployment","link":"/2020/02/14/how-to-start-githubPage/"},{"title":"YAMLException: can not read a block mapping entry, Hexo","text":"How to solve problem YAMLException: can not read a block mapping entry 포스팅을 하면서 다음과 같은 에러가 발생했다. 이는 아래와 같이 Hexo로 신규 게시물 작성 시 title에 [] 대괄호를 사용하면 발생하는 에러이다. (,나 :같은 기호를 사용해도 발생한다.) The error above occurs because of the [] square brackets used in a title of your post. 따라서 위의 타이틀을 “” 로 묶어주면 에러를 해결할 수 있다. Just add a title with double quotation marks to solve the problem. 1title : \"[딥러닝 스터디] 임베딩이란\"","link":"/2020/02/16/yaml-exception/"},{"title":"[딥러닝 스터디] 케라스(Keras) 실습","text":"다음의 책을 공부하며 정리한 내용입니다. https://wikidocs.net/48649 https://wikidocs.net/32105 케라스 공식문서 딥러닝 라이브러리 케라스의 사용법을 익히고 실제 RNN모델을 설계해본다. 케라스는 딥러닝을 도와주는 파이썬 라이브러리이다. 케라스 훑어보기전처리 도구Tokenizer글자 단위 RNN(Char RNN): 입출력의 단위가 글자인 RNN을 케라스로 구현하여 언어모델의 훈련/테스트 과정을 이해한다.","link":"/2020/02/19/ai-study6/"},{"title":"맥 Permission denied writing to file","text":"맥은 가끔보면 편한건지 불편한건지 모르겠다. 방금도 VSCode로 프로젝트 실습하다가 안되길래 개빡쳐서 알아보고 쓰는글임. Permission denied writing to file 오류가 떴다. 아니 내가 만든 프로젝트를 내가 편집하겠다는데 왜 안돼 왜… 왜…. why… https://support.apple.com/ko-kr/guide/mac-help/mchlp1203/mac 해결방법은 위의 링크에. 폴더 컨텍스트 메뉴에서 정보 가져오기 누르면 위의 메뉴가 뜬다. 오른쪽 아래 자물쇠 잠금해제 권한 변경 는 안됨. 응 안돼 돌아가^^ 아 Gae BBAK Chin da.. 결국 그냥 https://medium.com/@AnkitMaheshwariIn/mac-vs-code-error-permission-denied-writing-to-file-bb112180ede 터미널에서 해당 디렉터리에 대해 권한 풀어준다. sudo chmod -R 777","link":"/2020/02/24/change-permission/"},{"title":"[딥러닝 스터디] Attention을 활용한 기계번역","text":"다음의 책을 공부하며 정리한 내용입니다. https://wikidocs.net/24996 : seq2seq 정리 https://wikidocs.net/22893 : 어텐션 모델 정리 https://www.youtube.com/watch?v=c8y9ZAb9aks&amp;t=1032s : seq2seq에서 attention까지(매우 좋음, 꼭 참고하세요) https://www.tensorflow.org/tutorials/text/nmt_with_attention : attention 실습 시퀀스-투-시퀀스(seq2seq): 입력된 시퀀스로부터 다른 도메인의 시퀀스를 출력하는 모델. 이는 다음과 같은 분야에서 사용된다. 챗봇: 입력시퀀스와 출력시퀀스를 각각 질문/대답으로 구성하면 챗봇을 만들 수 있다. 기계번역: 입력시퀀스와 출력시퀀스를 입력/번역문장으로 구성하면 번역기를 만들 수 있다. Text Summerization, Speech to Text 등에 사용될 수 있다. seq2seq 모델은 기본적으로 위의 구조를 띄고 있다. 인코더와 디코더는 두개의 RNN 아키텍처이다. 입력 문장을 처리하는 RNN셀을 인코더, 출력 문장(번역된 문장)을 처리하는 RNN셀을 디코더라고 하는 것. 중간의 컨텍스트 벡터는 인코더 마지막 시점의 히든 스테이트의 크기이다. 즉, 이전의 내용이 함축된 하나의 벡터이다. 이때 디코더에서 단순히 매 단계마다 가장 가능성이 높은 단어 하나를 선택하는 방식은 생각보다 효율적이지 않다. 이때 적용하는 방법이 Beam search. Beam search : 매 스텝마다 가장 확률이 높은 n개의 단어를 선택하여, 이 N개의 단어 각각에 대해 다음 스텝에서 등장할 수 있는 모든 단어들의 확률을 예측한다. 이러한 방식으로 매 스텝마다 n개의 후보군을 유지하여 최적의 시퀀스 후보를 뽑아낸다. Attention model seq2seq 모델은 기본적으로 인코더 -&gt; [컨텍스트 벡터] -&gt; 디코더 의 구조를 갖는다. 이러한 모델의 문제는 아래와 같다 컨텍스트 벡터는 결국 하나의 벡터에 불과하다. 인코더의 모든 정보를 하나의 고정된 크기의 벡터에 압축하다 보면 필연적으로 정보의 손실이 발생하게 된다. RNN의 고질적인 Vanishing gradient 문제가 발생한다. : 이러한 문제는 결과적으로 입력 시퀀스가 길어질수록 번역의 품질이 저하되는 문제를 야기한다. Attention Overview어텐션 모델의 기본 아이디어는 다음과 같다. 디코더의 매 time step마다 인코더에서의 전체 입력문장을 다시 한번 참고한다. 이때 입력 문장의 전체 토큰을 동일한 비중으로 참고하는 것이 아니라, 해당 시점에서 예측해야할 단어와 연관이 있는 입력토큰 부분을 좀더 집중(attention)해서 참고한다. Dot product attention어텐션은 다양한 종류가 있다. 그 중 가장 기본적인 닷 프로덕트 어텐션의 구조를 살펴보자. 먼저 기본적인 용어 정의, seq2seq모델과 어텐션 모델의 차이점을 알아보자. 1, 2, 3… n : 인코더의 시점 h1, h2, h3… hn : 각 시점에서의 인코더의 은닉 상태(hidden state) t : 디코더의 현재시점 st : 현재 시점에서의 디코더의 은닉 상태 이전에 배웠던 seq2seq에서 디코더는 두개의 값(이전시점의 은닉상태, 이전시점의 출력)을 통해 현재시점의 은닉상태를 계산했다. 이때 어텐션 모델에서는 계산을 위해 필요한 값이 하나 더 추가된다. 바로 t시점의 어텐션 값 at이다. 따라서 어텐션 모델은 (seq2seq + 어텐션 값 계산) 인 모델이다. 즉, 어텐션 모델에서의 핵심은 이 어텐션 값을 어떻게 구하는가이며, 이 과정에서 어텐션 스코어값을 구하는 방법에 따라 닷 프로덕트 어텐션, 루옹 어텐션, 바다나우 어텐션 등으로 종류가 나뉘게 된다. 또한 모든 어텐션 값 at는(“값”이라는 명칭에서 예상할 수 있듯) 스칼라 값이다. Step 1. 어텐션 스코어를 구한다. 어텐션 스코어는 다음을 의미한다. Attention score : 인코더의 각 은닉상태 h1 - hn이 현재 시점의 디코더 은닉상태 st와 얼마나 유사한지의 정도 닷 프로덕트 어텐션에서는 이 스코어 값을 구하기 위해 st와 hi(i : 1~n)를 닷 프로덕트 한다. 이때 둘다 열벡터이므로 디코더의 은닉상태 st값을 전치하여 내적한다. 따라서 dot product attention의 attention score 함수 수식은 다음과 같다. score(st, hi) = stT * hi 따라서 디코더의 현재 시점 t에 대한 은닉상태 st와 인코더의 모든 시점에 대한 은닉상태의 어텐션 스코어의 모음(et)은 아래와 같다. et = [stT * h1, …, stT * hN] Step 2. Softmax를 통해 Attention Distribution(어텐션 분포)를 구한다. 구해낸 모든 어텐션 스코어의 모음, et에 softmax를 적용해 확률 분포를 얻어낸다. 이를 통해 얻어낸 분포를 Attention Distribution(어텐션 분포)라 하며, 각각의 값을 Attention Weight(어텐션 가중치)라고 한다. 어텐션 분포와 어텐션 값은 디코더의 현재시점 t에 대해 정의된다. Attention Distribution : 어텐션 스코어의 모음에 softmax를 적용해 얻어낸 확률분포. 어텐션 가중치의 모음값 Attention Weight : 어텐션 분포의 각각의 값 따라서 어텐션 분포를 αt의 식은 다음과 같다. αt = softmax(et) Step 3. Attention Weight과 인코더 은닉상태를 가중합하여 Attention Value를 구한다. 구해낸 Attention Distribution의 각 Attention Weight들을 해당 시점의 인코더 은닉상태(h1 ~ hN)와 가중합(Weighted sum) 한다. 결과로 나오는 벡터 a는 최종 어텐션 값, 즉 Attention Value가 되며 이는 인코더의 문맥을 내포하고 있다는 의미에서 Context Vector라고 부르기도 한다. a = ∑αti * hi (i : 1~N, ati : i번째 어텐션 분포의 값) Step 4. 어텐션 값과 현재 상태 대코더의 은닉상태 st를 연결한다(Concatenation). 최종적으로 구해낸 어텐션 값(컨텍스트 벡터) a를 현재 시점의 디코더 은닉상태 st와 결합한다. 이때 둘을 연결해 하나의 벡터로 만드는(concatenation) 작업을 수행한다. 해당 결합 작업을 통해 산출된 최종 벡터 vt는 t시점의 디코더 예측값 y_hat를 도출하기 위한 연산의 입력값으로 사용된다. Neural machine translation with attention 스페인어에서 영어로 기계번역을 수행하는 seq2seq 모델을 직접 구현해본다. 데이터셋 다운로드","link":"/2020/02/23/ai-study7/"},{"title":"[실전 JSP] 오리엔테이션 및 소개","text":"해당 포스트는 인프런의 강좌 실전 JSP renew 를 공부하며 작성하였습니다. 웹 프로그램 개요: 웹 프로그램이란 인터넷 서비스를 이용해서 서로 다른 구성요소들이 통신할 수 있는 프로그램이다. 가장 간단한 흐름의 형태는 사용자가 웹서버에 리퀘스트를 날리고, 이를 웹서버가 처리하여 다시 요청한 곳으로 응답해주는 것이다. 이떄 웹서버가 처리하는 로직을 개발하는 것이 웹 프로그램 개발이라고 할 수 있다. 네트워크 프로토콜: 통신을 위한 규약으로, HTTP, FTP, SMTP, POP 등이 있다. 도메인의 구조 http(protocol) :// www(인터넷 서비스구분) . google.com(도메인) : 80(port) / index.html(경로) 웹 프로그램의 동작 원리 웹서버와 사용자간의 통신에는 html 사용 사용자 데이터 및 요청 처리/가공은 동적 데이터인 컨테이너에서 처리한다. 개발 환경 설정다음의 목록을 설치한다. JDK Eclipse(본인은 Intellij 사용) Apache Tomcat 8.5 설치(웹 컨테이너) : 최신버전은 더 높지만 안정화된 8.5버전을 많이 사용한다..","link":"/2020/03/10/jsp-study/"},{"title":"Introduction to Nuxt.js","text":"Nuxt.js is a framework for creating Vue.js application. There are some awesome features that nuxt.js provides. SEO with Server Side Rendering (SSR) Pre Rendering Code Splitting 이런 기능을 서비스에 직접 구현해낼 필요 없이 Nuxt를 사용함으로서 benefit을 얻을 수 있다. What is Nuxt.js?Server Side RenderingBenefits Search Engine Optimization(검색엔진 최적화) Meta Tags Performance Why SSR? One of the common problems javascript developers have is that it is hard to deal with SEO and Meta Tags. : 처음 fetch로 데이터 로드할때 페이지는 비어있는 상태(The page is empty on the initial load of something like the content and meta titles). 뿐만 아니라 자바스크립트가 실행되면 there is no content to index or parse. =&gt; 따라서 많은 크롤링 시스템은 자바스크립트를 지원하지 않는다. 즉, 자바스크립트로 만든 페이지는 실제 제목이 아닌 title of undefined로 검색엔진에 보이는 것. SSR key ideas: 이에 대한 해결책으로 SSR이 등장하게 된다. SSR의 아이디어는 다음과 같다. 실행중인 서버가 있으며, 이는 html응답을 생성(create the html response) 한다. 생성된 응답은 클라이언트나 크롤링 시스템에 사용된다(serve it to the client or the crawler) 따라서 API콜은 서버에서 실행되며,(the call to the API would take place on the server) 실행이 완료되면 메타데이터와 최종 페이지가 생성된다(the meta data will be set and the final page will be served). 최종적으로 크롤링 시스템이 필요로 하는 SEO정보와 메타테그는 크롤링 되는 시점에 페이지 내에 존재하게 된다. 뿐만 아니라 페이지를 서버에서 렌더링 함으로서 속도적인 이득을 볼 수 있다. Pre Rendering: Pre rendering 기법을 통해 서버가 페이지의 생성과 배포를 담당하도록 하는 대신 페이지가 먼저 생성되게 된다(?).(Instead of having a server to generate and serve a page, the pages are generated upfront) 따라서 배포 폴더는 각 페이지에 대해 하나의 html파일을 갖게 된다. 따라서 트래픽이 많은 서비스에 대해서도 무료로 호스팅이 가능하다(SSR benefits + free hosting). Code Splitting: Nuxt는 어플리케이션이 code-splitting 되도록 한다. Code splitting : 자바스크립트 코드를 multiple files로 split하기위한 테크닉. =&gt; 서비스의 비용 절감, 속도 향상 예시 서비스 전체에 총 100개의 컴포넌트가 있고, 메인 페이지에서는 10개의 컴포넌트만 사용하는 경우 : 메인 페이지에서 사용되는 자바스크립트 파일(or 번들)은 사용되지 않는 컴포넌트까지 포함하고 있을 필요가 없다. Nuxt는 자동적으로 각 페이지에 대한 자바스크립트 파일을 생성하여 프로젝트의 의존관계를 관리한다(take care of the project’s dependencies). Create Nuxt App1npx create-nuxt-app nuxt-fundamentals 서버사이드 렌더링을 사용할 것이므로 렌더링 모드만 Universal로 해주도록 한다. Official scaffolding tool create-nuxt-app 사용한다. npx는 npm 5.2.0버전부터 기본적으로 제공된다. 맥을 사용한다면 명령어 앞에 sudo를 사용해줘야 한다(플젝 돌릴때도 마찬가지) 설치 완료됐으면 프로젝트를 실행시켜보자. 프로젝트 폴더로 들어가서 npm혹은 yarn명령어 실행 1npm run dev Guided Nuxt.js Project Tour: In this lesson, we’ll show you around in our newly created Nuxt project. Please note that each directory includes a readme file, that explains what the directory is. You can safely delete the directories of the features you do not need. 폴더명 역할 assets - 아직 컴파일되지 않은 asset들을 포함한다(SASS, 이미지, 폰트 등). - vue cli 프로젝트의 asset 디렉터리와 같은 역할을 한다. components - vue.js 컴포넌트들이 위치하는 디렉터리. layouts - 말 그대로 페이지의 레이아웃을 저장하는 디렉터리.- 기본 레이아웃, 갤러리 레이아웃 같이 원하는 커스텀 레이아웃 저장. middleware - 미들웨어를 저장하는 디렉터리.- 페이지가 렌더링되기 전에 실행가능한 함수 등을 정의할 수 있다. pages - Contains your application views and routes- nuxt는 이 디렉터리에 있는 모든 vue파일을 읽어 자동으로 application router를 생성한다. plugins - 자바스크립트 플러그인을 포함한다.- 해당 플러그인들은 instantiating the route vue instance하기 전에 실행된다.- 전역 컴포넌트를 등록하거나 상수/함수를 삽입하는 위치이기도 하다(register the components globally or to inject functions or constants). static - static 파일을 저장한다. 해당 파일들은 자동으로 서버의 루트위치에 저장된다(automatically mapped to the server’s root).- 해당 위치에 저장된 파일은 웹사이트 url/파일명 으로 접근이 가능하다. store - Contains your Vuex Store- Vuex Store는 별도의 설치나 구성 없이 Nuxt에서 바로 사용할 수 있지만(out of the box), 디폴트로 disable되어있다. 각 디렉터리의 Readme.md 파일에는 해당 디렉터리의 기능이 소개되어있다. 필요없는 기능의 디렉터리는 걍 지우면 됨.","link":"/2020/02/23/nuxt-study1/"},{"title":"Working with Nuxt.js","text":"본격적으로 Nuxt.js를 활용하는 방법에 대해 배워보도록 하자. Customize the home pagenpm으로 이전 포스트의 프로젝트를 실행하면 보이는 첫 페이지. 이때 계속 말했듯 맥북은 명령어 앞에 sudo를 붙여줘야 한다. 귀찮아 죽겠네. : 위의 화면은 /pages/index.vue 파일의 내용임. 디폴트로 지역 컴포넌트, 링크, heading등을 렌더링한다. Nuxt.js Page ComponentsNuxt application에서 페이지를 만들기 위해서는 페이지 디렉터리 내에 컴포넌트를 만들면 된다. =&gt; Nuxt will automatically create the route 만들어진 Nuxt.js 앱의 소스를 확인해보면 default layout을 쓰고 있음을 알 수 있다. Default layout은 vue-cli 프로젝트의 App.vue랑 비슷한데, 마찬가지로 top-level component이다. Default layout에 적용된 스타일은 해당 레이아웃을 사용하는 모든 페이지에 일괄적으로 적용된다. 이러한 전역 스타일을 사용하는 것은 기본 폰트설정, 표 등 몇가지 경우를 제외하고는 지양해야 한다. Network 탭에서 해당 페이지를 보여주기 위해 로드된 js번들을 확인할 수 있다. 메인 페이지에서 post.vue 와 관련된 번들은 로드되지 않았다. 이것이 바로 이전 포스트에서 얘기한 Code Splitting","link":"/2020/02/24/nuxt-study2/"}],"tags":[{"name":"pytorch","slug":"pytorch","link":"/tags/pytorch/"},{"name":"딥러닝기초","slug":"딥러닝기초","link":"/tags/%EB%94%A5%EB%9F%AC%EB%8B%9D%EA%B8%B0%EC%B4%88/"},{"name":"선형회귀","slug":"선형회귀","link":"/tags/%EC%84%A0%ED%98%95%ED%9A%8C%EA%B7%80/"},{"name":"linear regression","slug":"linear-regression","link":"/tags/linear-regression/"},{"name":"로지스틱 회귀","slug":"로지스틱-회귀","link":"/tags/%EB%A1%9C%EC%A7%80%EC%8A%A4%ED%8B%B1-%ED%9A%8C%EA%B7%80/"},{"name":"logistic regression","slug":"logistic-regression","link":"/tags/logistic-regression/"},{"name":"자연어처리","slug":"자연어처리","link":"/tags/%EC%9E%90%EC%97%B0%EC%96%B4%EC%B2%98%EB%A6%AC/"},{"name":"딥러닝","slug":"딥러닝","link":"/tags/%EB%94%A5%EB%9F%AC%EB%8B%9D/"},{"name":"언어모델","slug":"언어모델","link":"/tags/%EC%96%B8%EC%96%B4%EB%AA%A8%EB%8D%B8/"},{"name":"한국어 임베딩","slug":"한국어-임베딩","link":"/tags/%ED%95%9C%EA%B5%AD%EC%96%B4-%EC%9E%84%EB%B2%A0%EB%94%A9/"},{"name":"자연어 전처리","slug":"자연어-전처리","link":"/tags/%EC%9E%90%EC%97%B0%EC%96%B4-%EC%A0%84%EC%B2%98%EB%A6%AC/"},{"name":"RNN","slug":"RNN","link":"/tags/RNN/"},{"name":"순환신경망","slug":"순환신경망","link":"/tags/%EC%88%9C%ED%99%98%EC%8B%A0%EA%B2%BD%EB%A7%9D/"},{"name":"torchtext","slug":"torchtext","link":"/tags/torchtext/"},{"name":"githubpage","slug":"githubpage","link":"/tags/githubpage/"},{"name":"hexo","slug":"hexo","link":"/tags/hexo/"},{"name":"YAMLException","slug":"YAMLException","link":"/tags/YAMLException/"},{"name":"Hexo","slug":"Hexo","link":"/tags/Hexo/"},{"name":"mac","slug":"mac","link":"/tags/mac/"},{"name":"change permission","slug":"change-permission","link":"/tags/change-permission/"},{"name":"folder permission","slug":"folder-permission","link":"/tags/folder-permission/"},{"name":"케라스","slug":"케라스","link":"/tags/%EC%BC%80%EB%9D%BC%EC%8A%A4/"},{"name":"Attention","slug":"Attention","link":"/tags/Attention/"},{"name":"jsp","slug":"jsp","link":"/tags/jsp/"},{"name":"inflearn","slug":"inflearn","link":"/tags/inflearn/"},{"name":"nuxtjs","slug":"nuxtjs","link":"/tags/nuxtjs/"},{"name":"vuejs","slug":"vuejs","link":"/tags/vuejs/"},{"name":"vue","slug":"vue","link":"/tags/vue/"},{"name":"javascript","slug":"javascript","link":"/tags/javascript/"},{"name":"vueschool.io","slug":"vueschool-io","link":"/tags/vueschool-io/"},{"name":"tensorflow","slug":"tensorflow","link":"/tags/tensorflow/"},{"name":"Keras","slug":"Keras","link":"/tags/Keras/"}],"categories":[{"name":"개발자 공부","slug":"개발자-공부","link":"/categories/%EA%B0%9C%EB%B0%9C%EC%9E%90-%EA%B3%B5%EB%B6%80/"},{"name":"인공지능","slug":"개발자-공부/인공지능","link":"/categories/%EA%B0%9C%EB%B0%9C%EC%9E%90-%EA%B3%B5%EB%B6%80/%EC%9D%B8%EA%B3%B5%EC%A7%80%EB%8A%A5/"},{"name":"그냥 그런 일상","slug":"그냥-그런-일상","link":"/categories/%EA%B7%B8%EB%83%A5-%EA%B7%B8%EB%9F%B0-%EC%9D%BC%EC%83%81/"},{"name":"끄적끄적","slug":"그냥-그런-일상/끄적끄적","link":"/categories/%EA%B7%B8%EB%83%A5-%EA%B7%B8%EB%9F%B0-%EC%9D%BC%EC%83%81/%EB%81%84%EC%A0%81%EB%81%84%EC%A0%81/"},{"name":"그렇게 바보는 아님","slug":"그냥-그런-일상/그렇게-바보는-아님","link":"/categories/%EA%B7%B8%EB%83%A5-%EA%B7%B8%EB%9F%B0-%EC%9D%BC%EC%83%81/%EA%B7%B8%EB%A0%87%EA%B2%8C-%EB%B0%94%EB%B3%B4%EB%8A%94-%EC%95%84%EB%8B%98/"},{"name":"JSP","slug":"개발자-공부/JSP","link":"/categories/%EA%B0%9C%EB%B0%9C%EC%9E%90-%EA%B3%B5%EB%B6%80/JSP/"},{"name":"Javascript","slug":"개발자-공부/Javascript","link":"/categories/%EA%B0%9C%EB%B0%9C%EC%9E%90-%EA%B3%B5%EB%B6%80/Javascript/"},{"name":"Nuxt.js","slug":"개발자-공부/Javascript/Nuxt-js","link":"/categories/%EA%B0%9C%EB%B0%9C%EC%9E%90-%EA%B3%B5%EB%B6%80/Javascript/Nuxt-js/"}]}