<!doctype html>
<html lang="ko"><head><meta charset="utf-8"><meta name="viewport" content="width=device-width, initial-scale=1, maximum-scale=1"><meta name="naver-site-verification" content="071e98e503d41ec3f7c7a7e909eec970500c5657"><title>[딥러닝 스터디] Attention을 활용한 기계번역(실습) - Women&#039;s Techmaker</title><link rel="manifest" href="/manifest.json"><meta name="application-name" content="Women&#039;s Techmaker"><meta name="msapplication-TileImage" content="/img/favicon.svg"><meta name="apple-mobile-web-app-capable" content="yes"><meta name="apple-mobile-web-app-title" content="Women&#039;s Techmaker"><meta name="apple-mobile-web-app-status-bar-style" content="default"><meta name="description" content="텐서플로우 공식 가이드 중 Neural machine translation with attention 문서의 실습을 참고하였습니다. Neural machine translation with attention 스페인어에서 영어로 기계번역을 수행하는 seq2seq 모델을 직접 구현해본다."><meta property="og:type" content="blog"><meta property="og:title" content="[딥러닝 스터디] Attention을 활용한 기계번역(실습)"><meta property="og:url" content="https://katie0809.github.io/2020/03/23/ai-study8/"><meta property="og:site_name" content="Women&#039;s Techmaker"><meta property="og:description" content="텐서플로우 공식 가이드 중 Neural machine translation with attention 문서의 실습을 참고하였습니다. Neural machine translation with attention 스페인어에서 영어로 기계번역을 수행하는 seq2seq 모델을 직접 구현해본다."><meta property="og:locale" content="ko_KR"><meta property="og:image" content="https://katie0809.github.io/image/Elegant_Background-2.jpg"><meta property="article:published_time" content="2020-03-23T01:41:20.000Z"><meta property="article:modified_time" content="2021-11-04T02:12:41.745Z"><meta property="article:author" content="Kyungim Lee"><meta property="article:tag" content="pytorch"><meta property="article:tag" content="자연어처리"><meta property="article:tag" content="딥러닝"><meta property="article:tag" content="언어모델"><meta property="article:tag" content="한국어 임베딩"><meta property="article:tag" content="케라스"><meta property="article:tag" content="Attention"><meta property="twitter:card" content="summary"><meta property="twitter:image" content="/image/Elegant_Background-2.jpg"><script type="application/ld+json">{"@context":"https://schema.org","@type":"BlogPosting","mainEntityOfPage":{"@type":"WebPage","@id":"https://katie0809.github.io/2020/03/23/ai-study8/"},"headline":"[딥러닝 스터디] Attention을 활용한 기계번역(실습)","image":["https://katie0809.github.io/image/Elegant_Background-2.jpg"],"datePublished":"2020-03-23T01:41:20.000Z","dateModified":"2021-11-04T02:12:41.745Z","author":{"@type":"Person","name":"Kyungim Lee"},"publisher":{"@type":"Organization","name":"Women's Techmaker","logo":{"@type":"ImageObject","url":"https://katie0809.github.io/img/logo.svg"}},"description":"텐서플로우 공식 가이드 중 Neural machine translation with attention 문서의 실습을 참고하였습니다. Neural machine translation with attention 스페인어에서 영어로 기계번역을 수행하는 seq2seq 모델을 직접 구현해본다."}</script><link rel="canonical" href="https://katie0809.github.io/2020/03/23/ai-study8/"><link rel="icon" href="/img/favicon.svg"><link rel="stylesheet" href="https://use.fontawesome.com/releases/v5.15.2/css/all.css"><link rel="stylesheet" href="https://cdn.jsdelivr.net/npm/highlight.js@9.12.0/styles/vs2015.css"><link rel="stylesheet" href="https://fonts.googleapis.com/css2?family=Ubuntu:wght@400;600&amp;family=Source+Code+Pro"><link rel="stylesheet" href="/css/default.css"><style>body>.footer,body>.navbar,body>.section{opacity:0}</style><!--!--><script src="https://www.googletagmanager.com/gtag/js?id=G-ZPWK5T4901" async></script><script>window.dataLayer = window.dataLayer || [];
        function gtag(){dataLayer.push(arguments);}
        gtag('js', new Date());
    
        gtag('config', 'G-ZPWK5T4901');</script><style>.pace{-webkit-pointer-events:none;pointer-events:none;-webkit-user-select:none;-moz-user-select:none;user-select:none}.pace-inactive{display:none}.pace .pace-progress{background:#3273dc;position:fixed;z-index:2000;top:0;right:100%;width:100%;height:2px}</style><script src="https://cdn.jsdelivr.net/npm/pace-js@1.2.4/pace.min.js"></script><meta name="generator" content="Hexo 5.4.0"><link rel="alternate" href="/rss2.xml" title="Women's Techmaker" type="application/rss+xml">
</head><body class="is-2-column"><nav class="navbar navbar-main"><div class="container"><div class="navbar-brand justify-content-center"><a class="navbar-item navbar-logo" href="/"><img src="/img/logo.svg" alt="Women&#039;s Techmaker" height="28"></a></div><div class="navbar-menu"><div class="navbar-start"><a class="navbar-item" href="/">Home</a><a class="navbar-item" href="/archives">Archives</a><a class="navbar-item" href="/categories">Categories</a><a class="navbar-item" href="/tags">Tags</a><a class="navbar-item" href="/downloads">Downloads</a></div><div class="navbar-end"><a class="navbar-item" target="_blank" rel="external nofollow noopener noreferrer" title="Download on GitHub" href="https://github.com/ppoffice/hexo-theme-icarus"><i class="fab fa-github"></i></a><a class="navbar-item is-hidden-tablet catalogue" title="카탈로그" href="javascript:;"><i class="fas fa-list-ul"></i></a><a class="navbar-item search" title="검색" href="javascript:;"><i class="fas fa-search"></i></a></div></div></div></nav><section class="section"><div class="container"><div class="columns"><div class="column order-2 column-main is-8-tablet is-8-desktop is-8-widescreen"><div class="card"><div class="card-image"><span class="image is-7by3"><img class="fill" src="/image/Elegant_Background-2.jpg" alt="[딥러닝 스터디] Attention을 활용한 기계번역(실습)"></span></div><article class="card-content article" role="article"><div class="article-meta is-size-7 is-uppercase level is-mobile"><div class="level-left"><span class="level-item"><time datetime="2020-03-23T01:41:20.000Z" title="2020-3-23 10:41:20 ├F10: AM┤">2020-03-23</time>&nbsp;게시 됨</span><span class="level-item"><time datetime="2021-11-04T02:12:41.745Z" title="2021-11-4 11:12:41 ├F10: AM┤">2021-11-04</time>&nbsp;업데이트 됨</span><span class="level-item"><a class="link-muted" href="/categories/%EA%B0%9C%EB%B0%9C%EC%9E%90-%EA%B3%B5%EB%B6%80/">개발자 공부</a><span> / </span><a class="link-muted" href="/categories/%EA%B0%9C%EB%B0%9C%EC%9E%90-%EA%B3%B5%EB%B6%80/%EC%9D%B8%EA%B3%B5%EC%A7%80%EB%8A%A5/">인공지능</a></span><span class="level-item">18분안에 읽기 (약 2773 단어)</span></div></div><h1 class="title is-3 is-size-4-mobile">[딥러닝 스터디] Attention을 활용한 기계번역(실습)</h1><div class="content"><p>텐서플로우 공식 가이드 중 <a target="_blank" rel="external nofollow noopener noreferrer" href="https://www.tensorflow.org/tutorials/text/nmt_with_attention">Neural machine translation with attention</a> 문서의 실습을 참고하였습니다.</p>
<h1 id="Neural-machine-translation-with-attention"><a href="#Neural-machine-translation-with-attention" class="headerlink" title="Neural machine translation with attention"></a>Neural machine translation with attention</h1><ul>
<li>스페인어에서 영어로 기계번역을 수행하는 seq2seq 모델을 직접 구현해본다.</li>
</ul>
<span id="more"></span>

<p>기본적인 데이터의 처리 과정은 이전과 같다. 전처리 과정은 생략하고 실제 인코더-어텐션-디코더를 클래스 형태로 구현하는 부분의 코드를 분석해본다.</p>
<h2 id="Encoder"><a href="#Encoder" class="headerlink" title="Encoder"></a>Encoder</h2><h3 id="1-클래스-설계"><a href="#1-클래스-설계" class="headerlink" title="1. 클래스 설계"></a>1. 클래스 설계</h3><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br></pre></td><td class="code"><pre><span class="line"><span class="class"><span class="keyword">class</span> <span class="title">Encoder</span>(<span class="params">tf.keras.Model</span>):</span></span><br><span class="line">  <span class="function"><span class="keyword">def</span> <span class="title">__init__</span>(<span class="params">self, vocab_size, embedding_dim, enc_units, batch_sz</span>):</span></span><br><span class="line">    <span class="built_in">super</span>(Encoder, self).__init__()</span><br><span class="line">    self.batch_sz = batch_sz</span><br><span class="line">    self.enc_units = enc_units</span><br><span class="line">    self.embedding = tf.keras.layers.Embedding(vocab_size, embedding_dim)</span><br><span class="line">    self.gru = tf.keras.layers.GRU(self.enc_units,</span><br><span class="line">                                   return_sequences=<span class="literal">True</span>,</span><br><span class="line">                                   return_state=<span class="literal">True</span>,</span><br><span class="line">                                   recurrent_initializer=<span class="string">&#x27;glorot_uniform&#x27;</span>)</span><br><span class="line"></span><br><span class="line">  <span class="function"><span class="keyword">def</span> <span class="title">call</span>(<span class="params">self, x, hidden</span>):</span></span><br><span class="line">    x = self.embedding(x)</span><br><span class="line">    output, state = self.gru(x, initial_state = hidden)</span><br><span class="line">    <span class="keyword">return</span> output, state</span><br><span class="line"></span><br><span class="line">  <span class="function"><span class="keyword">def</span> <span class="title">initialize_hidden_state</span>(<span class="params">self</span>):</span></span><br><span class="line">    <span class="keyword">return</span> tf.zeros((self.batch_sz, self.enc_units))</span><br></pre></td></tr></table></figure>

<p>[기본 파이썬 문법]</p>
<ul>
<li>class Encoder(<code>tf.keras.Model</code>) : Encoder 클래스는 tf.keras.Model 클래스를 상속</li>
<li>__ init __ : 클래스 생성자. 객체 생성 시점에 자동 호출</li>
<li>super(Encoder, self).__ init __() : 부모 클래스 초기화. <a href="[https://hashcode.co.kr/questions/6419/python3-super%EC%99%80-supera-self%EC%9D%98-%EC%B0%A8%EC%9D%B4%EB%8A%94-%EB%AC%B4%EC%97%87%EC%9D%B8%EA%B0%80%EC%9A%94](https://hashcode.co.kr/questions/6419/python3-super와-supera-self의-차이는-무엇인가요)">super()와 super(A, self)의 차이점</a> 참고</li>
</ul>
<p>[코드 분석]</p>
<ul>
<li><p>tf.keras.layers.Embedding()</p>
<ul>
<li>단어를 밀집벡터로 만드는 케라스 함수. 임베딩 층을 만든다.</li>
<li><strong>( <code>샘플의 수(시퀀스 길이)</code>, <code>입력크기(단어집합 크기)</code>)</strong> 인 2D 텐서를 입력으로 받아 <strong>( <code>샘플의 수(시퀀스 길이)</code>, <code>입력크기(단어집합 크기)</code>, <code>임베딩 차원</code>)</strong> 인 3D 텐서를 반환한다.</li>
<li>호출 위한 기본 파라미터는<br>: <code>input_dim</code>(단어집합 크기), <code>output_dim</code>(임베딩 차원) 이다.</li>
</ul>
</li>
<li><p>tf.keras.layers.GRU()</p>
<figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br></pre></td><td class="code"><pre><span class="line">tf.compat.v1.keras.layers.GRU(</span><br><span class="line">    units, activation=<span class="string">&#x27;tanh&#x27;</span>, recurrent_activation=<span class="string">&#x27;hard_sigmoid&#x27;</span>, use_bias=<span class="literal">True</span>,</span><br><span class="line">    kernel_initializer=<span class="string">&#x27;glorot_uniform&#x27;</span>, recurrent_initializer=<span class="string">&#x27;orthogonal&#x27;</span>,</span><br><span class="line">    bias_initializer=<span class="string">&#x27;zeros&#x27;</span>, kernel_regularizer=<span class="literal">None</span>, recurrent_regularizer=<span class="literal">None</span>,</span><br><span class="line">    bias_regularizer=<span class="literal">None</span>, activity_regularizer=<span class="literal">None</span>, kernel_constraint=<span class="literal">None</span>,</span><br><span class="line">    recurrent_constraint=<span class="literal">None</span>, bias_constraint=<span class="literal">None</span>, dropout=<span class="number">0.0</span>,</span><br><span class="line">    recurrent_dropout=<span class="number">0.0</span>, implementation=<span class="number">1</span>, return_sequences=<span class="literal">False</span>,</span><br><span class="line">    return_state=<span class="literal">False</span>, go_backwards=<span class="literal">False</span>, stateful=<span class="literal">False</span>, unroll=<span class="literal">False</span>,</span><br><span class="line">    reset_after=<span class="literal">False</span>, **kwargs</span><br><span class="line">)</span><br></pre></td></tr></table></figure>

<ul>
<li><strong><code>units</code></strong>: Positive integer, dimensionality of the output space. 출력 텐서의 차원.</li>
<li><strong><code>return_sequences</code></strong>: Boolean. Whether to return the last output in the output sequence, or the full sequence. 모든 시점의 output을 출력할것인지(true), 아니면 최종 시점의 output만 출력할 것인지 결정.</li>
<li><strong><code>return_state</code></strong>: Boolean. Whether to return the last state in addition to the output. 최종 시점의 output뿐만 아니라 최종 시점의 은닉상태도 출력할지를 결정</li>
<li>호출 위한 기본 파라미터는<br>: <code>inputs</code> (3D tensor), training, initial_state</li>
</ul>
</li>
</ul>
<h3 id="2-사용"><a href="#2-사용" class="headerlink" title="2. 사용"></a>2. 사용</h3><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br></pre></td><td class="code"><pre><span class="line">encoder = Encoder(vocab_inp_size, embedding_dim, units, BATCH_SIZE)</span><br><span class="line"></span><br><span class="line"><span class="comment"># sample input</span></span><br><span class="line">sample_hidden = encoder.initialize_hidden_state()</span><br><span class="line">sample_output, sample_hidden = encoder(example_input_batch, sample_hidden)</span><br><span class="line"><span class="built_in">print</span> (<span class="string">&#x27;Encoder output shape: (batch size, sequence length, units) &#123;&#125;&#x27;</span>.<span class="built_in">format</span>(sample_output.shape))</span><br><span class="line"><span class="built_in">print</span> (<span class="string">&#x27;Encoder Hidden state shape: (batch size, units) &#123;&#125;&#x27;</span>.<span class="built_in">format</span>(sample_hidden.shape))</span><br></pre></td></tr></table></figure>

<h3 id="3-코드분석-Embedding-layer의-통과"><a href="#3-코드분석-Embedding-layer의-통과" class="headerlink" title="3. 코드분석 - Embedding layer의 통과"></a>3. 코드분석 - Embedding layer의 통과</h3><p>위의 코드는 이전의 전처리를 수행해야 시행해볼 수 있다. 바로 인코더 모델의 모습만 확인해 볼 수 있도록 임의의 텐서를 만들어 모델 구조를 확인해보자.</p>
<figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br><span class="line">23</span><br><span class="line">24</span><br><span class="line">25</span><br><span class="line">26</span><br><span class="line">27</span><br><span class="line">28</span><br><span class="line">29</span><br><span class="line">30</span><br><span class="line">31</span><br></pre></td><td class="code"><pre><span class="line"><span class="keyword">import</span> tensorflow <span class="keyword">as</span> tf</span><br><span class="line"></span><br><span class="line"><span class="class"><span class="keyword">class</span> <span class="title">Encoder</span>(<span class="params">tf.keras.Model</span>):</span></span><br><span class="line">  <span class="function"><span class="keyword">def</span> <span class="title">__init__</span>(<span class="params">self, vocab_size, embedding_dim, enc_units, batch_sz</span>):</span></span><br><span class="line">    <span class="built_in">super</span>(Encoder, self).__init__()</span><br><span class="line">    self.batch_sz = batch_sz</span><br><span class="line">    self.enc_units = enc_units</span><br><span class="line">    self.embedding = tf.keras.layers.Embedding(vocab_size, embedding_dim)</span><br><span class="line">    self.rnn = tf.keras.layers.SimpleRNN(self.enc_units,</span><br><span class="line">                                   return_sequences=<span class="literal">True</span>,</span><br><span class="line">                                   return_state=<span class="literal">True</span>,</span><br><span class="line">                                   recurrent_initializer=<span class="string">&#x27;glorot_uniform&#x27;</span>)</span><br><span class="line"></span><br><span class="line">  <span class="function"><span class="keyword">def</span> <span class="title">call</span>(<span class="params">self, x, hidden</span>):</span></span><br><span class="line">    x = self.embedding(x)</span><br><span class="line">    <span class="built_in">print</span> (<span class="string">&#x27;Encoder input shape: &#123;&#125;&#x27;</span>.<span class="built_in">format</span>(x.shape))</span><br><span class="line">    output, state = self.rnn(x, initial_state = hidden)</span><br><span class="line">    <span class="keyword">return</span> output, state</span><br><span class="line"></span><br><span class="line">  <span class="function"><span class="keyword">def</span> <span class="title">initialize_hidden_state</span>(<span class="params">self</span>):</span></span><br><span class="line">    <span class="keyword">return</span> tf.zeros((self.batch_sz, self.enc_units))</span><br><span class="line"></span><br><span class="line">example_input_batch = tf.zeros((<span class="number">3</span>, <span class="number">7</span>))</span><br><span class="line">encoder = Encoder(<span class="number">12</span>, <span class="number">4</span>, <span class="number">6</span>, <span class="number">3</span>)</span><br><span class="line"><span class="comment"># sample input</span></span><br><span class="line">sample_hidden = encoder.initialize_hidden_state()</span><br><span class="line">sample_output, sample_hidden = encoder(example_input_batch, sample_hidden)</span><br><span class="line"><span class="built_in">print</span> (<span class="string">&#x27;Encoder output shape: (batch size, sequence length, units) &#123;&#125;&#x27;</span>.<span class="built_in">format</span>(sample_output.shape))</span><br><span class="line"><span class="built_in">print</span> (<span class="string">&#x27;Encoder Hidden state shape: (batch size, units) &#123;&#125;&#x27;</span>.<span class="built_in">format</span>(sample_hidden.shape))</span><br><span class="line"></span><br><span class="line">encoder.summary()</span><br></pre></td></tr></table></figure>

<p>위의 코드는 </p>
<ol>
<li>단어집합 크기 : 12</li>
<li>임베딩 차원 : 4</li>
<li>은닉상태의 크기 : 6</li>
<li>배치크기 : 3</li>
</ol>
<p>인 경우이다. 이때 입력 배치는 (3, 7)의 텐서이다. 공부를 하면서 헷갈리는 부분이 있어서 텐서의 크기를 단어집합 크기와 동일하게 3, 12로 잡고 싶었는데 그렇게 하면 out of bound 인덱스 오류가 나더라. 즉, 모델의 단어집합 크기와 동일한 크기의 텐서를 초기 입력으로 넣을 수 없는 듯 하다. 어쩔 수 없이 만든 그림은 (1) 모델의 단어집합 크기가 12인 경우 모델의 모습 과 (2) 모델의 초기 입력 텐서의 크기가 (3(=batch_sz), 12)인 경우 두개의 짬뽕이 되어버림.</p>
<p>위의 코드를 시행하면 아래의 결과가 나온다.</p>
<p><img src="/image/Untitled_-_Colaboratory.png" alt="Untitled_-_Colaboratory"></p>
<h4 id="임베딩-레이어"><a href="#임베딩-레이어" class="headerlink" title="임베딩 레이어"></a>임베딩 레이어</h4><p><img src="/image/nlp_visualize7.png" alt="자연어처리_시각화-8320429"></p>
<p>케라스의 Embedding 함수는 단어집합의 크기와 임베딩 차원을 변수로 받아 임베딩 레이어를 만들어준다. 이때 레이어에 모델을 추가하는 것과 모델에 들어가는 텐서는 별도이다. 난 이 개념을 이해하는게 넘나 어려웠다..ㅋㅋㅋㅠ</p>
<p>이게 뭔말인고 하니… 위의 그림은 시퀀스 길이가 12, 시퀀스 개수가 3, 임베딩 차원이 4인 경우 Embedding Layer를 통과했을때 텐서의 크기변환 시각화이다. </p>
<p>즉, 아래와 같은 예시가 있다고 가정하자.</p>
<p>[[I, am, studying, neural, language, machine, translation, in, a, cafe, near, home], [I, am, studying, language], [neural, machine, translation]]</p>
<p>이때 각 시퀀스를 12의 길이로 패딩해주자.</p>
<p>[    [I, am, studying, neural, language, machine, translation, in, a, cafe, near, home], </p>
<p>​    [I, am, studying, language, <pad>, <pad>, <pad>, <pad>, <pad>, <pad>, <pad>, <pad>], </pad></pad></pad></pad></pad></pad></pad></pad></p>
<p>​    [neural, machine, translation, <pad>, <pad>, <pad>, <pad>, <pad>, <pad>, <pad>, <pad>, <pad>]]</pad></pad></pad></pad></pad></pad></pad></pad></pad></p>
<p>패딩된 시퀀스를 정수 인코딩해주면 대충 아래처럼 된다.</p>
<p>[[1, 2, 3, …, 12], [1, 2, 3, 5, 0, 0, … 0], [4, 6, 7, 0, 0….. 0]]</p>
<p>= 크기 (3, 12)</p>
<p>그럼 이 (3, 12)의 텐서가 Embedding(단어집합 크기, 임베딩차원) 으로 만들어진 임베딩 레이어에 들어가는거다. </p>
<p>이때 단어집합의 크기를 14라고 하면(단어장 개수 + <unk> + <pad>) 해당 텐서가 들어가는 <code>임베딩 레이어 모델</code> 에는 임베딩 작업을 위한 <code>룩업 테이블</code> 이 생성되고, 이때 이 룩업테이블의 크기는 <code>임베딩 레이어의 파라미터의 개수</code> 가 된다.</pad></unk></p>
<blockquote>
<dl><dt>num of parameters in Embedding layer</dt><dd><code>14(vocab sz) * 4(embedding dim)</code> </dd></dl></blockquote>
<p><img src="/image/nlp_visualize5.png" alt="자연어처리_시각화-8321348"></p>
<p>위의 코드에서는 example_input_batch가 입력으로 들어가는데 코드 설명을 보면 (64, 16)의 크기이며 단어장 크기는 9000정도라고 한다. 또한 임베딩 차원은 256이라고 한다.</p>
<p>그럼 결국 다음과 같다.</p>
<ul>
<li>길이가 16인 시퀀스가 64개 있다. : 16개의 단어로 이뤄진 문장이 64개</li>
<li>(64, 16)인 이 입력텐서는 임베딩 레이어를 거치면 (64, 16, 9000) 이 된다.</li>
<li>임베딩 레이어의 파라미터 개수는 9000 * 256</li>
</ul>
<h3 id="4-코드분석-RNN-은닉층의-통과"><a href="#4-코드분석-RNN-은닉층의-통과" class="headerlink" title="4. 코드분석 - RNN 은닉층의 통과"></a>4. 코드분석 - RNN 은닉층의 통과</h3><p>원본 코드에서는 GRU를 사용했지만 보다 용이한 (나의)이해를 위해 사용하는 은닉층을 SimpleRNN으로 변경해보았다.</p>
<p>아까 위에서 임베딩 층을 통과하면서 (시퀀스 개수, 시퀀스 길이, 임베딩 차원) 으로 변환된 텐서는 RNN의 입력으로 들어가게 된다.</p>
<p><img src="/image/nlp_visualize1.png" alt="자연어처리_시각화-8321950"></p>
<p>그리고 은닉층을 통과한 텐서는 위의 그림처럼 변환되게 된다. 이때 <em>units</em> 은 <code>은닉층의 크기</code> 를 의미한다. 은닉층의 파라미터는 아래 그림과 같다.</p>
<ol>
<li><img src="/image/nlp_visualize2.png" alt="자연어처리_시각화-8322229"></li>
<li><img src="/image/nlp_visualize3.png" alt="자연어처리_시각화-8322259"></li>
</ol>
<p>1번 그림의 vocab_sz는 sequence length인데 바꾸기가 귀찮았다. 여튼 내가 이해한 것은 이랬고, 결국 두개를 합쳐서 그려보면 아래와 같아진다.</p>
<p><img src="/image/nlp_visualize4.png" alt="자연어처리_시각화-8322406"></p>
<p>중간의 (3, 6) 텐서가 현재시점의 은닉상태이다. 즉, hidden state의 shape은 (num of sequence, 은닉상태 크기) 이다.</p>
<h2 id="Attention"><a href="#Attention" class="headerlink" title="Attention"></a>Attention</h2><h3 id="1-클래스-설계-1"><a href="#1-클래스-설계-1" class="headerlink" title="1. 클래스 설계"></a>1. 클래스 설계</h3><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br><span class="line">23</span><br><span class="line">24</span><br><span class="line">25</span><br><span class="line">26</span><br><span class="line">27</span><br><span class="line">28</span><br></pre></td><td class="code"><pre><span class="line"><span class="class"><span class="keyword">class</span> <span class="title">BahdanauAttention</span>(<span class="params">tf.keras.layers.Layer</span>):</span></span><br><span class="line">  <span class="function"><span class="keyword">def</span> <span class="title">__init__</span>(<span class="params">self, units</span>):</span></span><br><span class="line">    <span class="built_in">super</span>(BahdanauAttention, self).__init__()</span><br><span class="line">    self.W1 = tf.keras.layers.Dense(units)</span><br><span class="line">    self.W2 = tf.keras.layers.Dense(units)</span><br><span class="line">    self.V = tf.keras.layers.Dense(<span class="number">1</span>)</span><br><span class="line"></span><br><span class="line">  <span class="function"><span class="keyword">def</span> <span class="title">call</span>(<span class="params">self, query, values</span>):</span></span><br><span class="line">    <span class="comment"># query hidden state shape == (batch_size, hidden size)</span></span><br><span class="line">    <span class="comment"># query_with_time_axis shape == (batch_size, 1, hidden size)</span></span><br><span class="line">    <span class="comment"># values shape == (batch_size, max_len, hidden size)</span></span><br><span class="line">    <span class="comment"># we are doing this to broadcast addition along the time axis to calculate the score</span></span><br><span class="line">    query_with_time_axis = tf.expand_dims(query, <span class="number">1</span>)</span><br><span class="line"></span><br><span class="line">    <span class="comment"># score shape == (batch_size, max_length, 1)</span></span><br><span class="line">    <span class="comment"># we get 1 at the last axis because we are applying score to self.V</span></span><br><span class="line">    <span class="comment"># the shape of the tensor before applying self.V is (batch_size, max_length, units)</span></span><br><span class="line">    score = self.V(tf.nn.tanh(</span><br><span class="line">        self.W1(query_with_time_axis) + self.W2(values)))</span><br><span class="line"></span><br><span class="line">    <span class="comment"># attention_weights shape == (batch_size, max_length, 1)</span></span><br><span class="line">    attention_weights = tf.nn.softmax(score, axis=<span class="number">1</span>)</span><br><span class="line"></span><br><span class="line">    <span class="comment"># context_vector shape after sum == (batch_size, hidden_size)</span></span><br><span class="line">    context_vector = attention_weights * values</span><br><span class="line">    context_vector = tf.reduce_sum(context_vector, axis=<span class="number">1</span>)</span><br><span class="line"></span><br><span class="line">    <span class="keyword">return</span> context_vector, attention_weights</span><br></pre></td></tr></table></figure>



<h3 id="2-사용-1"><a href="#2-사용-1" class="headerlink" title="2. 사용"></a>2. 사용</h3><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br></pre></td><td class="code"><pre><span class="line">attention_layer = BahdanauAttention(<span class="number">10</span>)</span><br><span class="line">attention_result, attention_weights = attention_layer(sample_hidden, sample_output)</span><br><span class="line"></span><br><span class="line"><span class="built_in">print</span>(<span class="string">&quot;Attention result shape: (batch size, units) &#123;&#125;&quot;</span>.<span class="built_in">format</span>(attention_result.shape))</span><br><span class="line"><span class="built_in">print</span>(<span class="string">&quot;Attention weights shape: (batch_size, sequence_length, 1) &#123;&#125;&quot;</span>.<span class="built_in">format</span>(attention_weights.shape))</span><br></pre></td></tr></table></figure>



<h2 id="Decoder"><a href="#Decoder" class="headerlink" title="Decoder"></a>Decoder</h2><h3 id="1-클래스-설계-2"><a href="#1-클래스-설계-2" class="headerlink" title="1. 클래스 설계"></a>1. 클래스 설계</h3><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br><span class="line">23</span><br><span class="line">24</span><br><span class="line">25</span><br><span class="line">26</span><br><span class="line">27</span><br><span class="line">28</span><br><span class="line">29</span><br><span class="line">30</span><br><span class="line">31</span><br><span class="line">32</span><br><span class="line">33</span><br><span class="line">34</span><br><span class="line">35</span><br></pre></td><td class="code"><pre><span class="line"><span class="class"><span class="keyword">class</span> <span class="title">Decoder</span>(<span class="params">tf.keras.Model</span>):</span></span><br><span class="line">  <span class="function"><span class="keyword">def</span> <span class="title">__init__</span>(<span class="params">self, vocab_size, embedding_dim, dec_units, batch_sz</span>):</span></span><br><span class="line">    <span class="built_in">super</span>(Decoder, self).__init__()</span><br><span class="line">    self.batch_sz = batch_sz</span><br><span class="line">    self.dec_units = dec_units</span><br><span class="line">    self.embedding = tf.keras.layers.Embedding(vocab_size, embedding_dim)</span><br><span class="line">    self.gru = tf.keras.layers.GRU(self.dec_units,</span><br><span class="line">                                   return_sequences=<span class="literal">True</span>,</span><br><span class="line">                                   return_state=<span class="literal">True</span>,</span><br><span class="line">                                   recurrent_initializer=<span class="string">&#x27;glorot_uniform&#x27;</span>)</span><br><span class="line">    self.fc = tf.keras.layers.Dense(vocab_size)</span><br><span class="line"></span><br><span class="line">    <span class="comment"># used for attention</span></span><br><span class="line">    self.attention = BahdanauAttention(self.dec_units)</span><br><span class="line"></span><br><span class="line">  <span class="function"><span class="keyword">def</span> <span class="title">call</span>(<span class="params">self, x, hidden, enc_output</span>):</span></span><br><span class="line">    <span class="comment"># enc_output shape == (batch_size, max_length, hidden_size)</span></span><br><span class="line">    context_vector, attention_weights = self.attention(hidden, enc_output)</span><br><span class="line"></span><br><span class="line">    <span class="comment"># x shape after passing through embedding == (batch_size, 1, embedding_dim)</span></span><br><span class="line">    x = self.embedding(x)</span><br><span class="line"></span><br><span class="line">    <span class="comment"># x shape after concatenation == (batch_size, 1, embedding_dim + hidden_size)</span></span><br><span class="line">    x = tf.concat([tf.expand_dims(context_vector, <span class="number">1</span>), x], axis=-<span class="number">1</span>)</span><br><span class="line"></span><br><span class="line">    <span class="comment"># passing the concatenated vector to the GRU</span></span><br><span class="line">    output, state = self.gru(x)</span><br><span class="line"></span><br><span class="line">    <span class="comment"># output shape == (batch_size * 1, hidden_size)</span></span><br><span class="line">    output = tf.reshape(output, (-<span class="number">1</span>, output.shape[<span class="number">2</span>]))</span><br><span class="line"></span><br><span class="line">    <span class="comment"># output shape == (batch_size, vocab)</span></span><br><span class="line">    x = self.fc(output)</span><br><span class="line"></span><br><span class="line">    <span class="keyword">return</span> x, state, attention_weights</span><br></pre></td></tr></table></figure>



<h3 id="2-사용-2"><a href="#2-사용-2" class="headerlink" title="2. 사용"></a>2. 사용</h3><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br></pre></td><td class="code"><pre><span class="line">decoder = Decoder(vocab_tar_size, embedding_dim, units, BATCH_SIZE)</span><br><span class="line"></span><br><span class="line">sample_decoder_output, _, _ = decoder(tf.random.uniform((BATCH_SIZE, <span class="number">1</span>)),</span><br><span class="line">                                      sample_hidden, sample_output)</span><br><span class="line"></span><br><span class="line"><span class="built_in">print</span> (<span class="string">&#x27;Decoder output shape: (batch_size, vocab size) &#123;&#125;&#x27;</span>.<span class="built_in">format</span>(sample_decoder_output.shape))</span><br></pre></td></tr></table></figure>



<h2 id="The-optimizer-and-the-loss-function"><a href="#The-optimizer-and-the-loss-function" class="headerlink" title="The optimizer and the loss function"></a>The optimizer and the loss function</h2><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br></pre></td><td class="code"><pre><span class="line">optimizer = tf.keras.optimizers.Adam()</span><br><span class="line">loss_object = tf.keras.losses.SparseCategoricalCrossentropy(</span><br><span class="line">    from_logits=<span class="literal">True</span>, reduction=<span class="string">&#x27;none&#x27;</span>)</span><br><span class="line"></span><br><span class="line"><span class="function"><span class="keyword">def</span> <span class="title">loss_function</span>(<span class="params">real, pred</span>):</span></span><br><span class="line">  mask = tf.math.logical_not(tf.math.equal(real, <span class="number">0</span>))</span><br><span class="line">  loss_ = loss_object(real, pred)</span><br><span class="line"></span><br><span class="line">  mask = tf.cast(mask, dtype=loss_.dtype)</span><br><span class="line">  loss_ *= mask</span><br><span class="line"></span><br><span class="line">  <span class="keyword">return</span> tf.reduce_mean(loss_)</span><br></pre></td></tr></table></figure>



<h2 id="Checkpoints"><a href="#Checkpoints" class="headerlink" title="Checkpoints"></a>Checkpoints</h2><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br></pre></td><td class="code"><pre><span class="line">checkpoint_dir = <span class="string">&#x27;./training_checkpoints&#x27;</span></span><br><span class="line">checkpoint_prefix = os.path.join(checkpoint_dir, <span class="string">&quot;ckpt&quot;</span>)</span><br><span class="line">checkpoint = tf.train.Checkpoint(optimizer=optimizer,</span><br><span class="line">                                 encoder=encoder,</span><br><span class="line">                                 decoder=decoder)</span><br></pre></td></tr></table></figure>



<h2 id="Training"><a href="#Training" class="headerlink" title="Training"></a>Training</h2><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br><span class="line">23</span><br><span class="line">24</span><br><span class="line">25</span><br><span class="line">26</span><br><span class="line">27</span><br><span class="line">28</span><br><span class="line">29</span><br><span class="line">30</span><br><span class="line">31</span><br><span class="line">32</span><br><span class="line">33</span><br><span class="line">34</span><br><span class="line">35</span><br><span class="line">36</span><br><span class="line">37</span><br><span class="line">38</span><br><span class="line">39</span><br><span class="line">40</span><br><span class="line">41</span><br><span class="line">42</span><br><span class="line">43</span><br><span class="line">44</span><br><span class="line">45</span><br><span class="line">46</span><br><span class="line">47</span><br><span class="line">48</span><br><span class="line">49</span><br><span class="line">50</span><br><span class="line">51</span><br><span class="line">52</span><br><span class="line">53</span><br><span class="line">54</span><br></pre></td><td class="code"><pre><span class="line"><span class="meta">@tf.function</span></span><br><span class="line"><span class="function"><span class="keyword">def</span> <span class="title">train_step</span>(<span class="params">inp, targ, enc_hidden</span>):</span></span><br><span class="line">  loss = <span class="number">0</span></span><br><span class="line"></span><br><span class="line">  <span class="keyword">with</span> tf.GradientTape() <span class="keyword">as</span> tape:</span><br><span class="line">    enc_output, enc_hidden = encoder(inp, enc_hidden)</span><br><span class="line"></span><br><span class="line">    dec_hidden = enc_hidden</span><br><span class="line"></span><br><span class="line">    dec_input = tf.expand_dims([targ_lang.word_index[<span class="string">&#x27;&lt;start&gt;&#x27;</span>]] * BATCH_SIZE, <span class="number">1</span>)</span><br><span class="line"></span><br><span class="line">    <span class="comment"># Teacher forcing - feeding the target as the next input</span></span><br><span class="line">    <span class="keyword">for</span> t <span class="keyword">in</span> <span class="built_in">range</span>(<span class="number">1</span>, targ.shape[<span class="number">1</span>]):</span><br><span class="line">      <span class="comment"># passing enc_output to the decoder</span></span><br><span class="line">      predictions, dec_hidden, _ = decoder(dec_input, dec_hidden, enc_output)</span><br><span class="line"></span><br><span class="line">      loss += loss_function(targ[:, t], predictions)</span><br><span class="line"></span><br><span class="line">      <span class="comment"># using teacher forcing</span></span><br><span class="line">      dec_input = tf.expand_dims(targ[:, t], <span class="number">1</span>)</span><br><span class="line"></span><br><span class="line">  batch_loss = (loss / <span class="built_in">int</span>(targ.shape[<span class="number">1</span>]))</span><br><span class="line"></span><br><span class="line">  variables = encoder.trainable_variables + decoder.trainable_variables</span><br><span class="line"></span><br><span class="line">  gradients = tape.gradient(loss, variables)</span><br><span class="line"></span><br><span class="line">  optimizer.apply_gradients(<span class="built_in">zip</span>(gradients, variables))</span><br><span class="line"></span><br><span class="line">  <span class="keyword">return</span> batch_loss</span><br><span class="line">  </span><br><span class="line">  EPOCHS = <span class="number">10</span></span><br><span class="line"></span><br><span class="line"><span class="keyword">for</span> epoch <span class="keyword">in</span> <span class="built_in">range</span>(EPOCHS):</span><br><span class="line">  start = time.time()</span><br><span class="line"></span><br><span class="line">  enc_hidden = encoder.initialize_hidden_state()</span><br><span class="line">  total_loss = <span class="number">0</span></span><br><span class="line"></span><br><span class="line">  <span class="keyword">for</span> (batch, (inp, targ)) <span class="keyword">in</span> <span class="built_in">enumerate</span>(dataset.take(steps_per_epoch)):</span><br><span class="line">    batch_loss = train_step(inp, targ, enc_hidden)</span><br><span class="line">    total_loss += batch_loss</span><br><span class="line"></span><br><span class="line">    <span class="keyword">if</span> batch % <span class="number">100</span> == <span class="number">0</span>:</span><br><span class="line">      <span class="built_in">print</span>(<span class="string">&#x27;Epoch &#123;&#125; Batch &#123;&#125; Loss &#123;:.4f&#125;&#x27;</span>.<span class="built_in">format</span>(epoch + <span class="number">1</span>,</span><br><span class="line">                                                   batch,</span><br><span class="line">                                                   batch_loss.numpy()))</span><br><span class="line">  <span class="comment"># saving (checkpoint) the model every 2 epochs</span></span><br><span class="line">  <span class="keyword">if</span> (epoch + <span class="number">1</span>) % <span class="number">2</span> == <span class="number">0</span>:</span><br><span class="line">    checkpoint.save(file_prefix = checkpoint_prefix)</span><br><span class="line"></span><br><span class="line">  <span class="built_in">print</span>(<span class="string">&#x27;Epoch &#123;&#125; Loss &#123;:.4f&#125;&#x27;</span>.<span class="built_in">format</span>(epoch + <span class="number">1</span>,</span><br><span class="line">                                      total_loss / steps_per_epoch))</span><br><span class="line">  <span class="built_in">print</span>(<span class="string">&#x27;Time taken for 1 epoch &#123;&#125; sec\n&#x27;</span>.<span class="built_in">format</span>(time.time() - start))</span><br></pre></td></tr></table></figure>

</div><div class="article-tags is-size-7 mb-4"><span class="mr-2">#</span><a class="link-muted mr-2" rel="tag" href="/tags/pytorch/">pytorch</a><a class="link-muted mr-2" rel="tag" href="/tags/%EC%9E%90%EC%97%B0%EC%96%B4%EC%B2%98%EB%A6%AC/">자연어처리</a><a class="link-muted mr-2" rel="tag" href="/tags/%EB%94%A5%EB%9F%AC%EB%8B%9D/">딥러닝</a><a class="link-muted mr-2" rel="tag" href="/tags/%EC%96%B8%EC%96%B4%EB%AA%A8%EB%8D%B8/">언어모델</a><a class="link-muted mr-2" rel="tag" href="/tags/%ED%95%9C%EA%B5%AD%EC%96%B4-%EC%9E%84%EB%B2%A0%EB%94%A9/">한국어 임베딩</a><a class="link-muted mr-2" rel="tag" href="/tags/%EC%BC%80%EB%9D%BC%EC%8A%A4/">케라스</a><a class="link-muted mr-2" rel="tag" href="/tags/Attention/">Attention</a></div><link rel="stylesheet" href="https://cdn.jsdelivr.net/npm/social-share.js@1.0.16/dist/css/share.min.css"><div class="social-share"></div><script src="https://cdn.jsdelivr.net/npm/social-share.js@1.0.16/dist/js/social-share.min.js"></script></article></div><!--!--><nav class="post-navigation mt-4 level is-mobile"><div class="level-start"><a class="article-nav-prev level level-item link-muted" href="/2020/06/08/tf-study1/"><i class="level-item fas fa-chevron-left"></i><span class="level-item">Tensorflow 개발자 자격증 준비하기(1)</span></a></div><div class="level-end"><a class="article-nav-next level level-item link-muted" href="/2020/03/20/vue-study2/"><span class="level-item">Vue.js 코딩가이드 : Strongly Recommended (Improving Readability)</span><i class="level-item fas fa-chevron-right"></i></a></div></nav><div class="card"><div class="card-content"><h3 class="title is-5">댓글</h3><div class="fb-comments" data-width="100%" data-href="https://katie0809.github.io/2020/03/23/ai-study8/" data-num-posts="5"></div><script>(function(d, s, id) {
            var js, fjs = d.getElementsByTagName(s)[0];
            if (d.getElementById(id)) return;
            js = d.createElement(s); js.id = id;
            js.src = "//connect.facebook.net/ko/sdk.js#xfbml=1&version=v12.0";
            fjs.parentNode.insertBefore(js, fjs);
        }(document, 'script', 'facebook-jssdk'));</script></div></div></div><!--!--><div class="column column-right is-4-tablet is-4-desktop is-4-widescreen  order-3 is-sticky"><div class="card widget" id="toc" data-type="toc"><div class="card-content"><div class="menu"><h3 class="menu-label">카탈로그</h3><ul class="menu-list"><li><a class="level is-mobile" href="#Neural-machine-translation-with-attention"><span class="level-left"><span class="level-item">1</span><span class="level-item">Neural machine translation with attention</span></span></a><ul class="menu-list"><li><a class="level is-mobile" href="#Encoder"><span class="level-left"><span class="level-item">1.1</span><span class="level-item">Encoder</span></span></a><ul class="menu-list"><li><a class="level is-mobile" href="#1-클래스-설계"><span class="level-left"><span class="level-item">1.1.1</span><span class="level-item">1. 클래스 설계</span></span></a></li><li><a class="level is-mobile" href="#2-사용"><span class="level-left"><span class="level-item">1.1.2</span><span class="level-item">2. 사용</span></span></a></li><li><a class="level is-mobile" href="#3-코드분석-Embedding-layer의-통과"><span class="level-left"><span class="level-item">1.1.3</span><span class="level-item">3. 코드분석 - Embedding layer의 통과</span></span></a></li><li><a class="level is-mobile" href="#4-코드분석-RNN-은닉층의-통과"><span class="level-left"><span class="level-item">1.1.4</span><span class="level-item">4. 코드분석 - RNN 은닉층의 통과</span></span></a></li></ul></li><li><a class="level is-mobile" href="#Attention"><span class="level-left"><span class="level-item">1.2</span><span class="level-item">Attention</span></span></a><ul class="menu-list"><li><a class="level is-mobile" href="#1-클래스-설계-1"><span class="level-left"><span class="level-item">1.2.1</span><span class="level-item">1. 클래스 설계</span></span></a></li><li><a class="level is-mobile" href="#2-사용-1"><span class="level-left"><span class="level-item">1.2.2</span><span class="level-item">2. 사용</span></span></a></li></ul></li><li><a class="level is-mobile" href="#Decoder"><span class="level-left"><span class="level-item">1.3</span><span class="level-item">Decoder</span></span></a><ul class="menu-list"><li><a class="level is-mobile" href="#1-클래스-설계-2"><span class="level-left"><span class="level-item">1.3.1</span><span class="level-item">1. 클래스 설계</span></span></a></li><li><a class="level is-mobile" href="#2-사용-2"><span class="level-left"><span class="level-item">1.3.2</span><span class="level-item">2. 사용</span></span></a></li></ul></li><li><a class="level is-mobile" href="#The-optimizer-and-the-loss-function"><span class="level-left"><span class="level-item">1.4</span><span class="level-item">The optimizer and the loss function</span></span></a></li><li><a class="level is-mobile" href="#Checkpoints"><span class="level-left"><span class="level-item">1.5</span><span class="level-item">Checkpoints</span></span></a></li><li><a class="level is-mobile" href="#Training"><span class="level-left"><span class="level-item">1.6</span><span class="level-item">Training</span></span></a></li></ul></li></ul></div></div><style>#toc .menu-list > li > a.is-active + .menu-list { display: block; }#toc .menu-list > li > a + .menu-list { display: none; }</style><script src="/js/toc.js" defer></script></div><div class="card widget" data-type="categories"><div class="card-content"><div class="menu"><h3 class="menu-label">카테고리</h3><ul class="menu-list"><li><a class="level is-mobile" href="/categories/%EA%B0%9C%EB%B0%9C%EC%9E%90-%EA%B3%B5%EB%B6%80/"><span class="level-start"><span class="level-item">개발자 공부</span></span><span class="level-end"><span class="level-item tag">42</span></span></a><ul><li><a class="level is-mobile" href="/categories/%EA%B0%9C%EB%B0%9C%EC%9E%90-%EA%B3%B5%EB%B6%80/JSP/"><span class="level-start"><span class="level-item">JSP</span></span><span class="level-end"><span class="level-item tag">1</span></span></a></li><li><a class="level is-mobile" href="/categories/%EA%B0%9C%EB%B0%9C%EC%9E%90-%EA%B3%B5%EB%B6%80/Javascript/"><span class="level-start"><span class="level-item">Javascript</span></span><span class="level-end"><span class="level-item tag">6</span></span></a><ul><li><a class="level is-mobile" href="/categories/%EA%B0%9C%EB%B0%9C%EC%9E%90-%EA%B3%B5%EB%B6%80/Javascript/ES6/"><span class="level-start"><span class="level-item">ES6</span></span><span class="level-end"><span class="level-item tag">1</span></span></a></li><li><a class="level is-mobile" href="/categories/%EA%B0%9C%EB%B0%9C%EC%9E%90-%EA%B3%B5%EB%B6%80/Javascript/Node-js/"><span class="level-start"><span class="level-item">Node.js</span></span><span class="level-end"><span class="level-item tag">1</span></span></a></li><li><a class="level is-mobile" href="/categories/%EA%B0%9C%EB%B0%9C%EC%9E%90-%EA%B3%B5%EB%B6%80/Javascript/Nuxt-js/"><span class="level-start"><span class="level-item">Nuxt.js</span></span><span class="level-end"><span class="level-item tag">2</span></span></a></li><li><a class="level is-mobile" href="/categories/%EA%B0%9C%EB%B0%9C%EC%9E%90-%EA%B3%B5%EB%B6%80/Javascript/Vue-js/"><span class="level-start"><span class="level-item">Vue.js</span></span><span class="level-end"><span class="level-item tag">2</span></span></a></li></ul></li><li><a class="level is-mobile" href="/categories/%EA%B0%9C%EB%B0%9C%EC%9E%90-%EA%B3%B5%EB%B6%80/Kaggle/"><span class="level-start"><span class="level-item">Kaggle</span></span><span class="level-end"><span class="level-item tag">1</span></span></a></li><li><a class="level is-mobile" href="/categories/%EA%B0%9C%EB%B0%9C%EC%9E%90-%EA%B3%B5%EB%B6%80/%EA%B5%AC%EA%B8%80%EC%95%A0%EB%84%90%EB%A6%AC%ED%8B%B1%EC%8A%A4/"><span class="level-start"><span class="level-item">구글애널리틱스</span></span><span class="level-end"><span class="level-item tag">2</span></span></a></li><li><a class="level is-mobile" href="/categories/%EA%B0%9C%EB%B0%9C%EC%9E%90-%EA%B3%B5%EB%B6%80/%EC%9D%B8%EA%B3%B5%EC%A7%80%EB%8A%A5/"><span class="level-start"><span class="level-item">인공지능</span></span><span class="level-end"><span class="level-item tag">24</span></span></a></li><li><a class="level is-mobile" href="/categories/%EA%B0%9C%EB%B0%9C%EC%9E%90-%EA%B3%B5%EB%B6%80/%EC%BD%94%EB%94%A9%ED%85%8C%EC%8A%A4%ED%8A%B8/"><span class="level-start"><span class="level-item">코딩테스트</span></span><span class="level-end"><span class="level-item tag">8</span></span></a></li></ul></li><li><a class="level is-mobile" href="/categories/%EA%B7%B8%EB%83%A5-%EA%B7%B8%EB%9F%B0-%EC%9D%BC%EC%83%81/"><span class="level-start"><span class="level-item">그냥 그런 일상</span></span><span class="level-end"><span class="level-item tag">4</span></span></a><ul><li><a class="level is-mobile" href="/categories/%EA%B7%B8%EB%83%A5-%EA%B7%B8%EB%9F%B0-%EC%9D%BC%EC%83%81/%EA%B7%B8%EB%A0%87%EA%B2%8C-%EB%B0%94%EB%B3%B4%EB%8A%94-%EC%95%84%EB%8B%98/"><span class="level-start"><span class="level-item">그렇게 바보는 아님</span></span><span class="level-end"><span class="level-item tag">2</span></span></a></li><li><a class="level is-mobile" href="/categories/%EA%B7%B8%EB%83%A5-%EA%B7%B8%EB%9F%B0-%EC%9D%BC%EC%83%81/%EB%81%84%EC%A0%81%EB%81%84%EC%A0%81/"><span class="level-start"><span class="level-item">끄적끄적</span></span><span class="level-end"><span class="level-item tag">2</span></span></a></li></ul></li></ul></div></div></div><div class="card widget" data-type="tags"><div class="card-content"><div class="menu"><h3 class="menu-label">태그</h3><div class="field is-grouped is-grouped-multiline"><div class="control"><a class="tags has-addons" href="/tags/Attention/"><span class="tag">Attention</span><span class="tag">3</span></a></div><div class="control"><a class="tags has-addons" href="/tags/ES2015/"><span class="tag">ES2015+</span><span class="tag">1</span></a></div><div class="control"><a class="tags has-addons" href="/tags/ES6/"><span class="tag">ES6</span><span class="tag">1</span></a></div><div class="control"><a class="tags has-addons" href="/tags/GA/"><span class="tag">GA</span><span class="tag">2</span></a></div><div class="control"><a class="tags has-addons" href="/tags/Google-Analytics/"><span class="tag">Google Analytics</span><span class="tag">2</span></a></div><div class="control"><a class="tags has-addons" href="/tags/Hexo/"><span class="tag">Hexo</span><span class="tag">1</span></a></div><div class="control"><a class="tags has-addons" href="/tags/Keras/"><span class="tag">Keras</span><span class="tag">1</span></a></div><div class="control"><a class="tags has-addons" href="/tags/RNN/"><span class="tag">RNN</span><span class="tag">4</span></a></div><div class="control"><a class="tags has-addons" href="/tags/YAMLException/"><span class="tag">YAMLException</span><span class="tag">1</span></a></div><div class="control"><a class="tags has-addons" href="/tags/api-server/"><span class="tag">api_server</span><span class="tag">1</span></a></div><div class="control"><a class="tags has-addons" href="/tags/change-permission/"><span class="tag">change permission</span><span class="tag">1</span></a></div><div class="control"><a class="tags has-addons" href="/tags/codility/"><span class="tag">codility</span><span class="tag">8</span></a></div><div class="control"><a class="tags has-addons" href="/tags/coding-rule/"><span class="tag">coding rule</span><span class="tag">2</span></a></div><div class="control"><a class="tags has-addons" href="/tags/coding-test/"><span class="tag">coding test</span><span class="tag">8</span></a></div><div class="control"><a class="tags has-addons" href="/tags/dacon/"><span class="tag">dacon</span><span class="tag">1</span></a></div><div class="control"><a class="tags has-addons" href="/tags/data-analysis/"><span class="tag">data analysis</span><span class="tag">1</span></a></div><div class="control"><a class="tags has-addons" href="/tags/developer/"><span class="tag">developer</span><span class="tag">8</span></a></div><div class="control"><a class="tags has-addons" href="/tags/express/"><span class="tag">express</span><span class="tag">1</span></a></div><div class="control"><a class="tags has-addons" href="/tags/folder-permission/"><span class="tag">folder permission</span><span class="tag">1</span></a></div><div class="control"><a class="tags has-addons" href="/tags/githubpage/"><span class="tag">githubpage</span><span class="tag">2</span></a></div><div class="control"><a class="tags has-addons" href="/tags/hexo/"><span class="tag">hexo</span><span class="tag">2</span></a></div><div class="control"><a class="tags has-addons" href="/tags/inflearn/"><span class="tag">inflearn</span><span class="tag">1</span></a></div><div class="control"><a class="tags has-addons" href="/tags/javascript/"><span class="tag">javascript</span><span class="tag">6</span></a></div><div class="control"><a class="tags has-addons" href="/tags/jsp/"><span class="tag">jsp</span><span class="tag">1</span></a></div><div class="control"><a class="tags has-addons" href="/tags/kaggle/"><span class="tag">kaggle</span><span class="tag">1</span></a></div><div class="control"><a class="tags has-addons" href="/tags/linear-regression/"><span class="tag">linear regression</span><span class="tag">2</span></a></div><div class="control"><a class="tags has-addons" href="/tags/logistic-regression/"><span class="tag">logistic regression</span><span class="tag">1</span></a></div><div class="control"><a class="tags has-addons" href="/tags/mac/"><span class="tag">mac</span><span class="tag">1</span></a></div><div class="control"><a class="tags has-addons" href="/tags/nodejs/"><span class="tag">nodejs</span><span class="tag">1</span></a></div><div class="control"><a class="tags has-addons" href="/tags/numpy/"><span class="tag">numpy</span><span class="tag">2</span></a></div><div class="control"><a class="tags has-addons" href="/tags/nuxtjs/"><span class="tag">nuxtjs</span><span class="tag">2</span></a></div><div class="control"><a class="tags has-addons" href="/tags/python/"><span class="tag">python</span><span class="tag">2</span></a></div><div class="control"><a class="tags has-addons" href="/tags/pytorch/"><span class="tag">pytorch</span><span class="tag">10</span></a></div><div class="control"><a class="tags has-addons" href="/tags/style-guide/"><span class="tag">style guide</span><span class="tag">2</span></a></div><div class="control"><a class="tags has-addons" href="/tags/tensorflow/"><span class="tag">tensorflow</span><span class="tag">9</span></a></div><div class="control"><a class="tags has-addons" href="/tags/tensorflow-2-0/"><span class="tag">tensorflow 2.0</span><span class="tag">1</span></a></div><div class="control"><a class="tags has-addons" href="/tags/tensorflow-developer-certification/"><span class="tag">tensorflow developer certification</span><span class="tag">7</span></a></div><div class="control"><a class="tags has-addons" href="/tags/torchtext/"><span class="tag">torchtext</span><span class="tag">1</span></a></div><div class="control"><a class="tags has-addons" href="/tags/vue/"><span class="tag">vue</span><span class="tag">2</span></a></div><div class="control"><a class="tags has-addons" href="/tags/vuejs/"><span class="tag">vuejs</span><span class="tag">4</span></a></div><div class="control"><a class="tags has-addons" href="/tags/vueschool-io/"><span class="tag">vueschool.io</span><span class="tag">2</span></a></div><div class="control"><a class="tags has-addons" href="/tags/%EA%B5%AC%EA%B8%80/"><span class="tag">구글</span><span class="tag">2</span></a></div><div class="control"><a class="tags has-addons" href="/tags/%EA%B5%AC%EA%B8%80%EC%95%A0%EB%84%90%EB%A6%AC%ED%8B%B1%EC%8A%A4/"><span class="tag">구글애널리틱스</span><span class="tag">2</span></a></div><div class="control"><a class="tags has-addons" href="/tags/%EA%B7%B8%EB%9E%98%ED%94%84-%EC%8B%9C%EA%B0%81%ED%99%94/"><span class="tag">그래프 시각화</span><span class="tag">1</span></a></div><div class="control"><a class="tags has-addons" href="/tags/%EB%8D%B0%EC%9D%B4%EC%BD%98/"><span class="tag">데이콘</span><span class="tag">1</span></a></div><div class="control"><a class="tags has-addons" href="/tags/%EB%8D%B0%EC%9D%B4%ED%84%B0%EB%B6%84%EC%84%9D/"><span class="tag">데이터분석</span><span class="tag">1</span></a></div><div class="control"><a class="tags has-addons" href="/tags/%EB%94%A5%EB%9F%AC%EB%8B%9D/"><span class="tag">딥러닝</span><span class="tag">9</span></a></div><div class="control"><a class="tags has-addons" href="/tags/%EB%94%A5%EB%9F%AC%EB%8B%9D%EA%B8%B0%EC%B4%88/"><span class="tag">딥러닝기초</span><span class="tag">7</span></a></div><div class="control"><a class="tags has-addons" href="/tags/%EB%A1%9C%EC%A7%80%EC%8A%A4%ED%8B%B1-%ED%9A%8C%EA%B7%80/"><span class="tag">로지스틱 회귀</span><span class="tag">1</span></a></div><div class="control"><a class="tags has-addons" href="/tags/%EC%84%A0%ED%98%95%ED%9A%8C%EA%B7%80/"><span class="tag">선형회귀</span><span class="tag">2</span></a></div><div class="control"><a class="tags has-addons" href="/tags/%EC%88%9C%ED%99%98%EC%8B%A0%EA%B2%BD%EB%A7%9D/"><span class="tag">순환신경망</span><span class="tag">3</span></a></div><div class="control"><a class="tags has-addons" href="/tags/%EC%96%B8%EC%96%B4%EB%AA%A8%EB%8D%B8/"><span class="tag">언어모델</span><span class="tag">8</span></a></div><div class="control"><a class="tags has-addons" href="/tags/%EC%9D%B8%EA%B3%B5%EC%A7%80%EB%8A%A5/"><span class="tag">인공지능</span><span class="tag">8</span></a></div><div class="control"><a class="tags has-addons" href="/tags/%EC%9E%90%EA%B2%A9%EC%A6%9D/"><span class="tag">자격증</span><span class="tag">7</span></a></div><div class="control"><a class="tags has-addons" href="/tags/%EC%9E%90%EC%97%B0%EC%96%B4-%EC%A0%84%EC%B2%98%EB%A6%AC/"><span class="tag">자연어 전처리</span><span class="tag">2</span></a></div><div class="control"><a class="tags has-addons" href="/tags/%EC%9E%90%EC%97%B0%EC%96%B4%EC%B2%98%EB%A6%AC/"><span class="tag">자연어처리</span><span class="tag">17</span></a></div><div class="control"><a class="tags has-addons" href="/tags/%EC%BA%90%EA%B8%80/"><span class="tag">캐글</span><span class="tag">1</span></a></div><div class="control"><a class="tags has-addons" href="/tags/%EC%BC%80%EB%9D%BC%EC%8A%A4/"><span class="tag">케라스</span><span class="tag">3</span></a></div><div class="control"><a class="tags has-addons" href="/tags/%ED%95%9C%EA%B5%AD%EC%96%B4-%EC%9E%84%EB%B2%A0%EB%94%A9/"><span class="tag">한국어 임베딩</span><span class="tag">6</span></a></div></div></div></div></div></div></div></div></section><footer class="footer"><div class="container"><div class="level"><div class="level-start"><a class="footer-logo is-block mb-2" href="/"><img src="/img/logo.svg" alt="Women&#039;s Techmaker" height="28"></a><p class="is-size-7"><span>&copy; 2022 Kyungim Lee</span>  Powered by <a href="https://hexo.io/" target="_blank" rel="external nofollow noopener noreferrer">Hexo</a> &amp; <a href="https://github.com/ppoffice/hexo-theme-icarus" target="_blank" rel="external nofollow noopener noreferrer">Icarus</a></p></div><div class="level-end"><div class="field has-addons"><p class="control"><a class="button is-transparent is-large" target="_blank" rel="external nofollow noopener noreferrer" title="Creative Commons" href="https://creativecommons.org/"><i class="fab fa-creative-commons"></i></a></p><p class="control"><a class="button is-transparent is-large" target="_blank" rel="external nofollow noopener noreferrer" title="Attribution 4.0 International" href="https://creativecommons.org/licenses/by/4.0/"><i class="fab fa-creative-commons-by"></i></a></p><p class="control"><a class="button is-transparent is-large" target="_blank" rel="external nofollow noopener noreferrer" title="Download on GitHub" href="https://github.com/katie0809"><i class="fab fa-github"></i></a></p></div></div></div></div></footer><script src="https://cdn.jsdelivr.net/npm/jquery@3.3.1/dist/jquery.min.js"></script><script src="https://cdn.jsdelivr.net/npm/moment@2.22.2/min/moment-with-locales.min.js"></script><script src="https://cdn.jsdelivr.net/npm/clipboard@2.0.4/dist/clipboard.min.js" defer></script><script>moment.locale("ko");</script><script>var IcarusThemeSettings = {
            article: {
                highlight: {
                    clipboard: true,
                    fold: 'unfolded'
                }
            }
        };</script><script src="/js/column.js"></script><script src="/js/animation.js"></script><a id="back-to-top" title="맨 위로" href="javascript:;"><i class="fas fa-chevron-up"></i></a><script src="/js/back_to_top.js" defer></script><!--!--><!--!--><script src="/js/main.js" defer></script><div class="searchbox"><div class="searchbox-container"><div class="searchbox-header"><div class="searchbox-input-container"><input class="searchbox-input" type="text" placeholder="입력 하세요..."></div><a class="searchbox-close" href="javascript:;">×</a></div><div class="searchbox-body"></div></div></div><script src="/js/insight.js" defer></script><script>document.addEventListener('DOMContentLoaded', function () {
            loadInsight({"contentUrl":"/content.json"}, {"hint":"입력 하세요...","untitled":"(제목 없음)","posts":"포스트","pages":"페이지","categories":"카테고리","tags":"태그"});
        });</script></body></html>