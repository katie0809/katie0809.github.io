<!doctype html>
<html lang="ko"><head><meta charset="utf-8"><meta name="viewport" content="width=device-width, initial-scale=1, maximum-scale=1"><meta name="naver-site-verification" content="071e98e503d41ec3f7c7a7e909eec970500c5657"><title>[딥러닝 스터디] Attention을 활용한 기계번역 - Women&#039;s Techmaker</title><link rel="manifest" href="/manifest.json"><meta name="application-name" content="Women&#039;s Techmaker"><meta name="msapplication-TileImage" content="/img/favicon.svg"><meta name="apple-mobile-web-app-capable" content="yes"><meta name="apple-mobile-web-app-title" content="Women&#039;s Techmaker"><meta name="apple-mobile-web-app-status-bar-style" content="default"><meta name="description" content="다음의 책을 공부하며 정리한 내용입니다.  https:&amp;#x2F;&amp;#x2F;wikidocs.net&amp;#x2F;24996 : seq2seq 정리 https:&amp;#x2F;&amp;#x2F;wikidocs.net&amp;#x2F;22893 : 어텐션 모델 정리 https:&amp;#x2F;&amp;#x2F;www.youtube.com&amp;#x2F;watch?v&amp;#x3D;c8y9ZAb9aks&amp;amp;t&amp;#x3D;1032s : seq2seq에서 attention까지(매우 좋음, 꼭 참고하세요) h"><meta property="og:type" content="blog"><meta property="og:title" content="[딥러닝 스터디] Attention을 활용한 기계번역"><meta property="og:url" content="https://katie0809.github.io/2020/02/23/ai-study7/"><meta property="og:site_name" content="Women&#039;s Techmaker"><meta property="og:description" content="다음의 책을 공부하며 정리한 내용입니다.  https:&amp;#x2F;&amp;#x2F;wikidocs.net&amp;#x2F;24996 : seq2seq 정리 https:&amp;#x2F;&amp;#x2F;wikidocs.net&amp;#x2F;22893 : 어텐션 모델 정리 https:&amp;#x2F;&amp;#x2F;www.youtube.com&amp;#x2F;watch?v&amp;#x3D;c8y9ZAb9aks&amp;amp;t&amp;#x3D;1032s : seq2seq에서 attention까지(매우 좋음, 꼭 참고하세요) h"><meta property="og:locale" content="ko_KR"><meta property="og:image" content="https://katie0809.github.io/image/Elegant_Background-2.jpg"><meta property="article:published_time" content="2020-02-23T07:54:46.000Z"><meta property="article:modified_time" content="2021-11-04T02:12:41.723Z"><meta property="article:author" content="Kyungim Lee"><meta property="article:tag" content="pytorch"><meta property="article:tag" content="자연어처리"><meta property="article:tag" content="딥러닝"><meta property="article:tag" content="언어모델"><meta property="article:tag" content="한국어 임베딩"><meta property="article:tag" content="케라스"><meta property="article:tag" content="Attention"><meta property="twitter:card" content="summary"><meta property="twitter:image" content="/image/Elegant_Background-2.jpg"><script type="application/ld+json">{"@context":"https://schema.org","@type":"BlogPosting","mainEntityOfPage":{"@type":"WebPage","@id":"https://katie0809.github.io/2020/02/23/ai-study7/"},"headline":"[딥러닝 스터디] Attention을 활용한 기계번역","image":["https://katie0809.github.io/image/Elegant_Background-2.jpg"],"datePublished":"2020-02-23T07:54:46.000Z","dateModified":"2021-11-04T02:12:41.723Z","author":{"@type":"Person","name":"Kyungim Lee"},"publisher":{"@type":"Organization","name":"Women's Techmaker","logo":{"@type":"ImageObject","url":"https://katie0809.github.io/img/logo.svg"}},"description":"다음의 책을 공부하며 정리한 내용입니다.  https:&#x2F;&#x2F;wikidocs.net&#x2F;24996 : seq2seq 정리 https:&#x2F;&#x2F;wikidocs.net&#x2F;22893 : 어텐션 모델 정리 https:&#x2F;&#x2F;www.youtube.com&#x2F;watch?v&#x3D;c8y9ZAb9aks&amp;t&#x3D;1032s : seq2seq에서 attention까지(매우 좋음, 꼭 참고하세요) h"}</script><link rel="canonical" href="https://katie0809.github.io/2020/02/23/ai-study7/"><link rel="icon" href="/img/favicon.svg"><link rel="stylesheet" href="https://use.fontawesome.com/releases/v5.15.2/css/all.css"><link rel="stylesheet" href="https://cdn.jsdelivr.net/npm/highlight.js@9.12.0/styles/vs2015.css"><link rel="stylesheet" href="https://fonts.googleapis.com/css2?family=Ubuntu:wght@400;600&amp;family=Source+Code+Pro"><link rel="stylesheet" href="/css/default.css"><style>body>.footer,body>.navbar,body>.section{opacity:0}</style><!--!--><script src="https://www.googletagmanager.com/gtag/js?id=G-ZPWK5T4901" async></script><script>window.dataLayer = window.dataLayer || [];
        function gtag(){dataLayer.push(arguments);}
        gtag('js', new Date());
    
        gtag('config', 'G-ZPWK5T4901');</script><style>.pace{-webkit-pointer-events:none;pointer-events:none;-webkit-user-select:none;-moz-user-select:none;user-select:none}.pace-inactive{display:none}.pace .pace-progress{background:#3273dc;position:fixed;z-index:2000;top:0;right:100%;width:100%;height:2px}</style><script src="https://cdn.jsdelivr.net/npm/pace-js@1.2.4/pace.min.js"></script><meta name="generator" content="Hexo 5.4.0"><link rel="alternate" href="/rss2.xml" title="Women's Techmaker" type="application/rss+xml">
</head><body class="is-2-column"><nav class="navbar navbar-main"><div class="container"><div class="navbar-brand justify-content-center"><a class="navbar-item navbar-logo" href="/"><img src="/img/logo.svg" alt="Women&#039;s Techmaker" height="28"></a></div><div class="navbar-menu"><div class="navbar-start"><a class="navbar-item" href="/">Home</a><a class="navbar-item" href="/archives">Archives</a><a class="navbar-item" href="/categories">Categories</a><a class="navbar-item" href="/tags">Tags</a><a class="navbar-item" href="/downloads">Downloads</a></div><div class="navbar-end"><a class="navbar-item" target="_blank" rel="external nofollow noopener noreferrer" title="Download on GitHub" href="https://github.com/ppoffice/hexo-theme-icarus"><i class="fab fa-github"></i></a><a class="navbar-item is-hidden-tablet catalogue" title="카탈로그" href="javascript:;"><i class="fas fa-list-ul"></i></a><a class="navbar-item search" title="검색" href="javascript:;"><i class="fas fa-search"></i></a></div></div></div></nav><section class="section"><div class="container"><div class="columns"><div class="column order-2 column-main is-8-tablet is-8-desktop is-8-widescreen"><div class="card"><div class="card-image"><span class="image is-7by3"><img class="fill" src="/image/Elegant_Background-2.jpg" alt="[딥러닝 스터디] Attention을 활용한 기계번역"></span></div><article class="card-content article" role="article"><div class="article-meta is-size-7 is-uppercase level is-mobile"><div class="level-left"><span class="level-item"><time datetime="2020-02-23T07:54:46.000Z" title="2020-2-23 4:54:46 ├F10: PM┤">2020-02-23</time>&nbsp;게시 됨</span><span class="level-item"><time datetime="2021-11-04T02:12:41.723Z" title="2021-11-4 11:12:41 ├F10: AM┤">2021-11-04</time>&nbsp;업데이트 됨</span><span class="level-item"><a class="link-muted" href="/categories/%EA%B0%9C%EB%B0%9C%EC%9E%90-%EA%B3%B5%EB%B6%80/">개발자 공부</a><span> / </span><a class="link-muted" href="/categories/%EA%B0%9C%EB%B0%9C%EC%9E%90-%EA%B3%B5%EB%B6%80/%EC%9D%B8%EA%B3%B5%EC%A7%80%EB%8A%A5/">인공지능</a></span><span class="level-item">14분안에 읽기 (약 2038 단어)</span></div></div><h1 class="title is-3 is-size-4-mobile">[딥러닝 스터디] Attention을 활용한 기계번역</h1><div class="content"><p>다음의 책을 공부하며 정리한 내용입니다.</p>
<ul>
<li><a target="_blank" rel="external nofollow noopener noreferrer" href="https://wikidocs.net/24996">https://wikidocs.net/24996</a> : seq2seq 정리</li>
<li><a target="_blank" rel="external nofollow noopener noreferrer" href="https://wikidocs.net/22893">https://wikidocs.net/22893</a> : 어텐션 모델 정리</li>
<li><a target="_blank" rel="external nofollow noopener noreferrer" href="https://www.youtube.com/watch?v=c8y9ZAb9aks&amp;t=1032s">https://www.youtube.com/watch?v=c8y9ZAb9aks&amp;t=1032s</a> : seq2seq에서 attention까지(매우 좋음, 꼭 참고하세요)</li>
<li><a target="_blank" rel="external nofollow noopener noreferrer" href="https://www.tensorflow.org/tutorials/text/nmt_with_attention">https://www.tensorflow.org/tutorials/text/nmt_with_attention</a> : attention 실습</li>
</ul>
<h1 id="시퀀스-투-시퀀스-seq2seq"><a href="#시퀀스-투-시퀀스-seq2seq" class="headerlink" title="시퀀스-투-시퀀스(seq2seq)"></a>시퀀스-투-시퀀스(seq2seq)</h1><p>: 입력된 시퀀스로부터 다른 도메인의 시퀀스를 출력하는 모델.</p>
<p>이는 다음과 같은 분야에서 사용된다.</p>
<ul>
<li>챗봇: 입력시퀀스와 출력시퀀스를 각각 질문/대답으로 구성하면 챗봇을 만들 수 있다.</li>
<li>기계번역: 입력시퀀스와 출력시퀀스를 입력/번역문장으로 구성하면 번역기를 만들 수 있다.</li>
<li>Text Summerization, Speech to Text 등에 사용될 수 있다.</li>
</ul>
<span id="more"></span>

<p><img src="https://wikidocs.net/images/page/24996/%EC%9D%B8%EC%BD%94%EB%8D%94%EB%94%94%EC%BD%94%EB%8D%94%EB%AA%A8%EB%8D%B8.PNG" alt></p>
<p>seq2seq 모델은 기본적으로 위의 구조를 띄고 있다.</p>
<ul>
<li>인코더와 디코더는 두개의 RNN 아키텍처이다. 입력 문장을 처리하는 RNN셀을 인코더, 출력 문장(번역된 문장)을 처리하는 RNN셀을 디코더라고 하는 것.</li>
<li>중간의 컨텍스트 벡터는 인코더 마지막 시점의 히든 스테이트의 크기이다. 즉, 이전의 내용이 함축된 하나의 벡터이다.</li>
</ul>
<p>이때 디코더에서 단순히 매 단계마다 가장 가능성이 높은 단어 하나를 선택하는 방식은 생각보다 효율적이지 않다. 이때 적용하는 방법이 Beam search.</p>
<blockquote>
<p>Beam search : 매 스텝마다 가장 ㄴㄴ 높은 n개의 단어를 선택하여, 이 N개의 단어 각각에 대해 다음 스텝에서 등장할 수 있는 모든 단어들의 확률을 예측한다. 이러한 방식으로 <strong>매 스텝마다 n개의 후보군을 유지</strong>하여 최적의 시퀀스 후보를 뽑아낸다.</p>
</blockquote>
<hr>
<h1 id="Attention-model"><a href="#Attention-model" class="headerlink" title="Attention model"></a>Attention model</h1><p><img src="https://www.tensorflow.org/images/seq2seq/attention_mechanism.jpg" alt></p>
<ul>
<li><p>seq2seq 모델은 기본적으로 <code>인코더 -&gt; [컨텍스트 벡터] -&gt; 디코더</code> 의 구조를 갖는다.</p>
</li>
<li><p>이러한 모델의 문제는 아래와 같다</p>
<ul>
<li>컨텍스트 벡터는 결국 <code>하나의 벡터</code>에 불과하다. 인코더의 모든 정보를 하나의 고정된 크기의 벡터에 압축하다 보면 필연적으로 <strong><code>정보의 손실</code></strong>이 발생하게 된다.</li>
<li>RNN의 고질적인 Vanishing gradient 문제가 발생한다.</li>
</ul>
<p>: 이러한 문제는 결과적으로 <u>입력 시퀀스가 길어질수록 번역의 품질이 저하</u>되는 문제를 야기한다.</p>
</li>
</ul>
<h2 id="Attention-Overview"><a href="#Attention-Overview" class="headerlink" title="Attention Overview"></a>Attention Overview</h2><p>어텐션 모델의 기본 아이디어는 다음과 같다.</p>
<blockquote>
<p>디코더의 매 time step마다 인코더에서의 <code>전체 입력문장을 다시 한번 참고</code>한다.</p>
</blockquote>
<ul>
<li>이때 입력 문장의 전체 토큰을 동일한 비중으로 참고하는 것이 아니라, 해당 시점에서 예측해야할 단어와 <em>연관이 있는 입력토큰</em> 부분을 좀더 집중(<strong>attention</strong>)해서 참고한다. </li>
</ul>
<h2 id="Dot-product-attention"><a href="#Dot-product-attention" class="headerlink" title="Dot product attention"></a>Dot product attention</h2><p>어텐션은 다양한 종류가 있다. 그 중 가장 기본적인 닷 프로덕트 어텐션의 구조를 살펴보자.</p>
<p>먼저 기본적인 용어 정의, seq2seq모델과 어텐션 모델의 차이점을 알아보자.</p>
<ul>
<li><strong>1, 2, 3… n</strong> : 인코더의 시점</li>
<li><strong>h1, h2, h3… hn</strong> : 각 시점에서의 인코더의 은닉 상태(hidden state)</li>
<li><strong>t</strong> : 디코더의 현재시점</li>
<li><strong>st</strong> : 현재 시점에서의 디코더의 은닉 상태</li>
</ul>
<p>이전에 배웠던 seq2seq에서 디코더는 두개의 값(<u>이전시점의 은닉상태, 이전시점의 출력</u>)을 통해 현재시점의 은닉상태를 계산했다. 이때 어텐션 모델에서는 계산을 위해 필요한 값이 하나 더 추가된다. 바로 <strong><code>t시점의 어텐션 값 at</code></strong>이다.</p>
<p>따라서 <u>어텐션 모델은 (seq2seq + 어텐션 값 계산) 인 모델</u>이다. 즉, 어텐션 모델에서의 핵심은 이 <strong>어텐션 값을 어떻게 구하는가</strong>이며, 이 과정에서 어텐션 스코어값을 구하는 방법에 따라 닷 프로덕트 어텐션, 루옹 어텐션, 바다나우 어텐션 등으로 종류가 나뉘게 된다.</p>
<p>또한 모든 어텐션 값 at는(“값”이라는 명칭에서 예상할 수 있듯) 스칼라 값이다.</p>
<h3 id="Step-1-어텐션-스코어를-구한다"><a href="#Step-1-어텐션-스코어를-구한다" class="headerlink" title="Step 1. 어텐션 스코어를 구한다."></a>Step 1. 어텐션 스코어를 구한다.</h3><p><img src="https://wikidocs.net/images/page/22893/dotproductattention2_final.PNG" alt></p>
<p>어텐션 스코어는 다음을 의미한다.</p>
<blockquote>
<p><strong>Attention score</strong> : 인코더의 <strong>각 은닉상태 h1 - hn</strong>이 현재 시점의 디코더 은닉상태 <strong>st</strong>와 <code>얼마나 유사한지</code>의 정도</p>
</blockquote>
<p>닷 프로덕트 어텐션에서는 이 스코어 값을 구하기 위해 <u>st와 hi(i : 1~n)를 닷 프로덕트 한다</u>. 이때 둘다 열벡터이므로 디코더의 은닉상태 st값을 전치하여 내적한다.</p>
<p>따라서 dot product attention의 attention score 함수 수식은 다음과 같다.</p>
<blockquote>
<p>score(st, hi) = stT * hi</p>
</blockquote>
<p>따라서 디코더의 현재 시점 t에 대한 은닉상태 st와 인코더의 모든 시점에 대한 은닉상태의 어텐션 스코어의 모음(et)은 아래와 같다.</p>
<blockquote>
<p>et = [stT * h1, …, stT * hN]</p>
</blockquote>
<h3 id="Step-2-Softmax를-통해-Attention-Distribution-어텐션-분포-를-구한다"><a href="#Step-2-Softmax를-통해-Attention-Distribution-어텐션-분포-를-구한다" class="headerlink" title="Step 2. Softmax를 통해 Attention Distribution(어텐션 분포)를 구한다."></a>Step 2. Softmax를 통해 Attention Distribution(어텐션 분포)를 구한다.</h3><p><img src="https://wikidocs.net/images/page/22893/dotproductattention3_final.PNG" alt></p>
<p>구해낸 모든 어텐션 스코어의 모음, <u>et에 <strong>softmax</strong>를 적용해 확률 분포를 얻어낸다</u>. </p>
<p>이를 통해 얻어낸 분포를 <code>Attention Distribution</code>(어텐션 분포)라 하며, 각각의 값을 <code>Attention Weight</code>(어텐션 가중치)라고 한다.</p>
<p><strong>어텐션 분포와 어텐션 값</strong>은 <code>디코더의 현재시점 t에 대해 정의</code>된다.</p>
<blockquote>
<p>Attention Distribution : 어텐션 스코어의 모음에 softmax를 적용해 얻어낸 확률분포. 어텐션 가중치의 모음값</p>
<p>Attention Weight : 어텐션 분포의 각각의 값</p>
</blockquote>
<p>따라서 어텐션 분포를 αt의 식은 다음과 같다.</p>
<blockquote>
<p>αt = softmax(et)</p>
</blockquote>
<h3 id="Step-3-Attention-Weight과-인코더-은닉상태를-가중합하여-Attention-Value를-구한다"><a href="#Step-3-Attention-Weight과-인코더-은닉상태를-가중합하여-Attention-Value를-구한다" class="headerlink" title="Step 3. Attention Weight과 인코더 은닉상태를 가중합하여 Attention Value를 구한다."></a>Step 3. Attention Weight과 인코더 은닉상태를 가중합하여 Attention Value를 구한다.</h3><p><img src="https://wikidocs.net/images/page/22893/dotproductattention4_final.PNG" alt></p>
<p>구해낸 Attention Distribution의 각 <strong>Attention Weight</strong>들을 <strong>해당 시점의 인코더 은닉상태(h1 ~ hN)</strong>와 <code>가중합(Weighted sum)</code> 한다. </p>
<p><strong>결과로 나오는 벡터 a</strong>는 최종 어텐션 값, 즉 <u>Attention Value</u>가 되며 이는 인코더의 문맥을 내포하고 있다는 의미에서 <u>Context Vector</u>라고 부르기도 한다.</p>
<blockquote>
<p>a = ∑αti * hi (i : 1~N, ati : i번째 어텐션 분포의 값)</p>
</blockquote>
<h3 id="Step-4-어텐션-값과-현재-상태-대코더의-은닉상태-st를-연결한다-Concatenation"><a href="#Step-4-어텐션-값과-현재-상태-대코더의-은닉상태-st를-연결한다-Concatenation" class="headerlink" title="Step 4. 어텐션 값과 현재 상태 대코더의 은닉상태 st를 연결한다(Concatenation)."></a>Step 4. 어텐션 값과 현재 상태 대코더의 은닉상태 st를 연결한다(Concatenation).</h3><p><img src="https://wikidocs.net/images/page/22893/dotproductattention5_final_final.PNG" alt></p>
<p>최종적으로 구해낸 어텐션 값(컨텍스트 벡터) a를 현재 시점의 디코더 은닉상태 st와 결합한다. 이때 둘을 연결해 하나의 벡터로 만드는(concatenation) 작업을 수행한다.</p>
<p>해당 결합 작업을 통해 산출된 최종 벡터 vt는 t시점의 디코더 예측값 y_hat를 도출하기 위한 연산의 입력값으로 사용된다.</p>
<hr>
<h1 id="Bahdanau-Attention-바다나우-어텐션"><a href="#Bahdanau-Attention-바다나우-어텐션" class="headerlink" title="Bahdanau Attention(바다나우 어텐션)"></a>Bahdanau Attention(바다나우 어텐션)</h1><p>: 위에서 어텐션의 종류는 step 1의 어텐션 스코어를 구하는 방법에 따라 달라진다고 했다. </p>
<p>닷 프로덕트 어텐션은 인코더의 각 은닉상태 h1 ~ hN과 현재시점 디코더 은닉상태 st의 유사도인 어텐션 스코어를 내적으로 구하였기 때문에 닷 프로덕트 어텐션이라는 이름이 붙었다.</p>
<p>이때,</p>
<ul>
<li><p><code>현재시점 디코더의 은닉상태</code> : <strong>query</strong></p>
</li>
<li><p><code>이전 인코더의 모든 은닉상태</code> : <strong>key</strong>(=<strong>value</strong>)</p>
</li>
</ul>
<p>라고 한다면 닷 프로덕트 어텐션의 스코어 함수는 아래와 같을 것이다.</p>
<blockquote>
<p>score(query, key) = queryT * key</p>
</blockquote>
<p>이때 바다나우 어텐션은 아래와 같은 스코어 함수를 사용한다.</p>
<blockquote>
<p>score(query, key) = vT * tanh(W1 * key + W2 * query)</p>
<p>: vT는 transposed weight vector</p>
</blockquote>
</div><div class="article-tags is-size-7 mb-4"><span class="mr-2">#</span><a class="link-muted mr-2" rel="tag" href="/tags/pytorch/">pytorch</a><a class="link-muted mr-2" rel="tag" href="/tags/%EC%9E%90%EC%97%B0%EC%96%B4%EC%B2%98%EB%A6%AC/">자연어처리</a><a class="link-muted mr-2" rel="tag" href="/tags/%EB%94%A5%EB%9F%AC%EB%8B%9D/">딥러닝</a><a class="link-muted mr-2" rel="tag" href="/tags/%EC%96%B8%EC%96%B4%EB%AA%A8%EB%8D%B8/">언어모델</a><a class="link-muted mr-2" rel="tag" href="/tags/%ED%95%9C%EA%B5%AD%EC%96%B4-%EC%9E%84%EB%B2%A0%EB%94%A9/">한국어 임베딩</a><a class="link-muted mr-2" rel="tag" href="/tags/%EC%BC%80%EB%9D%BC%EC%8A%A4/">케라스</a><a class="link-muted mr-2" rel="tag" href="/tags/Attention/">Attention</a></div><link rel="stylesheet" href="https://cdn.jsdelivr.net/npm/social-share.js@1.0.16/dist/css/share.min.css"><div class="social-share"></div><script src="https://cdn.jsdelivr.net/npm/social-share.js@1.0.16/dist/js/social-share.min.js"></script></article></div><!--!--><nav class="post-navigation mt-4 level is-mobile"><div class="level-start"><a class="article-nav-prev level level-item link-muted" href="/2020/02/23/nuxt-study1/"><i class="level-item fas fa-chevron-left"></i><span class="level-item">Introduction to Nuxt.js</span></a></div><div class="level-end"><a class="article-nav-next level level-item link-muted" href="/2020/02/19/ai-study6/"><span class="level-item">[딥러닝 스터디] 케라스(Keras) 실습</span><i class="level-item fas fa-chevron-right"></i></a></div></nav><div class="card"><div class="card-content"><h3 class="title is-5">댓글</h3><div class="fb-comments" data-width="100%" data-href="https://katie0809.github.io/2020/02/23/ai-study7/" data-num-posts="5"></div><script>(function(d, s, id) {
            var js, fjs = d.getElementsByTagName(s)[0];
            if (d.getElementById(id)) return;
            js = d.createElement(s); js.id = id;
            js.src = "//connect.facebook.net/ko/sdk.js#xfbml=1&version=v12.0";
            fjs.parentNode.insertBefore(js, fjs);
        }(document, 'script', 'facebook-jssdk'));</script></div></div></div><!--!--><div class="column column-right is-4-tablet is-4-desktop is-4-widescreen  order-3 is-sticky"><div class="card widget" id="toc" data-type="toc"><div class="card-content"><div class="menu"><h3 class="menu-label">카탈로그</h3><ul class="menu-list"><li><a class="level is-mobile" href="#시퀀스-투-시퀀스-seq2seq"><span class="level-left"><span class="level-item">1</span><span class="level-item">시퀀스-투-시퀀스(seq2seq)</span></span></a></li><li><a class="level is-mobile" href="#Attention-model"><span class="level-left"><span class="level-item">2</span><span class="level-item">Attention model</span></span></a><ul class="menu-list"><li><a class="level is-mobile" href="#Attention-Overview"><span class="level-left"><span class="level-item">2.1</span><span class="level-item">Attention Overview</span></span></a></li><li><a class="level is-mobile" href="#Dot-product-attention"><span class="level-left"><span class="level-item">2.2</span><span class="level-item">Dot product attention</span></span></a><ul class="menu-list"><li><a class="level is-mobile" href="#Step-1-어텐션-스코어를-구한다"><span class="level-left"><span class="level-item">2.2.1</span><span class="level-item">Step 1. 어텐션 스코어를 구한다.</span></span></a></li><li><a class="level is-mobile" href="#Step-2-Softmax를-통해-Attention-Distribution-어텐션-분포-를-구한다"><span class="level-left"><span class="level-item">2.2.2</span><span class="level-item">Step 2. Softmax를 통해 Attention Distribution(어텐션 분포)를 구한다.</span></span></a></li><li><a class="level is-mobile" href="#Step-3-Attention-Weight과-인코더-은닉상태를-가중합하여-Attention-Value를-구한다"><span class="level-left"><span class="level-item">2.2.3</span><span class="level-item">Step 3. Attention Weight과 인코더 은닉상태를 가중합하여 Attention Value를 구한다.</span></span></a></li><li><a class="level is-mobile" href="#Step-4-어텐션-값과-현재-상태-대코더의-은닉상태-st를-연결한다-Concatenation"><span class="level-left"><span class="level-item">2.2.4</span><span class="level-item">Step 4. 어텐션 값과 현재 상태 대코더의 은닉상태 st를 연결한다(Concatenation).</span></span></a></li></ul></li></ul></li><li><a class="level is-mobile" href="#Bahdanau-Attention-바다나우-어텐션"><span class="level-left"><span class="level-item">3</span><span class="level-item">Bahdanau Attention(바다나우 어텐션)</span></span></a></li></ul></div></div><style>#toc .menu-list > li > a.is-active + .menu-list { display: block; }#toc .menu-list > li > a + .menu-list { display: none; }</style><script src="/js/toc.js" defer></script></div><div class="card widget" data-type="categories"><div class="card-content"><div class="menu"><h3 class="menu-label">카테고리</h3><ul class="menu-list"><li><a class="level is-mobile" href="/categories/%EA%B0%9C%EB%B0%9C%EC%9E%90-%EA%B3%B5%EB%B6%80/"><span class="level-start"><span class="level-item">개발자 공부</span></span><span class="level-end"><span class="level-item tag">42</span></span></a><ul><li><a class="level is-mobile" href="/categories/%EA%B0%9C%EB%B0%9C%EC%9E%90-%EA%B3%B5%EB%B6%80/JSP/"><span class="level-start"><span class="level-item">JSP</span></span><span class="level-end"><span class="level-item tag">1</span></span></a></li><li><a class="level is-mobile" href="/categories/%EA%B0%9C%EB%B0%9C%EC%9E%90-%EA%B3%B5%EB%B6%80/Javascript/"><span class="level-start"><span class="level-item">Javascript</span></span><span class="level-end"><span class="level-item tag">6</span></span></a><ul><li><a class="level is-mobile" href="/categories/%EA%B0%9C%EB%B0%9C%EC%9E%90-%EA%B3%B5%EB%B6%80/Javascript/ES6/"><span class="level-start"><span class="level-item">ES6</span></span><span class="level-end"><span class="level-item tag">1</span></span></a></li><li><a class="level is-mobile" href="/categories/%EA%B0%9C%EB%B0%9C%EC%9E%90-%EA%B3%B5%EB%B6%80/Javascript/Node-js/"><span class="level-start"><span class="level-item">Node.js</span></span><span class="level-end"><span class="level-item tag">1</span></span></a></li><li><a class="level is-mobile" href="/categories/%EA%B0%9C%EB%B0%9C%EC%9E%90-%EA%B3%B5%EB%B6%80/Javascript/Nuxt-js/"><span class="level-start"><span class="level-item">Nuxt.js</span></span><span class="level-end"><span class="level-item tag">2</span></span></a></li><li><a class="level is-mobile" href="/categories/%EA%B0%9C%EB%B0%9C%EC%9E%90-%EA%B3%B5%EB%B6%80/Javascript/Vue-js/"><span class="level-start"><span class="level-item">Vue.js</span></span><span class="level-end"><span class="level-item tag">2</span></span></a></li></ul></li><li><a class="level is-mobile" href="/categories/%EA%B0%9C%EB%B0%9C%EC%9E%90-%EA%B3%B5%EB%B6%80/Kaggle/"><span class="level-start"><span class="level-item">Kaggle</span></span><span class="level-end"><span class="level-item tag">1</span></span></a></li><li><a class="level is-mobile" href="/categories/%EA%B0%9C%EB%B0%9C%EC%9E%90-%EA%B3%B5%EB%B6%80/%EA%B5%AC%EA%B8%80%EC%95%A0%EB%84%90%EB%A6%AC%ED%8B%B1%EC%8A%A4/"><span class="level-start"><span class="level-item">구글애널리틱스</span></span><span class="level-end"><span class="level-item tag">2</span></span></a></li><li><a class="level is-mobile" href="/categories/%EA%B0%9C%EB%B0%9C%EC%9E%90-%EA%B3%B5%EB%B6%80/%EC%9D%B8%EA%B3%B5%EC%A7%80%EB%8A%A5/"><span class="level-start"><span class="level-item">인공지능</span></span><span class="level-end"><span class="level-item tag">24</span></span></a></li><li><a class="level is-mobile" href="/categories/%EA%B0%9C%EB%B0%9C%EC%9E%90-%EA%B3%B5%EB%B6%80/%EC%BD%94%EB%94%A9%ED%85%8C%EC%8A%A4%ED%8A%B8/"><span class="level-start"><span class="level-item">코딩테스트</span></span><span class="level-end"><span class="level-item tag">8</span></span></a></li></ul></li><li><a class="level is-mobile" href="/categories/%EA%B7%B8%EB%83%A5-%EA%B7%B8%EB%9F%B0-%EC%9D%BC%EC%83%81/"><span class="level-start"><span class="level-item">그냥 그런 일상</span></span><span class="level-end"><span class="level-item tag">4</span></span></a><ul><li><a class="level is-mobile" href="/categories/%EA%B7%B8%EB%83%A5-%EA%B7%B8%EB%9F%B0-%EC%9D%BC%EC%83%81/%EA%B7%B8%EB%A0%87%EA%B2%8C-%EB%B0%94%EB%B3%B4%EB%8A%94-%EC%95%84%EB%8B%98/"><span class="level-start"><span class="level-item">그렇게 바보는 아님</span></span><span class="level-end"><span class="level-item tag">2</span></span></a></li><li><a class="level is-mobile" href="/categories/%EA%B7%B8%EB%83%A5-%EA%B7%B8%EB%9F%B0-%EC%9D%BC%EC%83%81/%EB%81%84%EC%A0%81%EB%81%84%EC%A0%81/"><span class="level-start"><span class="level-item">끄적끄적</span></span><span class="level-end"><span class="level-item tag">2</span></span></a></li></ul></li></ul></div></div></div><div class="card widget" data-type="tags"><div class="card-content"><div class="menu"><h3 class="menu-label">태그</h3><div class="field is-grouped is-grouped-multiline"><div class="control"><a class="tags has-addons" href="/tags/Attention/"><span class="tag">Attention</span><span class="tag">3</span></a></div><div class="control"><a class="tags has-addons" href="/tags/ES2015/"><span class="tag">ES2015+</span><span class="tag">1</span></a></div><div class="control"><a class="tags has-addons" href="/tags/ES6/"><span class="tag">ES6</span><span class="tag">1</span></a></div><div class="control"><a class="tags has-addons" href="/tags/GA/"><span class="tag">GA</span><span class="tag">2</span></a></div><div class="control"><a class="tags has-addons" href="/tags/Google-Analytics/"><span class="tag">Google Analytics</span><span class="tag">2</span></a></div><div class="control"><a class="tags has-addons" href="/tags/Hexo/"><span class="tag">Hexo</span><span class="tag">1</span></a></div><div class="control"><a class="tags has-addons" href="/tags/Keras/"><span class="tag">Keras</span><span class="tag">1</span></a></div><div class="control"><a class="tags has-addons" href="/tags/RNN/"><span class="tag">RNN</span><span class="tag">4</span></a></div><div class="control"><a class="tags has-addons" href="/tags/YAMLException/"><span class="tag">YAMLException</span><span class="tag">1</span></a></div><div class="control"><a class="tags has-addons" href="/tags/api-server/"><span class="tag">api_server</span><span class="tag">1</span></a></div><div class="control"><a class="tags has-addons" href="/tags/change-permission/"><span class="tag">change permission</span><span class="tag">1</span></a></div><div class="control"><a class="tags has-addons" href="/tags/codility/"><span class="tag">codility</span><span class="tag">8</span></a></div><div class="control"><a class="tags has-addons" href="/tags/coding-rule/"><span class="tag">coding rule</span><span class="tag">2</span></a></div><div class="control"><a class="tags has-addons" href="/tags/coding-test/"><span class="tag">coding test</span><span class="tag">8</span></a></div><div class="control"><a class="tags has-addons" href="/tags/dacon/"><span class="tag">dacon</span><span class="tag">1</span></a></div><div class="control"><a class="tags has-addons" href="/tags/data-analysis/"><span class="tag">data analysis</span><span class="tag">1</span></a></div><div class="control"><a class="tags has-addons" href="/tags/developer/"><span class="tag">developer</span><span class="tag">8</span></a></div><div class="control"><a class="tags has-addons" href="/tags/express/"><span class="tag">express</span><span class="tag">1</span></a></div><div class="control"><a class="tags has-addons" href="/tags/folder-permission/"><span class="tag">folder permission</span><span class="tag">1</span></a></div><div class="control"><a class="tags has-addons" href="/tags/githubpage/"><span class="tag">githubpage</span><span class="tag">2</span></a></div><div class="control"><a class="tags has-addons" href="/tags/hexo/"><span class="tag">hexo</span><span class="tag">2</span></a></div><div class="control"><a class="tags has-addons" href="/tags/inflearn/"><span class="tag">inflearn</span><span class="tag">1</span></a></div><div class="control"><a class="tags has-addons" href="/tags/javascript/"><span class="tag">javascript</span><span class="tag">6</span></a></div><div class="control"><a class="tags has-addons" href="/tags/jsp/"><span class="tag">jsp</span><span class="tag">1</span></a></div><div class="control"><a class="tags has-addons" href="/tags/kaggle/"><span class="tag">kaggle</span><span class="tag">1</span></a></div><div class="control"><a class="tags has-addons" href="/tags/linear-regression/"><span class="tag">linear regression</span><span class="tag">2</span></a></div><div class="control"><a class="tags has-addons" href="/tags/logistic-regression/"><span class="tag">logistic regression</span><span class="tag">1</span></a></div><div class="control"><a class="tags has-addons" href="/tags/mac/"><span class="tag">mac</span><span class="tag">1</span></a></div><div class="control"><a class="tags has-addons" href="/tags/nodejs/"><span class="tag">nodejs</span><span class="tag">1</span></a></div><div class="control"><a class="tags has-addons" href="/tags/numpy/"><span class="tag">numpy</span><span class="tag">2</span></a></div><div class="control"><a class="tags has-addons" href="/tags/nuxtjs/"><span class="tag">nuxtjs</span><span class="tag">2</span></a></div><div class="control"><a class="tags has-addons" href="/tags/python/"><span class="tag">python</span><span class="tag">2</span></a></div><div class="control"><a class="tags has-addons" href="/tags/pytorch/"><span class="tag">pytorch</span><span class="tag">10</span></a></div><div class="control"><a class="tags has-addons" href="/tags/style-guide/"><span class="tag">style guide</span><span class="tag">2</span></a></div><div class="control"><a class="tags has-addons" href="/tags/tensorflow/"><span class="tag">tensorflow</span><span class="tag">9</span></a></div><div class="control"><a class="tags has-addons" href="/tags/tensorflow-2-0/"><span class="tag">tensorflow 2.0</span><span class="tag">1</span></a></div><div class="control"><a class="tags has-addons" href="/tags/tensorflow-developer-certification/"><span class="tag">tensorflow developer certification</span><span class="tag">7</span></a></div><div class="control"><a class="tags has-addons" href="/tags/torchtext/"><span class="tag">torchtext</span><span class="tag">1</span></a></div><div class="control"><a class="tags has-addons" href="/tags/vue/"><span class="tag">vue</span><span class="tag">2</span></a></div><div class="control"><a class="tags has-addons" href="/tags/vuejs/"><span class="tag">vuejs</span><span class="tag">4</span></a></div><div class="control"><a class="tags has-addons" href="/tags/vueschool-io/"><span class="tag">vueschool.io</span><span class="tag">2</span></a></div><div class="control"><a class="tags has-addons" href="/tags/%EA%B5%AC%EA%B8%80/"><span class="tag">구글</span><span class="tag">2</span></a></div><div class="control"><a class="tags has-addons" href="/tags/%EA%B5%AC%EA%B8%80%EC%95%A0%EB%84%90%EB%A6%AC%ED%8B%B1%EC%8A%A4/"><span class="tag">구글애널리틱스</span><span class="tag">2</span></a></div><div class="control"><a class="tags has-addons" href="/tags/%EA%B7%B8%EB%9E%98%ED%94%84-%EC%8B%9C%EA%B0%81%ED%99%94/"><span class="tag">그래프 시각화</span><span class="tag">1</span></a></div><div class="control"><a class="tags has-addons" href="/tags/%EB%8D%B0%EC%9D%B4%EC%BD%98/"><span class="tag">데이콘</span><span class="tag">1</span></a></div><div class="control"><a class="tags has-addons" href="/tags/%EB%8D%B0%EC%9D%B4%ED%84%B0%EB%B6%84%EC%84%9D/"><span class="tag">데이터분석</span><span class="tag">1</span></a></div><div class="control"><a class="tags has-addons" href="/tags/%EB%94%A5%EB%9F%AC%EB%8B%9D/"><span class="tag">딥러닝</span><span class="tag">9</span></a></div><div class="control"><a class="tags has-addons" href="/tags/%EB%94%A5%EB%9F%AC%EB%8B%9D%EA%B8%B0%EC%B4%88/"><span class="tag">딥러닝기초</span><span class="tag">7</span></a></div><div class="control"><a class="tags has-addons" href="/tags/%EB%A1%9C%EC%A7%80%EC%8A%A4%ED%8B%B1-%ED%9A%8C%EA%B7%80/"><span class="tag">로지스틱 회귀</span><span class="tag">1</span></a></div><div class="control"><a class="tags has-addons" href="/tags/%EC%84%A0%ED%98%95%ED%9A%8C%EA%B7%80/"><span class="tag">선형회귀</span><span class="tag">2</span></a></div><div class="control"><a class="tags has-addons" href="/tags/%EC%88%9C%ED%99%98%EC%8B%A0%EA%B2%BD%EB%A7%9D/"><span class="tag">순환신경망</span><span class="tag">3</span></a></div><div class="control"><a class="tags has-addons" href="/tags/%EC%96%B8%EC%96%B4%EB%AA%A8%EB%8D%B8/"><span class="tag">언어모델</span><span class="tag">8</span></a></div><div class="control"><a class="tags has-addons" href="/tags/%EC%9D%B8%EA%B3%B5%EC%A7%80%EB%8A%A5/"><span class="tag">인공지능</span><span class="tag">8</span></a></div><div class="control"><a class="tags has-addons" href="/tags/%EC%9E%90%EA%B2%A9%EC%A6%9D/"><span class="tag">자격증</span><span class="tag">7</span></a></div><div class="control"><a class="tags has-addons" href="/tags/%EC%9E%90%EC%97%B0%EC%96%B4-%EC%A0%84%EC%B2%98%EB%A6%AC/"><span class="tag">자연어 전처리</span><span class="tag">2</span></a></div><div class="control"><a class="tags has-addons" href="/tags/%EC%9E%90%EC%97%B0%EC%96%B4%EC%B2%98%EB%A6%AC/"><span class="tag">자연어처리</span><span class="tag">17</span></a></div><div class="control"><a class="tags has-addons" href="/tags/%EC%BA%90%EA%B8%80/"><span class="tag">캐글</span><span class="tag">1</span></a></div><div class="control"><a class="tags has-addons" href="/tags/%EC%BC%80%EB%9D%BC%EC%8A%A4/"><span class="tag">케라스</span><span class="tag">3</span></a></div><div class="control"><a class="tags has-addons" href="/tags/%ED%95%9C%EA%B5%AD%EC%96%B4-%EC%9E%84%EB%B2%A0%EB%94%A9/"><span class="tag">한국어 임베딩</span><span class="tag">6</span></a></div></div></div></div></div></div></div></div></section><footer class="footer"><div class="container"><div class="level"><div class="level-start"><a class="footer-logo is-block mb-2" href="/"><img src="/img/logo.svg" alt="Women&#039;s Techmaker" height="28"></a><p class="is-size-7"><span>&copy; 2022 Kyungim Lee</span>  Powered by <a href="https://hexo.io/" target="_blank" rel="external nofollow noopener noreferrer">Hexo</a> &amp; <a href="https://github.com/ppoffice/hexo-theme-icarus" target="_blank" rel="external nofollow noopener noreferrer">Icarus</a></p></div><div class="level-end"><div class="field has-addons"><p class="control"><a class="button is-transparent is-large" target="_blank" rel="external nofollow noopener noreferrer" title="Creative Commons" href="https://creativecommons.org/"><i class="fab fa-creative-commons"></i></a></p><p class="control"><a class="button is-transparent is-large" target="_blank" rel="external nofollow noopener noreferrer" title="Attribution 4.0 International" href="https://creativecommons.org/licenses/by/4.0/"><i class="fab fa-creative-commons-by"></i></a></p><p class="control"><a class="button is-transparent is-large" target="_blank" rel="external nofollow noopener noreferrer" title="Download on GitHub" href="https://github.com/katie0809"><i class="fab fa-github"></i></a></p></div></div></div></div></footer><script src="https://cdn.jsdelivr.net/npm/jquery@3.3.1/dist/jquery.min.js"></script><script src="https://cdn.jsdelivr.net/npm/moment@2.22.2/min/moment-with-locales.min.js"></script><script src="https://cdn.jsdelivr.net/npm/clipboard@2.0.4/dist/clipboard.min.js" defer></script><script>moment.locale("ko");</script><script>var IcarusThemeSettings = {
            article: {
                highlight: {
                    clipboard: true,
                    fold: 'unfolded'
                }
            }
        };</script><script src="/js/column.js"></script><script src="/js/animation.js"></script><a id="back-to-top" title="맨 위로" href="javascript:;"><i class="fas fa-chevron-up"></i></a><script src="/js/back_to_top.js" defer></script><!--!--><!--!--><script src="/js/main.js" defer></script><div class="searchbox"><div class="searchbox-container"><div class="searchbox-header"><div class="searchbox-input-container"><input class="searchbox-input" type="text" placeholder="입력 하세요..."></div><a class="searchbox-close" href="javascript:;">×</a></div><div class="searchbox-body"></div></div></div><script src="/js/insight.js" defer></script><script>document.addEventListener('DOMContentLoaded', function () {
            loadInsight({"contentUrl":"/content.json"}, {"hint":"입력 하세요...","untitled":"(제목 없음)","posts":"포스트","pages":"페이지","categories":"카테고리","tags":"태그"});
        });</script></body></html>